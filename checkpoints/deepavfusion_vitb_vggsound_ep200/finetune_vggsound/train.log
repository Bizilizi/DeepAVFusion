[17:13:31.297085] job dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound
[17:13:31.298035] env.world_size: 4
[17:13:31.298406] env.rank: 0
[17:13:31.298657] env.dist_url: tcp://localhost:50001
[17:13:31.298898] env.dist_backend: nccl
[17:13:31.299126] env.port: 50001
[17:13:31.299361] env.node: localhost
[17:13:31.299586] env.distributed: True
[17:13:31.299809] env.seed: None
[17:13:31.300039] env.gpu: None
[17:13:31.300264] env.ngpu: 4
[17:13:31.300494] env.mem_gb: 240
[17:13:31.300708] env.workers: 8
[17:13:31.301045] env.slurm: True
[17:13:31.301293] env.slurm_suffix: 
[17:13:31.301526] env.slurm_partition: NORMAL
[17:13:31.301772] env.slurm_comment: DeepAVFusion
[17:13:31.302060] env.slurm_timeout: 1440
[17:13:31.302292] env.nodelist: node14
[17:13:31.302511] env.exclude: 
[17:13:31.302771] log.print_freq: 100
[17:13:31.303071] log.save_freq: 50
[17:13:31.303314] log.eval_freq: 10
[17:13:31.303532] log.wandb_watch_freq: 0
[17:13:31.303767] log.debug: False
[17:13:31.303983] log.use_wandb: True
[17:13:31.304229] log.wandb_entity: audiovisual_diagnostics
[17:13:31.304477] log.wandb_project: efav
[17:13:31.304742] worker: eval_finetune
[17:13:31.304978] output_dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200
[17:13:31.305248] job_name: finetune_vggsound
[17:13:31.305481] pretrain_job_name: deepavfusion_vitb_vggsound_ep200
[17:13:31.305707] checkpoint: None
[17:13:31.305937] encoder_prefix: encoder.
[17:13:31.306158] pretrain_resume_epoch: latest
[17:13:31.306396] eval: False
[17:13:31.306619] debug: False
[17:13:31.306924] model.image.backbone: vit_base
[17:13:31.307152] model.image.pretrained: vit_base_mae_in1k
[17:13:31.307445] model.audio.backbone: vit_base
[17:13:31.307681] model.audio.pretrained: vit_base_audiomae_as2m
[17:13:31.307977] model.fusion.arch: factorized_mmi
[17:13:31.308708] model.fusion.layers: all
[17:13:31.308961] model.fusion.num_fusion_tkns: 16
[17:13:31.309207] model.fusion.num_aggr_image_tkns: 8
[17:13:31.309450] model.fusion.num_aggr_audio_tkns: 8
[17:13:31.309696] model.fusion.mlp_ratio: 1.0
[17:13:31.309938] model.fusion.attn_ratio: 0.25
[17:13:31.310186] model.fusion.num_heads: 12
[17:13:31.310478] opt.resume: True
[17:13:31.310713] opt.joint_loss: True
[17:13:31.310959] opt.batch_size: 32
[17:13:31.311218] opt.epochs: 100
[17:13:31.311478] opt.warmup_epochs: 20
[17:13:31.311705] opt.accum_iter: 4
[17:13:31.311949] opt.clip_grad: None
[17:13:31.312315] opt.weight_decay: 0.05
[17:13:31.312606] opt.layer_decay: 0.75
[17:13:31.312894] opt.smoothing: 0.1
[17:13:31.313118] opt.lr: None
[17:13:31.313379] opt.blr: 0.0003
[17:13:31.313634] opt.min_lr: 0.0
[17:13:31.313900] opt.drop_path: 0.2
[17:13:31.314190] opt.attn_drop: 0.0
[17:13:31.314493] opt.proj_drop: 0.0
[17:13:31.314759] opt.use_amp: False
[17:13:31.315057] data.dataset: vggsound
[17:13:31.315338] data.data_path: /tmp/zverev/vggsound
[17:13:31.315564] data.audio_rate: 16000
[17:13:31.315777] data.audio_dur: 3.0
[17:13:31.316005] data.audio_mels: 128
[17:13:31.316268] data.image_size: 224
[17:13:31.316482] data.crop_min: 0.5
[17:13:31.316720] data.mixup: 1.0
[17:13:31.316954] data.cutmix: 0.0
[17:13:31.317168] data.cutmix_minmax: None
[17:13:31.317483] data.mixup_prob: 1.0
[17:13:31.317695] data.mixup_switch_prob: 0.5
[17:13:31.317929] data.mixup_mode: batch
[17:13:31.318582] base lr: 3.00e-04
[17:13:31.318917] actual lr: 6.00e-04
[17:13:31.319226] accumulate grad iterations: 4
[17:13:31.319443] effective batch size: 512
[17:13:46.677843] Video CN7qu_1k3F8 not found
[17:14:00.361999] Video RflAC1ror3c not found
[17:14:26.976097] Video wvHLlBCYe8c not found
[17:14:31.168301] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 183727
 - Num classes: 310
[17:14:36.861790] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:42.658167] DenseVideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:42.659098] Mixup is activated!
[17:14:43.621501] Loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth
[17:14:43.622123] Error loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth: [Errno 2] No such file or directory: 'https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth' 
 Loading from torch hub ...
[17:14:47.029769] Loading checkpoint from assets/models/vitbase_audiomae_as2m.pth
[17:14:52.282402] Model = AVClassifier(
  (encoder): DeepAVFusion(
    (image): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (audio): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (fusion_blocks): ModuleList(
      (0-11): 12 x FusionBlock_FactorizedAVInteractions(
        (norm1_mm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_aud): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_img): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): CrossAttention_FactorizedAVInteractions(
          (attn_v): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (attn_a): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (q): Linear(in_features=768, out_features=192, bias=True)
          (k): Linear(in_features=1536, out_features=192, bias=True)
          (v): Linear(in_features=1536, out_features=768, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=768, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (fusion_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (image_head): Linear(in_features=768, out_features=310, bias=True)
  (audio_head): Linear(in_features=768, out_features=310, bias=True)
  (fusion_head): Linear(in_features=768, out_features=310, bias=True)
)
[17:14:55.862823] Loaded pre-trained checkpoint: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/checkpoints/checkpoint_latest.pth
[17:14:55.881053] criterion = SoftTargetCrossEntropy()
[17:14:56.038800] Loading /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth
[17:15:02.017541] Start training for 100 epochs
[17:15:13.374618] [Train][Ep-0/100]  [   0/1435]  eta: 4:31:31  lr: 0.0000 (0.0000)  time: 11.3526  data: 10.4303  max mem: 10643
[17:19:19.250614] [Train][Ep-0/100]  [ 100/1435]  eta: 0:56:39  lr: 0.0000 (0.0000)  loss: 2621.9917 (2622.0184)  grad_norm: 332.3825 (388.4210)  amp_scale: 1.0000 (1.0000)  time: 2.1483  data: 1.5644  max mem: 13797
[17:22:44.365677] [Train][Ep-0/100]  [ 200/1435]  eta: 0:47:20  lr: 0.0000 (0.0000)  loss: 2599.4851 (2618.6571)  grad_norm: 331.0141 (373.3987)  amp_scale: 1.0000 (1.0000)  time: 1.9413  data: 0.8757  max mem: 13797
[17:25:55.336672] [Train][Ep-0/100]  [ 300/1435]  eta: 0:41:03  lr: 0.0000 (0.0000)  loss: 2594.9211 (2617.9323)  grad_norm: 336.1728 (363.1726)  amp_scale: 1.0000 (1.0000)  time: 1.8154  data: 0.3859  max mem: 13797
[17:28:53.324145] [Train][Ep-0/100]  [ 400/1435]  eta: 0:35:45  lr: 0.0000 (0.0000)  loss: 2592.7026 (2615.6584)  grad_norm: 332.6407 (357.3514)  amp_scale: 1.0000 (1.0000)  time: 1.6955  data: 0.4344  max mem: 13797
[17:31:50.287661] [Train][Ep-0/100]  [ 500/1435]  eta: 0:31:21  lr: 0.0000 (0.0000)  loss: 2608.9009 (2614.7104)  grad_norm: 325.9750 (352.0515)  amp_scale: 1.0000 (1.0000)  time: 1.7189  data: 0.0196  max mem: 13797
[17:34:36.374889] [Train][Ep-0/100]  [ 600/1435]  eta: 0:27:11  lr: 0.0000 (0.0000)  loss: 2573.7168 (2609.2040)  grad_norm: 324.1283 (350.4162)  amp_scale: 1.0000 (1.0000)  time: 1.5863  data: 1.0071  max mem: 13797
[17:37:10.914373] [Train][Ep-0/100]  [ 700/1435]  eta: 0:23:13  lr: 0.0000 (0.0000)  loss: 2586.1316 (2606.2442)  grad_norm: 329.4289 (349.2904)  amp_scale: 1.0000 (1.0000)  time: 1.4802  data: 0.6046  max mem: 13797
[17:39:44.643682] [Train][Ep-0/100]  [ 800/1435]  eta: 0:19:35  lr: 0.0000 (0.0000)  loss: 2582.1106 (2603.1033)  grad_norm: 324.8765 (354.6215)  amp_scale: 1.0000 (1.0000)  time: 1.5854  data: 0.9965  max mem: 13797
[17:42:06.216576] [Train][Ep-0/100]  [ 900/1435]  eta: 0:16:04  lr: 0.0000 (0.0000)  loss: 2573.1816 (2599.4454)  grad_norm: 315.8960 (350.4361)  amp_scale: 1.0000 (1.0000)  time: 1.3582  data: 0.5643  max mem: 13797
[17:44:40.250337] [Train][Ep-0/100]  [1000/1435]  eta: 0:12:52  lr: 0.0000 (0.0000)  loss: 2553.2585 (2595.6169)  grad_norm: 313.2741 (347.1948)  amp_scale: 1.0000 (1.0000)  time: 1.5131  data: 0.0155  max mem: 13797
[17:47:05.443897] [Train][Ep-0/100]  [1100/1435]  eta: 0:09:45  lr: 0.0000 (0.0000)  loss: 2551.9551 (2591.8467)  grad_norm: 320.8311 (344.9176)  amp_scale: 1.0000 (1.0000)  time: 1.3711  data: 0.0485  max mem: 13797
[17:49:29.754066] [Train][Ep-0/100]  [1200/1435]  eta: 0:06:44  lr: 0.0000 (0.0000)  loss: 2532.5994 (2587.5194)  grad_norm: 316.7364 (343.0827)  amp_scale: 1.0000 (1.0000)  time: 1.4463  data: 0.0005  max mem: 13797
[17:51:53.336793] [Train][Ep-0/100]  [1300/1435]  eta: 0:03:49  lr: 0.0000 (0.0000)  loss: 2539.3513 (2584.1365)  grad_norm: 325.0536 (342.7344)  amp_scale: 1.0000 (1.0000)  time: 1.4877  data: 0.0006  max mem: 13797
[17:54:18.344530] [Train][Ep-0/100]  [1400/1435]  eta: 0:00:58  lr: 0.0000 (0.0000)  loss: 2526.6660 (2580.1527)  grad_norm: 340.3698 (343.3074)  amp_scale: 1.0000 (1.0000)  time: 1.4478  data: 0.4119  max mem: 13797
[17:55:06.567498] [Train][Ep-0/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 2525.5361 (2578.5755)  grad_norm: 340.5636 (343.5397)  amp_scale: 1.0000 (1.0000)  time: 1.4436  data: 0.3098  max mem: 13797
[17:55:06.568935] [Train][Ep-0/100] Total time: 0:40:04 (1.6756 s / it)
[17:55:06.569694] Syncing meters...
[17:55:06.572128] Averaged stats: lr: 0.0000 (0.0000)  loss: 2525.5361 (2576.7666)  grad_norm: 340.5636 (343.5397)  amp_scale: 1.0000 (1.0000)
[17:55:12.522205] [Eval][Ep-0/100]  [  0/121]  eta: 0:11:58  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 5.9417  data: 5.7755  max mem: 13797
[17:56:56.790698] [Eval][Ep-0/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.0823  data: 0.9192  max mem: 13797
[17:57:18.049205] [Eval][Ep-0/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.0628  data: 0.9017  max mem: 13797
[17:57:18.050422] [Eval][Ep-0/100] Total time: 0:02:11 (1.0865 s / it)
[17:57:18.526685] [Eval][Ep-0/100] val_acc1_image=0.93 | val_acc1_audio=1.13 | val_acc1_fusion=2.25 | val_acc1_all=3.15
[17:57:27.839075] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 1)
[17:57:29.949672] [Train][Ep-1/100]  [   0/1435]  eta: 0:45:56  lr: 0.0000 (0.0000)  time: 1.9206  data: 1.4301  max mem: 13797
[17:59:40.493455] [Train][Ep-1/100]  [ 100/1435]  eta: 0:29:10  lr: 0.0000 (0.0000)  loss: 2487.8120 (2499.4845)  grad_norm: 362.8018 (363.7354)  amp_scale: 1.0000 (1.0000)  time: 1.2968  data: 0.7120  max mem: 13797
[18:01:49.896361] [Train][Ep-1/100]  [ 200/1435]  eta: 0:26:48  lr: 0.0000 (0.0000)  loss: 2466.2393 (2489.6781)  grad_norm: 377.9850 (373.5135)  amp_scale: 1.0000 (1.0000)  time: 1.3223  data: 0.6062  max mem: 13797
[18:03:56.470537] [Train][Ep-1/100]  [ 300/1435]  eta: 0:24:24  lr: 0.0000 (0.0000)  loss: 2460.8945 (2479.2753)  grad_norm: 398.7503 (381.5027)  amp_scale: 1.0000 (1.0000)  time: 1.2242  data: 0.6223  max mem: 13797
[18:06:03.951954] [Train][Ep-1/100]  [ 400/1435]  eta: 0:22:11  lr: 0.0000 (0.0000)  loss: 2421.4456 (2469.0803)  grad_norm: 419.8355 (390.1525)  amp_scale: 1.0000 (1.0000)  time: 1.2500  data: 0.1063  max mem: 13797
[18:08:15.286806] [Train][Ep-1/100]  [ 500/1435]  eta: 0:20:07  lr: 0.0000 (0.0000)  loss: 2383.4185 (2451.7630)  grad_norm: 425.6128 (398.0472)  amp_scale: 1.0000 (1.0000)  time: 1.3027  data: 0.0634  max mem: 13797
[18:10:27.720356] [Train][Ep-1/100]  [ 600/1435]  eta: 0:18:03  lr: 0.0000 (0.0000)  loss: 2373.3096 (2439.7335)  grad_norm: 451.0333 (411.7955)  amp_scale: 1.0000 (1.0000)  time: 1.3354  data: 0.0008  max mem: 13797
[18:12:43.648893] [Train][Ep-1/100]  [ 700/1435]  eta: 0:15:59  lr: 0.0000 (0.0000)  loss: 2314.1650 (2423.3867)  grad_norm: 465.2586 (419.8491)  amp_scale: 1.0000 (1.0000)  time: 1.2969  data: 0.0002  max mem: 13797
[18:14:57.368926] [Train][Ep-1/100]  [ 800/1435]  eta: 0:13:51  lr: 0.0000 (0.0000)  loss: 2269.6750 (2406.3412)  grad_norm: 479.2411 (429.2933)  amp_scale: 1.0000 (1.0000)  time: 1.3162  data: 0.0004  max mem: 13797
[18:17:12.570362] [Train][Ep-1/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0000 (0.0000)  loss: 2233.7393 (2388.8755)  grad_norm: 503.1520 (437.7363)  amp_scale: 1.0000 (1.0000)  time: 1.3966  data: 0.0046  max mem: 13797
[18:19:33.832021] [Train][Ep-1/100]  [1000/1435]  eta: 0:09:36  lr: 0.0001 (0.0000)  loss: 2218.1084 (2370.7932)  grad_norm: 521.9769 (446.6772)  amp_scale: 1.0000 (1.0000)  time: 1.3703  data: 0.0030  max mem: 13797
[18:21:55.359460] [Train][Ep-1/100]  [1100/1435]  eta: 0:07:26  lr: 0.0001 (0.0000)  loss: 2193.2844 (2353.7545)  grad_norm: 532.2932 (455.2024)  amp_scale: 1.0000 (1.0000)  time: 1.4642  data: 0.0393  max mem: 13797
[18:24:19.455706] [Train][Ep-1/100]  [1200/1435]  eta: 0:05:15  lr: 0.0001 (0.0000)  loss: 2204.1748 (2340.0247)  grad_norm: 532.5417 (462.8968)  amp_scale: 1.0000 (1.0000)  time: 1.4460  data: 0.0083  max mem: 13797
[18:26:43.371891] [Train][Ep-1/100]  [1300/1435]  eta: 0:03:02  lr: 0.0001 (0.0000)  loss: 2083.8074 (2322.6193)  grad_norm: 547.3959 (470.2667)  amp_scale: 1.0000 (1.0000)  time: 1.4772  data: 0.0304  max mem: 13797
[18:29:08.841839] [Train][Ep-1/100]  [1400/1435]  eta: 0:00:47  lr: 0.0001 (0.0000)  loss: 2122.5518 (2309.5212)  grad_norm: 592.5518 (478.8522)  amp_scale: 1.0000 (1.0000)  time: 1.4301  data: 0.0005  max mem: 13797
[18:29:56.049888] [Train][Ep-1/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0000)  loss: 2131.6016 (2305.2622)  grad_norm: 596.2884 (481.7634)  amp_scale: 1.0000 (1.0000)  time: 1.4173  data: 0.0268  max mem: 13797
[18:29:56.050942] [Train][Ep-1/100] Total time: 0:32:28 (1.3575 s / it)
[18:29:56.051442] Syncing meters...
[18:29:57.453314] Averaged stats: lr: 0.0001 (0.0000)  loss: 2131.6016 (2310.9401)  grad_norm: 596.2884 (481.7634)  amp_scale: 1.0000 (1.0000)
[18:30:07.118656] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 2)
[18:30:09.397702] [Train][Ep-2/100]  [   0/1435]  eta: 0:51:09  lr: 0.0001 (0.0001)  time: 2.1388  data: 1.6427  max mem: 13797
[18:32:13.154870] [Train][Ep-2/100]  [ 100/1435]  eta: 0:27:44  lr: 0.0001 (0.0001)  loss: 2060.9968 (2083.3657)  grad_norm: 592.3734 (749.0643)  amp_scale: 1.0000 (1.0000)  time: 1.2722  data: 0.0128  max mem: 13797
[18:34:21.942426] [Train][Ep-2/100]  [ 200/1435]  eta: 0:26:04  lr: 0.0001 (0.0001)  loss: 2054.5098 (2070.9446)  grad_norm: 613.1323 (689.9120)  amp_scale: 1.0000 (1.0000)  time: 1.3093  data: 0.0003  max mem: 13797
[18:36:30.383336] [Train][Ep-2/100]  [ 300/1435]  eta: 0:24:04  lr: 0.0001 (0.0001)  loss: 2057.1606 (2063.0159)  grad_norm: 639.0955 (676.0499)  amp_scale: 1.0000 (1.0000)  time: 1.2968  data: 0.0009  max mem: 13797
[18:38:44.218771] [Train][Ep-2/100]  [ 400/1435]  eta: 0:22:14  lr: 0.0001 (0.0001)  loss: 2022.9004 (2053.4360)  grad_norm: 636.5552 (672.2840)  amp_scale: 1.0000 (1.0000)  time: 1.3318  data: 0.0003  max mem: 13797
[18:40:59.918052] [Train][Ep-2/100]  [ 500/1435]  eta: 0:20:17  lr: 0.0001 (0.0001)  loss: 1999.5372 (2038.1295)  grad_norm: 652.2379 (672.8345)  amp_scale: 1.0000 (1.0000)  time: 1.3510  data: 0.0004  max mem: 13797
[18:43:15.967598] [Train][Ep-2/100]  [ 600/1435]  eta: 0:18:15  lr: 0.0001 (0.0001)  loss: 1975.6068 (2030.2007)  grad_norm: 676.9940 (676.1875)  amp_scale: 1.0000 (1.0000)  time: 1.3451  data: 0.0010  max mem: 13797
[18:45:29.090564] [Train][Ep-2/100]  [ 700/1435]  eta: 0:16:06  lr: 0.0001 (0.0001)  loss: 2007.8593 (2023.1898)  grad_norm: 665.0380 (677.8625)  amp_scale: 1.0000 (1.0000)  time: 1.3102  data: 0.0010  max mem: 13797
[18:47:46.436661] [Train][Ep-2/100]  [ 800/1435]  eta: 0:13:59  lr: 0.0001 (0.0001)  loss: 1936.0583 (2013.8200)  grad_norm: 713.8329 (684.3833)  amp_scale: 1.0000 (1.0000)  time: 1.3011  data: 0.0022  max mem: 13797
[18:49:57.793967] [Train][Ep-2/100]  [ 900/1435]  eta: 0:11:46  lr: 0.0001 (0.0001)  loss: 1917.9835 (2002.6057)  grad_norm: 705.0850 (690.6803)  amp_scale: 1.0000 (1.0000)  time: 1.3483  data: 0.0004  max mem: 13797
[18:52:12.823216] [Train][Ep-2/100]  [1000/1435]  eta: 0:09:36  lr: 0.0001 (0.0001)  loss: 1954.1616 (1996.1346)  grad_norm: 738.3226 (698.2386)  amp_scale: 1.0000 (1.0000)  time: 1.3339  data: 0.0129  max mem: 13797
[18:54:29.585492] [Train][Ep-2/100]  [1100/1435]  eta: 0:07:24  lr: 0.0001 (0.0001)  loss: 1932.9783 (1987.8177)  grad_norm: 752.3262 (705.0520)  amp_scale: 1.0000 (1.0000)  time: 1.4405  data: 0.0009  max mem: 13797
[18:56:45.810090] [Train][Ep-2/100]  [1200/1435]  eta: 0:05:12  lr: 0.0001 (0.0001)  loss: 1922.2244 (1982.7421)  grad_norm: 752.2128 (708.8928)  amp_scale: 1.0000 (1.0000)  time: 1.3406  data: 0.0041  max mem: 13797
[18:59:04.915893] [Train][Ep-2/100]  [1300/1435]  eta: 0:03:00  lr: 0.0001 (0.0001)  loss: 1848.6633 (1975.2918)  grad_norm: 808.7743 (724.8693)  amp_scale: 1.0000 (1.0000)  time: 1.3856  data: 0.0002  max mem: 13797
[19:01:25.346640] [Train][Ep-2/100]  [1400/1435]  eta: 0:00:46  lr: 0.0001 (0.0001)  loss: 1892.5063 (1969.5421)  grad_norm: 756.8181 (731.1104)  amp_scale: 1.0000 (1.0000)  time: 1.4445  data: 0.0004  max mem: 13797
[19:02:12.253253] [Train][Ep-2/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1898.4275 (1968.4871)  grad_norm: 775.7325 (732.8122)  amp_scale: 1.0000 (1.0000)  time: 1.4185  data: 0.0081  max mem: 13797
[19:02:12.254266] [Train][Ep-2/100] Total time: 0:32:04 (1.3415 s / it)
[19:02:12.254711] Syncing meters...
[19:02:13.154683] Averaged stats: lr: 0.0001 (0.0001)  loss: 1898.4275 (1968.5223)  grad_norm: 775.7325 (732.8122)  amp_scale: 1.0000 (1.0000)
[19:02:22.901045] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 3)
[19:02:25.328081] [Train][Ep-3/100]  [   0/1435]  eta: 0:53:31  lr: 0.0001 (0.0001)  time: 2.2378  data: 1.7415  max mem: 13797
[19:04:27.883791] [Train][Ep-3/100]  [ 100/1435]  eta: 0:27:29  lr: 0.0001 (0.0001)  loss: 1885.8702 (1844.1219)  grad_norm: 791.2436 (797.4071)  amp_scale: 1.0000 (1.0000)  time: 1.3191  data: 0.0007  max mem: 13797
[19:06:38.249782] [Train][Ep-3/100]  [ 200/1435]  eta: 0:26:07  lr: 0.0001 (0.0001)  loss: 1840.0648 (1833.7071)  grad_norm: 782.0264 (794.8221)  amp_scale: 1.0000 (1.0000)  time: 1.2911  data: 0.0007  max mem: 13797
[19:08:52.322401] [Train][Ep-3/100]  [ 300/1435]  eta: 0:24:27  lr: 0.0001 (0.0001)  loss: 1752.0919 (1824.3404)  grad_norm: 772.2114 (794.1392)  amp_scale: 1.0000 (1.0000)  time: 1.3232  data: 0.0005  max mem: 13797
[19:11:08.106599] [Train][Ep-3/100]  [ 400/1435]  eta: 0:22:35  lr: 0.0001 (0.0001)  loss: 1801.3407 (1823.3185)  grad_norm: 813.3647 (801.8391)  amp_scale: 1.0000 (1.0000)  time: 1.3954  data: 0.0237  max mem: 13797
[19:13:22.975101] [Train][Ep-3/100]  [ 500/1435]  eta: 0:20:31  lr: 0.0001 (0.0001)  loss: 1752.4475 (1818.2568)  grad_norm: 828.3424 (807.7145)  amp_scale: 1.0000 (1.0000)  time: 1.2952  data: 0.4177  max mem: 13797
[19:15:37.279443] [Train][Ep-3/100]  [ 600/1435]  eta: 0:18:23  lr: 0.0001 (0.0001)  loss: 1774.4076 (1817.5360)  grad_norm: 817.9628 (810.3515)  amp_scale: 1.0000 (1.0000)  time: 1.4112  data: 0.8259  max mem: 13797
[19:17:52.323782] [Train][Ep-3/100]  [ 700/1435]  eta: 0:16:14  lr: 0.0001 (0.0001)  loss: 1836.2286 (1816.9518)  grad_norm: 813.3636 (814.2682)  amp_scale: 1.0000 (1.0000)  time: 1.2056  data: 0.6257  max mem: 13797
[19:20:01.181923] [Train][Ep-3/100]  [ 800/1435]  eta: 0:13:58  lr: 0.0001 (0.0001)  loss: 1796.8395 (1815.6280)  grad_norm: 837.5607 (818.0689)  amp_scale: 1.0000 (1.0000)  time: 1.3588  data: 0.2690  max mem: 13797
[19:22:09.551273] [Train][Ep-3/100]  [ 900/1435]  eta: 0:11:44  lr: 0.0001 (0.0001)  loss: 1766.0627 (1810.9953)  grad_norm: 830.2333 (823.1871)  amp_scale: 1.0000 (1.0000)  time: 1.3292  data: 0.7385  max mem: 13797
[19:24:22.794637] [Train][Ep-3/100]  [1000/1435]  eta: 0:09:33  lr: 0.0001 (0.0001)  loss: 1740.7327 (1805.8020)  grad_norm: 839.7922 (825.2021)  amp_scale: 1.0000 (1.0000)  time: 1.3736  data: 0.7854  max mem: 13797
[19:26:40.500084] [Train][Ep-3/100]  [1100/1435]  eta: 0:07:23  lr: 0.0001 (0.0001)  loss: 1731.6711 (1801.5296)  grad_norm: 834.9109 (827.2241)  amp_scale: 1.0000 (1.0000)  time: 1.3999  data: 0.8116  max mem: 13797
[19:29:02.865998] [Train][Ep-3/100]  [1200/1435]  eta: 0:05:13  lr: 0.0001 (0.0001)  loss: 1773.8164 (1799.7408)  grad_norm: 827.9187 (829.0617)  amp_scale: 1.0000 (1.0000)  time: 1.4166  data: 0.8301  max mem: 13797
[19:31:14.471440] [Train][Ep-3/100]  [1300/1435]  eta: 0:02:59  lr: 0.0001 (0.0001)  loss: 1783.9531 (1800.8225)  grad_norm: 854.8282 (832.9449)  amp_scale: 1.0000 (1.0000)  time: 1.3369  data: 0.2374  max mem: 13797
[19:33:32.444185] [Train][Ep-3/100]  [1400/1435]  eta: 0:00:46  lr: 0.0001 (0.0001)  loss: 1719.5101 (1796.5232)  grad_norm: 830.5724 (847.9565)  amp_scale: 1.0000 (1.0000)  time: 1.3525  data: 0.1747  max mem: 13797
[19:34:16.166558] [Train][Ep-3/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1719.5101 (1795.2856)  grad_norm: 858.1581 (849.0083)  amp_scale: 1.0000 (1.0000)  time: 1.3227  data: 0.1447  max mem: 13797
[19:34:16.168083] [Train][Ep-3/100] Total time: 0:31:53 (1.3332 s / it)
[19:34:16.168806] Syncing meters...
[19:34:17.509590] Averaged stats: lr: 0.0001 (0.0001)  loss: 1719.5101 (1799.5878)  grad_norm: 858.1581 (849.0083)  amp_scale: 1.0000 (1.0000)
[19:34:27.865262] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 4)
[19:34:29.977982] [Train][Ep-4/100]  [   0/1435]  eta: 0:46:38  lr: 0.0001 (0.0001)  time: 1.9499  data: 1.4505  max mem: 13797
[19:36:35.400762] [Train][Ep-4/100]  [ 100/1435]  eta: 0:28:03  lr: 0.0001 (0.0001)  loss: 1744.3540 (1764.6188)  grad_norm: 845.0632 (846.3353)  amp_scale: 1.0000 (1.0000)  time: 1.2791  data: 0.6973  max mem: 13797
[19:38:45.831563] [Train][Ep-4/100]  [ 200/1435]  eta: 0:26:23  lr: 0.0001 (0.0001)  loss: 1743.1274 (1745.6426)  grad_norm: 855.0924 (849.1137)  amp_scale: 1.0000 (1.0000)  time: 1.2888  data: 0.5840  max mem: 13797
[19:40:57.044219] [Train][Ep-4/100]  [ 300/1435]  eta: 0:24:26  lr: 0.0001 (0.0001)  loss: 1694.8195 (1735.4472)  grad_norm: 838.6198 (849.2345)  amp_scale: 1.0000 (1.0000)  time: 1.2877  data: 0.4288  max mem: 13797
[19:43:06.025336] [Train][Ep-4/100]  [ 400/1435]  eta: 0:22:16  lr: 0.0001 (0.0001)  loss: 1699.8885 (1735.1211)  grad_norm: 866.3149 (854.8824)  amp_scale: 1.0000 (1.0000)  time: 1.2804  data: 0.7018  max mem: 13797
[19:45:13.901335] [Train][Ep-4/100]  [ 500/1435]  eta: 0:20:05  lr: 0.0001 (0.0001)  loss: 1706.7065 (1732.5678)  grad_norm: 843.3374 (854.9502)  amp_scale: 1.0000 (1.0000)  time: 1.2357  data: 0.5218  max mem: 13797
[19:47:23.224357] [Train][Ep-4/100]  [ 600/1435]  eta: 0:17:56  lr: 0.0001 (0.0001)  loss: 1741.0039 (1735.6253)  grad_norm: 868.2667 (858.0682)  amp_scale: 1.0000 (1.0000)  time: 1.3615  data: 0.7694  max mem: 13797
[19:49:40.480547] [Train][Ep-4/100]  [ 700/1435]  eta: 0:15:56  lr: 0.0001 (0.0001)  loss: 1725.0264 (1732.8806)  grad_norm: 869.2966 (859.7017)  amp_scale: 1.0000 (1.0000)  time: 1.3265  data: 0.7399  max mem: 13797
[19:51:55.896120] [Train][Ep-4/100]  [ 800/1435]  eta: 0:13:50  lr: 0.0001 (0.0001)  loss: 1696.9565 (1728.2432)  grad_norm: 843.9393 (860.0552)  amp_scale: 1.0000 (1.0000)  time: 1.3141  data: 0.7338  max mem: 13797
[19:54:13.569981] [Train][Ep-4/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0001 (0.0001)  loss: 1658.4430 (1722.5727)  grad_norm: 850.3213 (860.5938)  amp_scale: 1.0000 (1.0000)  time: 1.2816  data: 0.6985  max mem: 13797
[19:56:27.853350] [Train][Ep-4/100]  [1000/1435]  eta: 0:09:33  lr: 0.0001 (0.0001)  loss: 1701.4254 (1720.7358)  grad_norm: 849.3491 (860.3678)  amp_scale: 1.0000 (1.0000)  time: 1.3969  data: 0.8087  max mem: 13797
[19:58:44.603460] [Train][Ep-4/100]  [1100/1435]  eta: 0:07:23  lr: 0.0001 (0.0001)  loss: 1671.8260 (1717.6425)  grad_norm: 850.7293 (859.7038)  amp_scale: 1.0000 (1.0000)  time: 1.3832  data: 0.7954  max mem: 13797
[20:01:07.883414] [Train][Ep-4/100]  [1200/1435]  eta: 0:05:13  lr: 0.0001 (0.0001)  loss: 1730.8037 (1714.9554)  grad_norm: 860.0585 (861.7181)  amp_scale: 1.0000 (1.0000)  time: 1.4465  data: 0.8712  max mem: 13797
[20:03:27.546129] [Train][Ep-4/100]  [1300/1435]  eta: 0:03:00  lr: 0.0001 (0.0001)  loss: 1679.7411 (1714.5982)  grad_norm: 865.0037 (862.0599)  amp_scale: 1.0000 (1.0000)  time: 1.4257  data: 0.4630  max mem: 13797
[20:05:50.684775] [Train][Ep-4/100]  [1400/1435]  eta: 0:00:47  lr: 0.0001 (0.0001)  loss: 1688.2064 (1712.4333)  grad_norm: 839.6976 (860.7927)  amp_scale: 1.0000 (1.0000)  time: 1.4684  data: 0.1128  max mem: 13797
[20:06:39.267047] [Train][Ep-4/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1709.9181 (1712.5275)  grad_norm: 830.9955 (861.1262)  amp_scale: 1.0000 (1.0000)  time: 1.4096  data: 0.0233  max mem: 13797
[20:06:39.267972] [Train][Ep-4/100] Total time: 0:32:11 (1.3458 s / it)
[20:06:39.268397] Syncing meters...
[20:06:40.211619] Averaged stats: lr: 0.0001 (0.0001)  loss: 1709.9181 (1711.5283)  grad_norm: 830.9955 (861.1262)  amp_scale: 1.0000 (1.0000)
[20:06:50.165181] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 5)
[20:06:52.377575] [Train][Ep-5/100]  [   0/1435]  eta: 0:49:06  lr: 0.0002 (0.0001)  time: 2.0536  data: 1.5576  max mem: 13797
[20:09:03.562643] [Train][Ep-5/100]  [ 100/1435]  eta: 0:29:21  lr: 0.0002 (0.0002)  loss: 1661.7213 (1675.0520)  grad_norm: 870.0848 (888.9945)  amp_scale: 1.0000 (1.0000)  time: 1.2655  data: 0.6367  max mem: 13797
[20:11:10.423450] [Train][Ep-5/100]  [ 200/1435]  eta: 0:26:38  lr: 0.0002 (0.0002)  loss: 1694.7885 (1676.5252)  grad_norm: 843.4988 (881.7823)  amp_scale: 1.0000 (1.0000)  time: 1.3209  data: 0.5237  max mem: 13797
[20:13:19.287873] [Train][Ep-5/100]  [ 300/1435]  eta: 0:24:26  lr: 0.0002 (0.0002)  loss: 1625.1208 (1674.5304)  grad_norm: 859.8408 (877.0011)  amp_scale: 1.0000 (1.0000)  time: 1.2423  data: 0.6594  max mem: 13797
[20:15:24.659308] [Train][Ep-5/100]  [ 400/1435]  eta: 0:22:07  lr: 0.0002 (0.0002)  loss: 1628.6680 (1669.5105)  grad_norm: 890.6827 (882.0047)  amp_scale: 1.0000 (1.0000)  time: 1.2506  data: 0.3413  max mem: 13797
[20:17:40.642846] [Train][Ep-5/100]  [ 500/1435]  eta: 0:20:13  lr: 0.0002 (0.0002)  loss: 1674.5303 (1668.6741)  grad_norm: 864.6095 (895.6273)  amp_scale: 1.0000 (1.0000)  time: 1.3945  data: 0.0004  max mem: 13797
[20:19:59.089767] [Train][Ep-5/100]  [ 600/1435]  eta: 0:18:15  lr: 0.0002 (0.0002)  loss: 1625.0154 (1667.3353)  grad_norm: 840.6566 (887.7029)  amp_scale: 1.0000 (1.0000)  time: 1.3459  data: 0.1088  max mem: 13797
[20:22:12.810713] [Train][Ep-5/100]  [ 700/1435]  eta: 0:16:07  lr: 0.0002 (0.0002)  loss: 1611.4240 (1664.1928)  grad_norm: 857.4145 (884.5384)  amp_scale: 1.0000 (1.0000)  time: 1.2810  data: 0.4582  max mem: 13797
[20:24:22.827609] [Train][Ep-5/100]  [ 800/1435]  eta: 0:13:54  lr: 0.0002 (0.0002)  loss: 1611.3385 (1658.2884)  grad_norm: 860.8411 (882.6367)  amp_scale: 1.0000 (1.0000)  time: 1.2751  data: 0.3714  max mem: 13797
[20:26:35.191617] [Train][Ep-5/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0002 (0.0002)  loss: 1687.9884 (1658.6803)  grad_norm: 827.5122 (880.9806)  amp_scale: 1.0000 (1.0000)  time: 1.3926  data: 0.0003  max mem: 13797
[20:28:52.172460] [Train][Ep-5/100]  [1000/1435]  eta: 0:09:34  lr: 0.0002 (0.0002)  loss: 1681.5726 (1660.5803)  grad_norm: 858.1031 (879.1761)  amp_scale: 1.0000 (1.0000)  time: 1.3359  data: 0.1723  max mem: 13797
[20:31:02.848799] [Train][Ep-5/100]  [1100/1435]  eta: 0:07:21  lr: 0.0002 (0.0002)  loss: 1637.1274 (1660.1399)  grad_norm: 866.9827 (881.9162)  amp_scale: 1.0000 (1.0000)  time: 1.2808  data: 0.6113  max mem: 13797
[20:33:16.186358] [Train][Ep-5/100]  [1200/1435]  eta: 0:05:10  lr: 0.0002 (0.0002)  loss: 1621.7693 (1657.7301)  grad_norm: 888.5745 (884.9410)  amp_scale: 1.0000 (1.0000)  time: 1.3780  data: 0.6377  max mem: 13797
[20:35:38.931712] [Train][Ep-5/100]  [1300/1435]  eta: 0:02:59  lr: 0.0002 (0.0002)  loss: 1603.4192 (1653.5344)  grad_norm: 873.0604 (884.9907)  amp_scale: 1.0000 (1.0000)  time: 1.4009  data: 0.8149  max mem: 13797
[20:37:57.040809] [Train][Ep-5/100]  [1400/1435]  eta: 0:00:46  lr: 0.0002 (0.0002)  loss: 1615.6342 (1651.8905)  grad_norm: 870.6663 (883.3290)  amp_scale: 1.0000 (1.0000)  time: 1.3945  data: 0.8124  max mem: 13797
[20:38:44.059628] [Train][Ep-5/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1609.6312 (1651.5503)  grad_norm: 866.7414 (882.6968)  amp_scale: 1.0000 (1.0000)  time: 1.3542  data: 0.7788  max mem: 13797
[20:38:44.060640] [Train][Ep-5/100] Total time: 0:31:53 (1.3336 s / it)
[20:38:44.061180] Syncing meters...
[20:38:44.062996] Averaged stats: lr: 0.0002 (0.0002)  loss: 1609.6312 (1655.0409)  grad_norm: 866.7414 (882.6968)  amp_scale: 1.0000 (1.0000)
[20:38:54.869369] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 6)
[20:38:57.103761] [Train][Ep-6/100]  [   0/1435]  eta: 0:48:07  lr: 0.0002 (0.0002)  time: 2.0124  data: 1.5150  max mem: 13797
[20:41:09.697177] [Train][Ep-6/100]  [ 100/1435]  eta: 0:29:39  lr: 0.0002 (0.0002)  loss: 1608.9463 (1616.3043)  grad_norm: 870.5626 (860.0313)  amp_scale: 1.0000 (1.0000)  time: 1.2458  data: 0.6660  max mem: 13797
[20:43:20.951813] [Train][Ep-6/100]  [ 200/1435]  eta: 0:27:13  lr: 0.0002 (0.0002)  loss: 1630.2224 (1632.3228)  grad_norm: 870.2109 (867.2117)  amp_scale: 1.0000 (1.0000)  time: 1.3466  data: 0.7654  max mem: 13797
[20:45:34.696625] [Train][Ep-6/100]  [ 300/1435]  eta: 0:25:06  lr: 0.0002 (0.0002)  loss: 1549.1390 (1625.0228)  grad_norm: 856.2421 (865.7586)  amp_scale: 1.0000 (1.0000)  time: 1.3540  data: 0.7600  max mem: 13797
[20:47:53.435006] [Train][Ep-6/100]  [ 400/1435]  eta: 0:23:09  lr: 0.0002 (0.0002)  loss: 1590.4266 (1617.0090)  grad_norm: 808.7647 (855.3923)  amp_scale: 1.0000 (1.0000)  time: 1.3988  data: 0.8154  max mem: 13797
[20:50:04.965677] [Train][Ep-6/100]  [ 500/1435]  eta: 0:20:50  lr: 0.0002 (0.0002)  loss: 1588.6852 (1609.7932)  grad_norm: 859.1080 (855.3122)  amp_scale: 1.0000 (1.0000)  time: 1.2426  data: 0.6567  max mem: 13797
[20:52:16.216699] [Train][Ep-6/100]  [ 600/1435]  eta: 0:18:32  lr: 0.0002 (0.0002)  loss: 1610.1721 (1611.6043)  grad_norm: 868.4265 (856.3449)  amp_scale: 1.0000 (1.0000)  time: 1.3350  data: 0.7464  max mem: 13797
[20:54:26.326511] [Train][Ep-6/100]  [ 700/1435]  eta: 0:16:16  lr: 0.0002 (0.0002)  loss: 1592.0369 (1613.7214)  grad_norm: 852.1135 (856.7879)  amp_scale: 1.0000 (1.0000)  time: 1.3461  data: 0.7649  max mem: 13797
[20:56:35.971077] [Train][Ep-6/100]  [ 800/1435]  eta: 0:14:00  lr: 0.0002 (0.0002)  loss: 1579.0869 (1608.5667)  grad_norm: 833.2542 (856.5039)  amp_scale: 1.0000 (1.0000)  time: 1.2407  data: 0.6534  max mem: 13797
[20:58:43.748044] [Train][Ep-6/100]  [ 900/1435]  eta: 0:11:45  lr: 0.0002 (0.0002)  loss: 1550.6285 (1606.6166)  grad_norm: 852.6778 (856.6488)  amp_scale: 1.0000 (1.0000)  time: 1.2581  data: 0.6672  max mem: 13797
[21:00:58.322252] [Train][Ep-6/100]  [1000/1435]  eta: 0:09:34  lr: 0.0002 (0.0002)  loss: 1578.1691 (1606.9511)  grad_norm: 851.4530 (858.2978)  amp_scale: 1.0000 (1.0000)  time: 1.3493  data: 0.7679  max mem: 13797
[21:03:16.436287] [Train][Ep-6/100]  [1100/1435]  eta: 0:07:24  lr: 0.0002 (0.0002)  loss: 1584.9519 (1605.7468)  grad_norm: 831.0497 (856.9344)  amp_scale: 1.0000 (1.0000)  time: 1.3815  data: 0.7918  max mem: 13797
[21:05:29.090792] [Train][Ep-6/100]  [1200/1435]  eta: 0:05:11  lr: 0.0002 (0.0002)  loss: 1573.1707 (1603.7625)  grad_norm: 838.1979 (857.6576)  amp_scale: 1.0000 (1.0000)  time: 1.3424  data: 0.7534  max mem: 13797
[21:07:48.348663] [Train][Ep-6/100]  [1300/1435]  eta: 0:02:59  lr: 0.0002 (0.0002)  loss: 1587.6283 (1604.1629)  grad_norm: 832.6609 (856.5289)  amp_scale: 1.0000 (1.0000)  time: 1.3589  data: 0.7668  max mem: 13797
[21:10:01.663602] [Train][Ep-6/100]  [1400/1435]  eta: 0:00:46  lr: 0.0002 (0.0002)  loss: 1546.5510 (1602.2226)  grad_norm: 831.3867 (855.4180)  amp_scale: 1.0000 (1.0000)  time: 1.3400  data: 0.2113  max mem: 13797
[21:10:47.528602] [Train][Ep-6/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1597.3110 (1602.5673)  grad_norm: 844.4983 (855.5456)  amp_scale: 1.0000 (1.0000)  time: 1.4103  data: 0.1910  max mem: 13797
[21:10:47.529653] [Train][Ep-6/100] Total time: 0:31:52 (1.3327 s / it)
[21:10:47.530112] Syncing meters...
[21:10:48.172962] Averaged stats: lr: 0.0002 (0.0002)  loss: 1597.3110 (1605.6548)  grad_norm: 844.4983 (855.5456)  amp_scale: 1.0000 (1.0000)
[21:10:58.473476] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 7)
[21:11:00.632107] [Train][Ep-7/100]  [   0/1435]  eta: 0:46:20  lr: 0.0002 (0.0002)  time: 1.9374  data: 1.4405  max mem: 13797
[21:13:16.178128] [Train][Ep-7/100]  [ 100/1435]  eta: 0:30:17  lr: 0.0002 (0.0002)  loss: 1595.1565 (1591.7872)  grad_norm: 825.0665 (831.6121)  amp_scale: 1.0000 (1.0000)  time: 1.3470  data: 0.7680  max mem: 13797
[21:15:25.897791] [Train][Ep-7/100]  [ 200/1435]  eta: 0:27:21  lr: 0.0002 (0.0002)  loss: 1600.7518 (1593.1660)  grad_norm: 860.1097 (843.9537)  amp_scale: 1.0000 (1.0000)  time: 1.3515  data: 0.7687  max mem: 13797
[21:17:42.470296] [Train][Ep-7/100]  [ 300/1435]  eta: 0:25:22  lr: 0.0002 (0.0002)  loss: 1617.6787 (1593.2804)  grad_norm: 852.8697 (849.2975)  amp_scale: 1.0000 (1.0000)  time: 1.3923  data: 0.8118  max mem: 13797
[21:19:55.691530] [Train][Ep-7/100]  [ 400/1435]  eta: 0:23:05  lr: 0.0002 (0.0002)  loss: 1591.9789 (1594.2868)  grad_norm: 835.7380 (848.5248)  amp_scale: 1.0000 (1.0000)  time: 1.1716  data: 0.5939  max mem: 13797
[21:22:00.395114] [Train][Ep-7/100]  [ 500/1435]  eta: 0:20:34  lr: 0.0002 (0.0002)  loss: 1522.1250 (1589.7758)  grad_norm: 842.4583 (849.2100)  amp_scale: 1.0000 (1.0000)  time: 1.2302  data: 0.1748  max mem: 13797
[21:24:14.171465] [Train][Ep-7/100]  [ 600/1435]  eta: 0:18:25  lr: 0.0002 (0.0002)  loss: 1522.6104 (1586.2065)  grad_norm: 829.2107 (847.0052)  amp_scale: 1.0000 (1.0000)  time: 1.3644  data: 0.0010  max mem: 13797
[21:26:24.323512] [Train][Ep-7/100]  [ 700/1435]  eta: 0:16:10  lr: 0.0002 (0.0002)  loss: 1558.9675 (1584.9719)  grad_norm: 818.0228 (844.2103)  amp_scale: 1.0000 (1.0000)  time: 1.2604  data: 0.4675  max mem: 13797
[21:28:34.610125] [Train][Ep-7/100]  [ 800/1435]  eta: 0:13:57  lr: 0.0002 (0.0002)  loss: 1555.0106 (1582.0301)  grad_norm: 831.3595 (842.8931)  amp_scale: 1.0000 (1.0000)  time: 1.2751  data: 0.1364  max mem: 13797
[21:30:48.879989] [Train][Ep-7/100]  [ 900/1435]  eta: 0:11:46  lr: 0.0002 (0.0002)  loss: 1577.4893 (1578.9632)  grad_norm: 845.0468 (843.2484)  amp_scale: 1.0000 (1.0000)  time: 1.2828  data: 0.7017  max mem: 13797
[21:32:57.543527] [Train][Ep-7/100]  [1000/1435]  eta: 0:09:33  lr: 0.0002 (0.0002)  loss: 1539.2314 (1578.4464)  grad_norm: 827.9534 (842.9237)  amp_scale: 1.0000 (1.0000)  time: 1.2540  data: 0.6091  max mem: 13797
[21:35:05.896860] [Train][Ep-7/100]  [1100/1435]  eta: 0:07:20  lr: 0.0002 (0.0002)  loss: 1555.7271 (1577.0661)  grad_norm: 848.4616 (842.3795)  amp_scale: 1.0000 (1.0000)  time: 1.3470  data: 0.7608  max mem: 13797
[21:37:17.614010] [Train][Ep-7/100]  [1200/1435]  eta: 0:05:08  lr: 0.0002 (0.0002)  loss: 1569.8357 (1576.9631)  grad_norm: 811.4264 (840.9543)  amp_scale: 1.0000 (1.0000)  time: 1.2769  data: 0.6754  max mem: 13797
[21:39:31.133770] [Train][Ep-7/100]  [1300/1435]  eta: 0:02:57  lr: 0.0002 (0.0002)  loss: 1496.3954 (1573.4547)  grad_norm: 844.7869 (841.4429)  amp_scale: 1.0000 (1.0000)  time: 1.3431  data: 0.7641  max mem: 13797
[21:41:45.520016] [Train][Ep-7/100]  [1400/1435]  eta: 0:00:46  lr: 0.0002 (0.0002)  loss: 1535.9183 (1571.1856)  grad_norm: 813.5344 (840.0576)  amp_scale: 1.0000 (1.0000)  time: 1.4097  data: 0.8318  max mem: 13797
[21:42:28.388334] [Train][Ep-7/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1535.9183 (1570.1893)  grad_norm: 813.1293 (839.6516)  amp_scale: 1.0000 (1.0000)  time: 1.2178  data: 0.6385  max mem: 13797
[21:42:28.389324] [Train][Ep-7/100] Total time: 0:31:29 (1.3169 s / it)
[21:42:28.389819] Syncing meters...
[21:42:28.391672] Averaged stats: lr: 0.0002 (0.0002)  loss: 1535.9183 (1575.9066)  grad_norm: 813.1293 (839.6516)  amp_scale: 1.0000 (1.0000)
[21:42:38.899727] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 8)
[21:42:41.135125] [Train][Ep-8/100]  [   0/1435]  eta: 0:47:43  lr: 0.0002 (0.0002)  time: 1.9957  data: 1.4955  max mem: 13797
[21:44:50.838204] [Train][Ep-8/100]  [ 100/1435]  eta: 0:29:00  lr: 0.0002 (0.0002)  loss: 1600.4971 (1552.5146)  grad_norm: 833.1833 (833.6932)  amp_scale: 1.0000 (1.0000)  time: 1.2887  data: 0.7013  max mem: 13797
[21:47:04.704029] [Train][Ep-8/100]  [ 200/1435]  eta: 0:27:11  lr: 0.0002 (0.0002)  loss: 1570.0242 (1566.6745)  grad_norm: 837.3281 (836.9842)  amp_scale: 1.0000 (1.0000)  time: 1.3634  data: 0.7815  max mem: 13797
[21:49:14.728637] [Train][Ep-8/100]  [ 300/1435]  eta: 0:24:51  lr: 0.0002 (0.0002)  loss: 1513.7467 (1555.4343)  grad_norm: 825.6516 (835.8273)  amp_scale: 1.0000 (1.0000)  time: 1.3146  data: 0.7321  max mem: 13797
[21:51:24.460184] [Train][Ep-8/100]  [ 400/1435]  eta: 0:22:35  lr: 0.0002 (0.0002)  loss: 1550.5277 (1561.2277)  grad_norm: 823.5449 (833.5847)  amp_scale: 1.0000 (1.0000)  time: 1.2628  data: 0.6758  max mem: 13797
[21:53:34.027928] [Train][Ep-8/100]  [ 500/1435]  eta: 0:20:22  lr: 0.0002 (0.0002)  loss: 1529.5055 (1560.1386)  grad_norm: 823.2613 (831.4342)  amp_scale: 1.0000 (1.0000)  time: 1.4288  data: 0.8420  max mem: 13797
[21:55:49.719653] [Train][Ep-8/100]  [ 600/1435]  eta: 0:18:18  lr: 0.0003 (0.0002)  loss: 1549.5468 (1562.3878)  grad_norm: 797.2669 (826.7077)  amp_scale: 1.0000 (1.0000)  time: 1.3351  data: 0.7539  max mem: 13797
[21:58:07.652151] [Train][Ep-8/100]  [ 700/1435]  eta: 0:16:13  lr: 0.0003 (0.0002)  loss: 1562.9609 (1561.1845)  grad_norm: 832.8251 (827.7219)  amp_scale: 1.0000 (1.0000)  time: 1.4140  data: 0.8363  max mem: 13797
[22:00:26.929497] [Train][Ep-8/100]  [ 800/1435]  eta: 0:14:06  lr: 0.0003 (0.0002)  loss: 1507.5895 (1557.4960)  grad_norm: 816.2729 (827.8781)  amp_scale: 1.0000 (1.0000)  time: 1.2980  data: 0.7182  max mem: 13797
[22:02:38.467417] [Train][Ep-8/100]  [ 900/1435]  eta: 0:11:52  lr: 0.0003 (0.0002)  loss: 1542.0743 (1560.1305)  grad_norm: 832.5555 (827.7019)  amp_scale: 1.0000 (1.0000)  time: 1.3767  data: 0.7946  max mem: 13797
[22:04:52.902254] [Train][Ep-8/100]  [1000/1435]  eta: 0:09:39  lr: 0.0003 (0.0003)  loss: 1581.3053 (1561.4278)  grad_norm: 821.7076 (837.7140)  amp_scale: 1.0000 (1.0000)  time: 1.3533  data: 0.7014  max mem: 13797
[22:07:12.640268] [Train][Ep-8/100]  [1100/1435]  eta: 0:07:28  lr: 0.0003 (0.0003)  loss: 1562.9794 (1561.9617)  grad_norm: 826.9598 (836.2556)  amp_scale: 1.0000 (1.0000)  time: 1.4036  data: 0.0318  max mem: 13797
[22:09:32.332180] [Train][Ep-8/100]  [1200/1435]  eta: 0:05:15  lr: 0.0003 (0.0003)  loss: 1548.7094 (1560.6461)  grad_norm: 815.1273 (834.8594)  amp_scale: 1.0000 (1.0000)  time: 1.3756  data: 0.7613  max mem: 13797
[22:11:53.608266] [Train][Ep-8/100]  [1300/1435]  eta: 0:03:02  lr: 0.0003 (0.0003)  loss: 1543.7401 (1559.5666)  grad_norm: 819.2936 (833.9528)  amp_scale: 1.0000 (1.0000)  time: 1.4467  data: 0.8657  max mem: 13797
[22:14:12.459190] [Train][Ep-8/100]  [1400/1435]  eta: 0:00:47  lr: 0.0003 (0.0003)  loss: 1551.5583 (1558.1382)  grad_norm: 804.0184 (832.4311)  amp_scale: 1.0000 (1.0000)  time: 1.4212  data: 0.8430  max mem: 13797
[22:14:58.285434] [Train][Ep-8/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1561.1305 (1558.2185)  grad_norm: 817.4329 (832.6579)  amp_scale: 1.0000 (1.0000)  time: 1.2948  data: 0.7183  max mem: 13797
[22:14:58.286463] [Train][Ep-8/100] Total time: 0:32:19 (1.3513 s / it)
[22:14:58.286968] Syncing meters...
[22:14:58.289500] Averaged stats: lr: 0.0003 (0.0003)  loss: 1561.1305 (1550.3884)  grad_norm: 817.4329 (832.6579)  amp_scale: 1.0000 (1.0000)
[22:15:08.279290] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 9)
[22:15:10.445098] [Train][Ep-9/100]  [   0/1435]  eta: 0:47:15  lr: 0.0003 (0.0003)  time: 1.9763  data: 1.4796  max mem: 13797
[22:17:30.303812] [Train][Ep-9/100]  [ 100/1435]  eta: 0:31:14  lr: 0.0003 (0.0003)  loss: 1483.8208 (1497.3707)  grad_norm: 817.2602 (806.6471)  amp_scale: 1.0000 (1.0000)  time: 1.3899  data: 0.8123  max mem: 13797
[22:19:47.377639] [Train][Ep-9/100]  [ 200/1435]  eta: 0:28:33  lr: 0.0003 (0.0003)  loss: 1560.6859 (1514.0921)  grad_norm: 812.4625 (820.5574)  amp_scale: 1.0000 (1.0000)  time: 1.3939  data: 0.8168  max mem: 13797
[22:22:06.814889] [Train][Ep-9/100]  [ 300/1435]  eta: 0:26:17  lr: 0.0003 (0.0003)  loss: 1499.3903 (1517.2149)  grad_norm: 819.7039 (822.5334)  amp_scale: 1.0000 (1.0000)  time: 1.3604  data: 0.7801  max mem: 13797
[22:24:21.274046] [Train][Ep-9/100]  [ 400/1435]  eta: 0:23:46  lr: 0.0003 (0.0003)  loss: 1542.8461 (1520.9857)  grad_norm: 814.7083 (818.3273)  amp_scale: 1.0000 (1.0000)  time: 1.3281  data: 0.7189  max mem: 13797
[22:26:33.455844] [Train][Ep-9/100]  [ 500/1435]  eta: 0:21:18  lr: 0.0003 (0.0003)  loss: 1515.5310 (1526.4701)  grad_norm: 820.8949 (820.3400)  amp_scale: 1.0000 (1.0000)  time: 1.3204  data: 0.7427  max mem: 13797
[22:28:51.377280] [Train][Ep-9/100]  [ 600/1435]  eta: 0:19:03  lr: 0.0003 (0.0003)  loss: 1493.6984 (1527.5136)  grad_norm: 799.5758 (819.1331)  amp_scale: 1.0000 (1.0000)  time: 1.3551  data: 0.7761  max mem: 13797
[22:31:09.590331] [Train][Ep-9/100]  [ 700/1435]  eta: 0:16:47  lr: 0.0003 (0.0003)  loss: 1504.2858 (1526.3126)  grad_norm: 797.6704 (817.5400)  amp_scale: 1.0000 (1.0000)  time: 1.3375  data: 0.7571  max mem: 13797
[22:33:26.813840] [Train][Ep-9/100]  [ 800/1435]  eta: 0:14:30  lr: 0.0003 (0.0003)  loss: 1503.5986 (1523.6110)  grad_norm: 792.5308 (815.6196)  amp_scale: 1.0000 (1.0000)  time: 1.3941  data: 0.0663  max mem: 13797
[22:35:49.754937] [Train][Ep-9/100]  [ 900/1435]  eta: 0:12:17  lr: 0.0003 (0.0003)  loss: 1549.5865 (1529.1197)  grad_norm: 813.4757 (815.4043)  amp_scale: 1.0000 (1.0000)  time: 1.4470  data: 0.3033  max mem: 13797
[22:38:05.164155] [Train][Ep-9/100]  [1000/1435]  eta: 0:09:58  lr: 0.0003 (0.0003)  loss: 1512.8137 (1528.0076)  grad_norm: 813.6260 (814.9946)  amp_scale: 1.0000 (1.0000)  time: 1.3728  data: 0.1472  max mem: 13797
[22:40:20.576705] [Train][Ep-9/100]  [1100/1435]  eta: 0:07:40  lr: 0.0003 (0.0003)  loss: 1517.4004 (1527.1725)  grad_norm: 790.0909 (813.5047)  amp_scale: 1.0000 (1.0000)  time: 1.3571  data: 0.7681  max mem: 13797
[22:42:30.883411] [Train][Ep-9/100]  [1200/1435]  eta: 0:05:21  lr: 0.0003 (0.0003)  loss: 1482.0627 (1525.4932)  grad_norm: 801.1567 (813.6004)  amp_scale: 1.0000 (1.0000)  time: 1.3491  data: 0.2516  max mem: 13797
[22:44:48.771055] [Train][Ep-9/100]  [1300/1435]  eta: 0:03:04  lr: 0.0003 (0.0003)  loss: 1481.6179 (1524.0221)  grad_norm: 783.8645 (812.7317)  amp_scale: 1.0000 (1.0000)  time: 1.4182  data: 0.0913  max mem: 13797
[22:47:06.998437] [Train][Ep-9/100]  [1400/1435]  eta: 0:00:47  lr: 0.0003 (0.0003)  loss: 1459.5619 (1522.1171)  grad_norm: 815.8289 (812.6058)  amp_scale: 1.0000 (1.0000)  time: 1.4159  data: 0.0062  max mem: 13797
[22:47:51.747378] [Train][Ep-9/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1515.8483 (1523.1711)  grad_norm: 806.7057 (812.8140)  amp_scale: 1.0000 (1.0000)  time: 1.3523  data: 0.0291  max mem: 13797
[22:47:51.748542] [Train][Ep-9/100] Total time: 0:32:43 (1.3681 s / it)
[22:47:51.749122] Syncing meters...
[22:47:52.961771] Averaged stats: lr: 0.0003 (0.0003)  loss: 1515.8483 (1527.6240)  grad_norm: 806.7057 (812.8140)  amp_scale: 1.0000 (1.0000)
[22:48:03.502717] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 10)
[22:48:05.687685] [Train][Ep-10/100]  [   0/1435]  eta: 0:47:07  lr: 0.0003 (0.0003)  time: 1.9703  data: 1.4727  max mem: 13797
[22:50:14.742668] [Train][Ep-10/100]  [ 100/1435]  eta: 0:28:51  lr: 0.0003 (0.0003)  loss: 1469.5043 (1532.2267)  grad_norm: 800.1342 (799.0944)  amp_scale: 1.0000 (1.0000)  time: 1.2879  data: 0.7014  max mem: 13797
[22:52:32.553912] [Train][Ep-10/100]  [ 200/1435]  eta: 0:27:31  lr: 0.0003 (0.0003)  loss: 1530.1947 (1522.7334)  grad_norm: 802.7648 (800.3822)  amp_scale: 1.0000 (1.0000)  time: 1.3929  data: 0.8062  max mem: 13797
[22:54:42.749515] [Train][Ep-10/100]  [ 300/1435]  eta: 0:25:04  lr: 0.0003 (0.0003)  loss: 1476.1633 (1506.4385)  grad_norm: 808.3511 (800.9333)  amp_scale: 1.0000 (1.0000)  time: 1.2403  data: 0.6549  max mem: 13797
[22:56:55.277100] [Train][Ep-10/100]  [ 400/1435]  eta: 0:22:51  lr: 0.0003 (0.0003)  loss: 1561.5880 (1513.8581)  grad_norm: 812.8608 (801.1333)  amp_scale: 1.0000 (1.0000)  time: 1.2953  data: 0.7135  max mem: 13797
[22:59:01.334733] [Train][Ep-10/100]  [ 500/1435]  eta: 0:20:27  lr: 0.0003 (0.0003)  loss: 1538.9835 (1517.7010)  grad_norm: 792.1584 (802.8676)  amp_scale: 1.0000 (1.0000)  time: 1.3221  data: 0.6644  max mem: 13797
[23:01:13.546026] [Train][Ep-10/100]  [ 600/1435]  eta: 0:18:17  lr: 0.0003 (0.0003)  loss: 1453.0146 (1513.1164)  grad_norm: 800.8129 (803.3265)  amp_scale: 1.0000 (1.0000)  time: 1.2906  data: 0.7050  max mem: 13797
[23:03:20.190978] [Train][Ep-10/100]  [ 700/1435]  eta: 0:16:00  lr: 0.0003 (0.0003)  loss: 1480.7875 (1508.6759)  grad_norm: 777.4521 (801.0935)  amp_scale: 1.0000 (1.0000)  time: 1.2703  data: 0.4234  max mem: 13797
[23:05:31.109093] [Train][Ep-10/100]  [ 800/1435]  eta: 0:13:50  lr: 0.0003 (0.0003)  loss: 1569.5369 (1511.5839)  grad_norm: 803.6148 (801.6697)  amp_scale: 1.0000 (1.0000)  time: 1.3166  data: 0.1450  max mem: 13797
[23:07:48.556202] [Train][Ep-10/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0003 (0.0003)  loss: 1543.5526 (1516.9377)  grad_norm: 786.1936 (801.6697)  amp_scale: 1.0000 (1.0000)  time: 1.3751  data: 0.0432  max mem: 13797
[23:10:03.533083] [Train][Ep-10/100]  [1000/1435]  eta: 0:09:33  lr: 0.0003 (0.0003)  loss: 1468.5647 (1511.8977)  grad_norm: 810.4762 (801.9307)  amp_scale: 1.0000 (1.0000)  time: 1.3581  data: 0.7519  max mem: 13797
[23:12:13.576736] [Train][Ep-10/100]  [1100/1435]  eta: 0:07:21  lr: 0.0003 (0.0003)  loss: 1419.8425 (1505.4335)  grad_norm: 820.6138 (803.6690)  amp_scale: 1.0000 (1.0000)  time: 1.3486  data: 0.7678  max mem: 13797
[23:14:32.920567] [Train][Ep-10/100]  [1200/1435]  eta: 0:05:10  lr: 0.0003 (0.0003)  loss: 1474.8121 (1505.6423)  grad_norm: 795.6725 (804.0436)  amp_scale: 1.0000 (1.0000)  time: 1.4119  data: 0.8315  max mem: 13797
[23:16:40.045569] [Train][Ep-10/100]  [1300/1435]  eta: 0:02:58  lr: 0.0003 (0.0003)  loss: 1476.8451 (1503.8943)  grad_norm: 794.3943 (803.5224)  amp_scale: 1.0000 (1.0000)  time: 1.1948  data: 0.5522  max mem: 13797
[23:18:46.279793] [Train][Ep-10/100]  [1400/1435]  eta: 0:00:46  lr: 0.0003 (0.0003)  loss: 1492.9884 (1505.4637)  grad_norm: 814.7272 (803.8283)  amp_scale: 1.0000 (1.0000)  time: 1.3034  data: 0.0899  max mem: 13797
[23:19:27.066303] [Train][Ep-10/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1485.0223 (1505.6533)  grad_norm: 801.4257 (803.5077)  amp_scale: 1.0000 (1.0000)  time: 1.1787  data: 0.2526  max mem: 13797
[23:19:27.067494] [Train][Ep-10/100] Total time: 0:31:23 (1.3124 s / it)
[23:19:27.067978] Syncing meters...
[23:19:27.672978] Averaged stats: lr: 0.0003 (0.0003)  loss: 1485.0223 (1507.0339)  grad_norm: 801.4257 (803.5077)  amp_scale: 1.0000 (1.0000)
[23:19:29.738464] [Eval][Ep-10/100]  [  0/121]  eta: 0:04:08  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0549  data: 1.8868  max mem: 13797
[23:21:27.170484] [Eval][Ep-10/100]  [100/121]  eta: 0:00:24  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1494  data: 0.9862  max mem: 13797
[23:21:50.426880] [Eval][Ep-10/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1627  data: 1.0019  max mem: 13797
[23:21:50.427998] [Eval][Ep-10/100] Total time: 0:02:22 (1.1797 s / it)
[23:21:50.972970] [Eval][Ep-10/100] val_acc1_image=24.11 | val_acc1_audio=33.08 | val_acc1_fusion=33.80 | val_acc1_all=44.33
[23:22:01.170934] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 11)
[23:22:03.464600] [Train][Ep-11/100]  [   0/1435]  eta: 0:49:30  lr: 0.0003 (0.0003)  time: 2.0702  data: 1.5797  max mem: 13797
[23:24:21.771248] [Train][Ep-11/100]  [ 100/1435]  eta: 0:30:55  lr: 0.0003 (0.0003)  loss: 1480.6642 (1450.6920)  grad_norm: 789.2831 (801.0831)  amp_scale: 1.0000 (1.0000)  time: 1.4186  data: 0.8408  max mem: 13797
[23:26:39.141233] [Train][Ep-11/100]  [ 200/1435]  eta: 0:28:26  lr: 0.0003 (0.0003)  loss: 1478.6837 (1455.7616)  grad_norm: 791.9073 (799.6209)  amp_scale: 1.0000 (1.0000)  time: 1.3739  data: 0.7842  max mem: 13797
[23:28:51.483468] [Train][Ep-11/100]  [ 300/1435]  eta: 0:25:46  lr: 0.0003 (0.0003)  loss: 1480.3754 (1455.7514)  grad_norm: 797.8846 (800.3301)  amp_scale: 1.0000 (1.0000)  time: 1.3076  data: 0.7039  max mem: 13797
[23:31:03.758698] [Train][Ep-11/100]  [ 400/1435]  eta: 0:23:19  lr: 0.0003 (0.0003)  loss: 1417.0443 (1456.0053)  grad_norm: 787.5522 (798.9151)  amp_scale: 1.0000 (1.0000)  time: 1.3478  data: 0.7675  max mem: 13797
[23:33:16.883896] [Train][Ep-11/100]  [ 500/1435]  eta: 0:21:00  lr: 0.0003 (0.0003)  loss: 1464.0342 (1462.8397)  grad_norm: 796.0375 (799.6438)  amp_scale: 1.0000 (1.0000)  time: 1.3810  data: 0.7462  max mem: 13797
[23:35:23.627448] [Train][Ep-11/100]  [ 600/1435]  eta: 0:18:34  lr: 0.0003 (0.0003)  loss: 1506.1558 (1469.8669)  grad_norm: 806.0685 (804.2216)  amp_scale: 1.0000 (1.0000)  time: 1.2623  data: 0.4301  max mem: 13797
[23:37:33.619403] [Train][Ep-11/100]  [ 700/1435]  eta: 0:16:17  lr: 0.0003 (0.0003)  loss: 1506.3379 (1470.5929)  grad_norm: 776.1223 (800.7289)  amp_scale: 1.0000 (1.0000)  time: 1.2956  data: 0.5498  max mem: 13797
[23:39:46.886931] [Train][Ep-11/100]  [ 800/1435]  eta: 0:14:04  lr: 0.0003 (0.0003)  loss: 1470.0643 (1471.5856)  grad_norm: 791.6164 (801.5641)  amp_scale: 1.0000 (1.0000)  time: 1.2871  data: 0.7036  max mem: 13797
[23:41:57.590343] [Train][Ep-11/100]  [ 900/1435]  eta: 0:11:50  lr: 0.0003 (0.0003)  loss: 1465.8284 (1471.9793)  grad_norm: 792.6722 (800.4383)  amp_scale: 1.0000 (1.0000)  time: 1.2926  data: 0.7134  max mem: 13797
[23:44:16.791148] [Train][Ep-11/100]  [1000/1435]  eta: 0:09:40  lr: 0.0004 (0.0003)  loss: 1515.0237 (1472.7089)  grad_norm: 781.1528 (800.1211)  amp_scale: 1.0000 (1.0000)  time: 1.4024  data: 0.8198  max mem: 13797
[23:46:30.405633] [Train][Ep-11/100]  [1100/1435]  eta: 0:07:26  lr: 0.0004 (0.0003)  loss: 1490.6920 (1473.4370)  grad_norm: 793.8931 (800.1001)  amp_scale: 1.0000 (1.0000)  time: 1.3261  data: 0.7449  max mem: 13797
[23:48:49.870715] [Train][Ep-11/100]  [1200/1435]  eta: 0:05:14  lr: 0.0004 (0.0003)  loss: 1491.7330 (1475.8575)  grad_norm: 798.3542 (800.3577)  amp_scale: 1.0000 (1.0000)  time: 1.3841  data: 0.8056  max mem: 13797
[23:51:02.366241] [Train][Ep-11/100]  [1300/1435]  eta: 0:03:00  lr: 0.0004 (0.0003)  loss: 1514.5818 (1478.1820)  grad_norm: 787.3379 (799.5637)  amp_scale: 1.0000 (1.0000)  time: 1.4259  data: 0.5923  max mem: 13797
[23:53:20.039438] [Train][Ep-11/100]  [1400/1435]  eta: 0:00:46  lr: 0.0004 (0.0003)  loss: 1447.7490 (1477.6705)  grad_norm: 792.0729 (799.0608)  amp_scale: 1.0000 (1.0000)  time: 1.4494  data: 0.8656  max mem: 13797
[23:54:05.501421] [Train][Ep-11/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0003)  loss: 1471.2891 (1477.5968)  grad_norm: 787.4952 (798.7831)  amp_scale: 1.0000 (1.0000)  time: 1.2574  data: 0.6815  max mem: 13797
[23:54:05.502427] [Train][Ep-11/100] Total time: 0:32:04 (1.3408 s / it)
[23:54:05.502885] Syncing meters...
[23:54:05.505016] Averaged stats: lr: 0.0004 (0.0003)  loss: 1471.2891 (1486.6607)  grad_norm: 787.4952 (798.7831)  amp_scale: 1.0000 (1.0000)
[23:54:15.652356] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 12)
[23:54:17.775326] [Train][Ep-12/100]  [   0/1435]  eta: 0:44:38  lr: 0.0004 (0.0004)  time: 1.8664  data: 1.3694  max mem: 13797
[23:56:22.421144] [Train][Ep-12/100]  [ 100/1435]  eta: 0:27:52  lr: 0.0004 (0.0004)  loss: 1486.9908 (1476.8216)  grad_norm: 779.5732 (790.4796)  amp_scale: 1.0000 (1.0000)  time: 1.2592  data: 0.6721  max mem: 13797
[23:58:22.656975] [Train][Ep-12/100]  [ 200/1435]  eta: 0:25:16  lr: 0.0004 (0.0004)  loss: 1423.6693 (1475.7925)  grad_norm: 775.4441 (787.7304)  amp_scale: 1.0000 (1.0000)  time: 1.2580  data: 0.1601  max mem: 13797
[00:00:26.173599] [Train][Ep-12/100]  [ 300/1435]  eta: 0:23:16  lr: 0.0004 (0.0004)  loss: 1423.5754 (1465.8498)  grad_norm: 779.7360 (787.4014)  amp_scale: 1.0000 (1.0000)  time: 1.2426  data: 0.6356  max mem: 13797
[00:02:32.931647] [Train][Ep-12/100]  [ 400/1435]  eta: 0:21:22  lr: 0.0004 (0.0004)  loss: 1421.1903 (1456.4984)  grad_norm: 758.0377 (782.4103)  amp_scale: 1.0000 (1.0000)  time: 1.2451  data: 0.5786  max mem: 13797
[00:04:46.886532] [Train][Ep-12/100]  [ 500/1435]  eta: 0:19:37  lr: 0.0004 (0.0004)  loss: 1467.4938 (1457.4972)  grad_norm: 791.2122 (782.4729)  amp_scale: 1.0000 (1.0000)  time: 1.3839  data: 0.7997  max mem: 13797
[00:07:03.542584] [Train][Ep-12/100]  [ 600/1435]  eta: 0:17:46  lr: 0.0004 (0.0004)  loss: 1489.2032 (1464.6273)  grad_norm: 773.8831 (783.6857)  amp_scale: 1.0000 (1.0000)  time: 1.3930  data: 0.8142  max mem: 13797
[00:09:20.920931] [Train][Ep-12/100]  [ 700/1435]  eta: 0:15:48  lr: 0.0004 (0.0004)  loss: 1462.8462 (1464.6275)  grad_norm: 769.3760 (781.9135)  amp_scale: 1.0000 (1.0000)  time: 1.3450  data: 0.7693  max mem: 13797
[00:11:39.427535] [Train][Ep-12/100]  [ 800/1435]  eta: 0:13:47  lr: 0.0004 (0.0004)  loss: 1483.2925 (1464.0559)  grad_norm: 770.5577 (781.0565)  amp_scale: 1.0000 (1.0000)  time: 1.3841  data: 0.8037  max mem: 13797
[00:13:46.809691] [Train][Ep-12/100]  [ 900/1435]  eta: 0:11:35  lr: 0.0004 (0.0004)  loss: 1488.2970 (1467.6239)  grad_norm: 777.3527 (780.6968)  amp_scale: 1.0000 (1.0000)  time: 1.2955  data: 0.5593  max mem: 13797
[00:15:59.443052] [Train][Ep-12/100]  [1000/1435]  eta: 0:09:26  lr: 0.0004 (0.0004)  loss: 1459.2340 (1468.3752)  grad_norm: 770.4255 (780.0071)  amp_scale: 1.0000 (1.0000)  time: 1.3519  data: 0.7608  max mem: 13797
[00:18:12.780241] [Train][Ep-12/100]  [1100/1435]  eta: 0:07:17  lr: 0.0004 (0.0004)  loss: 1454.4592 (1468.6362)  grad_norm: 781.7685 (780.9901)  amp_scale: 1.0000 (1.0000)  time: 1.3704  data: 0.7827  max mem: 13797
[00:20:25.149822] [Train][Ep-12/100]  [1200/1435]  eta: 0:05:07  lr: 0.0004 (0.0004)  loss: 1454.8328 (1468.3941)  grad_norm: 761.6832 (780.6599)  amp_scale: 1.0000 (1.0000)  time: 1.2862  data: 0.7094  max mem: 13797
[00:22:43.880672] [Train][Ep-12/100]  [1300/1435]  eta: 0:02:57  lr: 0.0004 (0.0004)  loss: 1478.0206 (1469.1255)  grad_norm: 779.4803 (781.0134)  amp_scale: 1.0000 (1.0000)  time: 1.3955  data: 0.8140  max mem: 13797
[00:25:01.812245] [Train][Ep-12/100]  [1400/1435]  eta: 0:00:46  lr: 0.0004 (0.0004)  loss: 1462.6101 (1469.3626)  grad_norm: 765.6675 (780.4730)  amp_scale: 1.0000 (1.0000)  time: 1.3971  data: 0.8020  max mem: 13797
[00:25:45.939675] [Train][Ep-12/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1489.4830 (1469.9349)  grad_norm: 765.6675 (780.3624)  amp_scale: 1.0000 (1.0000)  time: 1.2745  data: 0.6936  max mem: 13797
[00:25:45.941696] [Train][Ep-12/100] Total time: 0:31:30 (1.3171 s / it)
[00:25:45.942250] Syncing meters...
[00:25:45.944617] Averaged stats: lr: 0.0004 (0.0004)  loss: 1489.4830 (1474.5541)  grad_norm: 765.6675 (780.3624)  amp_scale: 1.0000 (1.0000)
[00:25:55.867403] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 13)
[00:25:58.386402] [Train][Ep-13/100]  [   0/1435]  eta: 0:54:33  lr: 0.0004 (0.0004)  time: 2.2811  data: 1.7858  max mem: 13797
[00:28:04.703884] [Train][Ep-13/100]  [ 100/1435]  eta: 0:28:19  lr: 0.0004 (0.0004)  loss: 1448.5522 (1459.5352)  grad_norm: 777.5254 (775.5444)  amp_scale: 1.0000 (1.0000)  time: 1.3405  data: 0.0300  max mem: 13797
[00:30:12.760812] [Train][Ep-13/100]  [ 200/1435]  eta: 0:26:16  lr: 0.0004 (0.0004)  loss: 1441.7863 (1462.4131)  grad_norm: 749.1600 (772.4665)  amp_scale: 1.0000 (1.0000)  time: 1.2826  data: 0.6756  max mem: 13797
[00:32:16.856739] [Train][Ep-13/100]  [ 300/1435]  eta: 0:23:55  lr: 0.0004 (0.0004)  loss: 1436.7119 (1458.2987)  grad_norm: 764.6748 (771.1479)  amp_scale: 1.0000 (1.0000)  time: 1.1816  data: 0.4811  max mem: 13797
[00:34:30.284879] [Train][Ep-13/100]  [ 400/1435]  eta: 0:22:07  lr: 0.0004 (0.0004)  loss: 1462.7413 (1461.4313)  grad_norm: 790.1014 (774.9145)  amp_scale: 1.0000 (1.0000)  time: 1.3806  data: 0.8062  max mem: 13797
[00:36:43.907559] [Train][Ep-13/100]  [ 500/1435]  eta: 0:20:08  lr: 0.0004 (0.0004)  loss: 1425.1670 (1455.8637)  grad_norm: 775.8614 (775.4205)  amp_scale: 1.0000 (1.0000)  time: 1.3805  data: 0.7941  max mem: 13797
[00:39:01.941765] [Train][Ep-13/100]  [ 600/1435]  eta: 0:18:11  lr: 0.0004 (0.0004)  loss: 1469.0808 (1456.4832)  grad_norm: 784.8320 (776.7190)  amp_scale: 1.0000 (1.0000)  time: 1.3522  data: 0.7675  max mem: 13797
[00:41:15.412942] [Train][Ep-13/100]  [ 700/1435]  eta: 0:16:03  lr: 0.0004 (0.0004)  loss: 1474.9546 (1458.5986)  grad_norm: 755.1335 (775.8598)  amp_scale: 1.0000 (1.0000)  time: 1.2937  data: 0.7077  max mem: 13797
[00:43:26.161740] [Train][Ep-13/100]  [ 800/1435]  eta: 0:13:52  lr: 0.0004 (0.0004)  loss: 1428.1952 (1457.3836)  grad_norm: 757.0028 (774.0132)  amp_scale: 1.0000 (1.0000)  time: 1.3566  data: 0.3217  max mem: 13797
[00:45:39.868913] [Train][Ep-13/100]  [ 900/1435]  eta: 0:11:42  lr: 0.0004 (0.0004)  loss: 1486.4186 (1457.7894)  grad_norm: 788.5292 (775.5384)  amp_scale: 1.0000 (1.0000)  time: 1.4025  data: 0.0604  max mem: 13797
[00:47:54.783703] [Train][Ep-13/100]  [1000/1435]  eta: 0:09:33  lr: 0.0004 (0.0004)  loss: 1498.1121 (1461.0224)  grad_norm: 760.5457 (774.6105)  amp_scale: 1.0000 (1.0000)  time: 1.3169  data: 0.3503  max mem: 13797
[00:50:08.343426] [Train][Ep-13/100]  [1100/1435]  eta: 0:07:21  lr: 0.0004 (0.0004)  loss: 1435.4590 (1458.0147)  grad_norm: 761.0690 (773.9834)  amp_scale: 1.0000 (1.0000)  time: 1.4049  data: 0.0859  max mem: 13797
[00:52:26.545609] [Train][Ep-13/100]  [1200/1435]  eta: 0:05:11  lr: 0.0004 (0.0004)  loss: 1436.1447 (1457.7620)  grad_norm: 761.6063 (773.9981)  amp_scale: 1.0000 (1.0000)  time: 1.3425  data: 0.2100  max mem: 13797
[00:54:39.286392] [Train][Ep-13/100]  [1300/1435]  eta: 0:02:58  lr: 0.0004 (0.0004)  loss: 1427.4371 (1459.1674)  grad_norm: 771.9827 (773.8510)  amp_scale: 1.0000 (1.0000)  time: 1.3252  data: 0.6958  max mem: 13797
[00:56:51.279014] [Train][Ep-13/100]  [1400/1435]  eta: 0:00:46  lr: 0.0004 (0.0004)  loss: 1503.9796 (1460.9741)  grad_norm: 752.8845 (774.0842)  amp_scale: 1.0000 (1.0000)  time: 1.3213  data: 0.6225  max mem: 13797
[00:57:35.055928] [Train][Ep-13/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1448.9709 (1458.8254)  grad_norm: 769.8800 (774.1948)  amp_scale: 1.0000 (1.0000)  time: 1.2581  data: 0.6823  max mem: 13797
[00:57:35.056917] [Train][Ep-13/100] Total time: 0:31:38 (1.3233 s / it)
[00:57:35.057412] Syncing meters...
[00:57:35.059498] Averaged stats: lr: 0.0004 (0.0004)  loss: 1448.9709 (1458.7089)  grad_norm: 769.8800 (774.1948)  amp_scale: 1.0000 (1.0000)
[00:57:45.305180] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 14)
[00:57:47.626383] [Train][Ep-14/100]  [   0/1435]  eta: 0:50:24  lr: 0.0004 (0.0004)  time: 2.1077  data: 1.6137  max mem: 13797
[00:59:58.757987] [Train][Ep-14/100]  [ 100/1435]  eta: 0:29:21  lr: 0.0004 (0.0004)  loss: 1459.2133 (1484.0948)  grad_norm: 758.5378 (764.3490)  amp_scale: 1.0000 (1.0000)  time: 1.3217  data: 0.7440  max mem: 13797
[01:02:09.936909] [Train][Ep-14/100]  [ 200/1435]  eta: 0:27:04  lr: 0.0004 (0.0004)  loss: 1453.9556 (1469.9010)  grad_norm: 758.4468 (766.8277)  amp_scale: 1.0000 (1.0000)  time: 1.2625  data: 0.6849  max mem: 13797
[01:04:17.557124] [Train][Ep-14/100]  [ 300/1435]  eta: 0:24:38  lr: 0.0004 (0.0004)  loss: 1457.2058 (1472.0641)  grad_norm: 768.5692 (767.0284)  amp_scale: 1.0000 (1.0000)  time: 1.2913  data: 0.6954  max mem: 13797
[01:06:27.281599] [Train][Ep-14/100]  [ 400/1435]  eta: 0:22:26  lr: 0.0004 (0.0004)  loss: 1464.5310 (1471.6029)  grad_norm: 764.4069 (766.5205)  amp_scale: 1.0000 (1.0000)  time: 1.2693  data: 0.6827  max mem: 13797
[01:08:42.424744] [Train][Ep-14/100]  [ 500/1435]  eta: 0:20:25  lr: 0.0004 (0.0004)  loss: 1392.6154 (1463.0757)  grad_norm: 740.6488 (765.8188)  amp_scale: 1.0000 (1.0000)  time: 1.3919  data: 0.8137  max mem: 13797
[01:10:54.623152] [Train][Ep-14/100]  [ 600/1435]  eta: 0:18:16  lr: 0.0004 (0.0004)  loss: 1431.0671 (1459.4322)  grad_norm: 765.9442 (766.4842)  amp_scale: 1.0000 (1.0000)  time: 1.3630  data: 0.7796  max mem: 13797
[01:13:06.894689] [Train][Ep-14/100]  [ 700/1435]  eta: 0:16:06  lr: 0.0004 (0.0004)  loss: 1403.0675 (1458.2012)  grad_norm: 742.6499 (765.5077)  amp_scale: 1.0000 (1.0000)  time: 1.4118  data: 0.8354  max mem: 13797
[01:15:17.929100] [Train][Ep-14/100]  [ 800/1435]  eta: 0:13:54  lr: 0.0004 (0.0004)  loss: 1444.3302 (1461.8399)  grad_norm: 763.0056 (764.9140)  amp_scale: 1.0000 (1.0000)  time: 1.3572  data: 0.0008  max mem: 13797
[01:17:28.864903] [Train][Ep-14/100]  [ 900/1435]  eta: 0:11:42  lr: 0.0004 (0.0004)  loss: 1505.4658 (1462.4789)  grad_norm: 755.3288 (765.1437)  amp_scale: 1.0000 (1.0000)  time: 1.2915  data: 0.2128  max mem: 13797
[01:19:40.381706] [Train][Ep-14/100]  [1000/1435]  eta: 0:09:31  lr: 0.0004 (0.0004)  loss: 1428.1433 (1458.8576)  grad_norm: 752.2277 (764.4474)  amp_scale: 1.0000 (1.0000)  time: 1.3705  data: 0.0007  max mem: 13797
[01:21:55.496338] [Train][Ep-14/100]  [1100/1435]  eta: 0:07:21  lr: 0.0004 (0.0004)  loss: 1411.4122 (1457.8498)  grad_norm: 744.3776 (763.2128)  amp_scale: 1.0000 (1.0000)  time: 1.3626  data: 0.0012  max mem: 13797
[01:24:11.402270] [Train][Ep-14/100]  [1200/1435]  eta: 0:05:10  lr: 0.0004 (0.0004)  loss: 1481.8119 (1459.4811)  grad_norm: 751.5839 (763.1351)  amp_scale: 1.0000 (1.0000)  time: 1.3539  data: 0.0006  max mem: 13797
[01:26:24.384941] [Train][Ep-14/100]  [1300/1435]  eta: 0:02:58  lr: 0.0004 (0.0004)  loss: 1440.9711 (1459.6355)  grad_norm: 747.6266 (762.6622)  amp_scale: 1.0000 (1.0000)  time: 1.2758  data: 0.0009  max mem: 13797
[01:28:36.373496] [Train][Ep-14/100]  [1400/1435]  eta: 0:00:46  lr: 0.0004 (0.0004)  loss: 1400.2900 (1457.8301)  grad_norm: 735.1891 (761.7464)  amp_scale: 1.0000 (1.0000)  time: 1.3656  data: 0.0005  max mem: 13797
[01:29:19.580638] [Train][Ep-14/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1458.2523 (1458.3550)  grad_norm: 770.1998 (762.0470)  amp_scale: 1.0000 (1.0000)  time: 1.2988  data: 0.0005  max mem: 13797
[01:29:19.581663] [Train][Ep-14/100] Total time: 0:31:34 (1.3199 s / it)
[01:29:19.582130] Syncing meters...
[01:29:20.319162] Averaged stats: lr: 0.0004 (0.0004)  loss: 1458.2523 (1448.9869)  grad_norm: 770.1998 (762.0470)  amp_scale: 1.0000 (1.0000)
[01:29:30.374255] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 15)
[01:29:32.625148] [Train][Ep-15/100]  [   0/1435]  eta: 0:48:24  lr: 0.0004 (0.0004)  time: 2.0243  data: 1.5301  max mem: 13797
[01:31:33.477598] [Train][Ep-15/100]  [ 100/1435]  eta: 0:27:04  lr: 0.0005 (0.0005)  loss: 1428.5638 (1450.9626)  grad_norm: 747.3345 (752.5515)  amp_scale: 1.0000 (1.0000)  time: 1.1921  data: 0.2858  max mem: 13797
[01:33:39.638586] [Train][Ep-15/100]  [ 200/1435]  eta: 0:25:30  lr: 0.0005 (0.0005)  loss: 1420.1003 (1438.0409)  grad_norm: 756.7926 (748.4026)  amp_scale: 1.0000 (1.0000)  time: 1.3797  data: 0.0009  max mem: 13797
[01:35:44.821258] [Train][Ep-15/100]  [ 300/1435]  eta: 0:23:31  lr: 0.0005 (0.0005)  loss: 1392.8402 (1427.7300)  grad_norm: 761.5800 (753.2856)  amp_scale: 1.0000 (1.0000)  time: 1.2539  data: 0.0004  max mem: 13797
[01:37:51.389048] [Train][Ep-15/100]  [ 400/1435]  eta: 0:21:32  lr: 0.0005 (0.0005)  loss: 1411.1982 (1424.0750)  grad_norm: 732.9533 (748.0706)  amp_scale: 1.0000 (1.0000)  time: 1.2949  data: 0.0015  max mem: 13797
[01:40:03.453683] [Train][Ep-15/100]  [ 500/1435]  eta: 0:19:41  lr: 0.0005 (0.0005)  loss: 1399.9766 (1423.2656)  grad_norm: 762.4261 (751.5819)  amp_scale: 1.0000 (1.0000)  time: 1.3314  data: 0.0006  max mem: 13797
[01:42:11.791839] [Train][Ep-15/100]  [ 600/1435]  eta: 0:17:37  lr: 0.0005 (0.0005)  loss: 1428.6804 (1423.7748)  grad_norm: 771.0498 (755.4615)  amp_scale: 1.0000 (1.0000)  time: 1.3069  data: 0.0068  max mem: 13797
[01:44:22.257111] [Train][Ep-15/100]  [ 700/1435]  eta: 0:15:34  lr: 0.0005 (0.0005)  loss: 1455.7061 (1427.5579)  grad_norm: 766.7008 (758.9072)  amp_scale: 1.0000 (1.0000)  time: 1.2875  data: 0.0013  max mem: 13797
[01:46:33.739343] [Train][Ep-15/100]  [ 800/1435]  eta: 0:13:31  lr: 0.0005 (0.0005)  loss: 1444.0903 (1429.4252)  grad_norm: 742.6966 (758.2754)  amp_scale: 1.0000 (1.0000)  time: 1.3777  data: 0.0004  max mem: 13797
[01:48:47.459419] [Train][Ep-15/100]  [ 900/1435]  eta: 0:11:26  lr: 0.0005 (0.0005)  loss: 1431.4078 (1427.9944)  grad_norm: 757.4327 (757.5074)  amp_scale: 1.0000 (1.0000)  time: 1.3563  data: 0.0004  max mem: 13797
[01:51:01.375910] [Train][Ep-15/100]  [1000/1435]  eta: 0:09:20  lr: 0.0005 (0.0005)  loss: 1471.2782 (1430.3077)  grad_norm: 762.5687 (757.8500)  amp_scale: 1.0000 (1.0000)  time: 1.3511  data: 0.0005  max mem: 13797
[01:53:16.218857] [Train][Ep-15/100]  [1100/1435]  eta: 0:07:13  lr: 0.0005 (0.0005)  loss: 1429.8828 (1431.5990)  grad_norm: 735.9945 (756.3351)  amp_scale: 1.0000 (1.0000)  time: 1.3443  data: 0.0002  max mem: 13797
[01:55:29.362610] [Train][Ep-15/100]  [1200/1435]  eta: 0:05:04  lr: 0.0005 (0.0005)  loss: 1459.6127 (1432.4706)  grad_norm: 743.8859 (756.1326)  amp_scale: 1.0000 (1.0000)  time: 1.2924  data: 0.0219  max mem: 13797
[01:57:34.326341] [Train][Ep-15/100]  [1300/1435]  eta: 0:02:54  lr: 0.0005 (0.0005)  loss: 1430.1436 (1433.0323)  grad_norm: 751.9055 (757.4160)  amp_scale: 1.0000 (1.0000)  time: 1.2433  data: 0.0182  max mem: 13797
[01:59:41.190247] [Train][Ep-15/100]  [1400/1435]  eta: 0:00:45  lr: 0.0005 (0.0005)  loss: 1393.7706 (1432.9262)  grad_norm: 764.1927 (757.8821)  amp_scale: 1.0000 (1.0000)  time: 1.2096  data: 0.2398  max mem: 13797
[02:00:20.340624] [Train][Ep-15/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1402.4186 (1432.9012)  grad_norm: 760.6254 (757.5700)  amp_scale: 1.0000 (1.0000)  time: 1.1274  data: 0.4333  max mem: 13797
[02:00:20.341761] [Train][Ep-15/100] Total time: 0:30:49 (1.2890 s / it)
[02:00:20.342309] Syncing meters...
[02:00:20.708797] Averaged stats: lr: 0.0005 (0.0005)  loss: 1402.4186 (1436.2964)  grad_norm: 760.6254 (757.5700)  amp_scale: 1.0000 (1.0000)
[02:00:30.732670] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 16)
[02:00:33.151056] [Train][Ep-16/100]  [   0/1435]  eta: 0:51:16  lr: 0.0005 (0.0005)  time: 2.1436  data: 1.6484  max mem: 13797
[02:02:27.475685] [Train][Ep-16/100]  [ 100/1435]  eta: 0:25:39  lr: 0.0005 (0.0005)  loss: 1391.0245 (1409.7553)  grad_norm: 749.3354 (754.4823)  amp_scale: 1.0000 (1.0000)  time: 1.1318  data: 0.3444  max mem: 13797
[02:04:31.482591] [Train][Ep-16/100]  [ 200/1435]  eta: 0:24:37  lr: 0.0005 (0.0005)  loss: 1431.5873 (1426.5727)  grad_norm: 746.2070 (751.0336)  amp_scale: 1.0000 (1.0000)  time: 1.2684  data: 0.0006  max mem: 13797
[02:06:40.199711] [Train][Ep-16/100]  [ 300/1435]  eta: 0:23:12  lr: 0.0005 (0.0005)  loss: 1415.6621 (1428.6543)  grad_norm: 741.8346 (749.5542)  amp_scale: 1.0000 (1.0000)  time: 1.2971  data: 0.0016  max mem: 13797
[02:08:53.481800] [Train][Ep-16/100]  [ 400/1435]  eta: 0:21:36  lr: 0.0005 (0.0005)  loss: 1411.7665 (1426.5927)  grad_norm: 741.2386 (750.7046)  amp_scale: 1.0000 (1.0000)  time: 1.3479  data: 0.1288  max mem: 13797
[02:10:59.673777] [Train][Ep-16/100]  [ 500/1435]  eta: 0:19:33  lr: 0.0005 (0.0005)  loss: 1381.1244 (1422.9047)  grad_norm: 752.5072 (750.7437)  amp_scale: 1.0000 (1.0000)  time: 1.1876  data: 0.2128  max mem: 13797
[02:13:00.101787] [Train][Ep-16/100]  [ 600/1435]  eta: 0:17:20  lr: 0.0005 (0.0005)  loss: 1388.2472 (1417.9508)  grad_norm: 738.7692 (749.1158)  amp_scale: 1.0000 (1.0000)  time: 1.2893  data: 0.0010  max mem: 13797
[02:15:10.162295] [Train][Ep-16/100]  [ 700/1435]  eta: 0:15:21  lr: 0.0005 (0.0005)  loss: 1455.4939 (1423.2880)  grad_norm: 764.0892 (751.3630)  amp_scale: 1.0000 (1.0000)  time: 1.3702  data: 0.0003  max mem: 13797
[02:17:19.355542] [Train][Ep-16/100]  [ 800/1435]  eta: 0:13:19  lr: 0.0005 (0.0005)  loss: 1440.3643 (1425.7912)  grad_norm: 754.3554 (752.5832)  amp_scale: 1.0000 (1.0000)  time: 1.1938  data: 0.0038  max mem: 13797
[02:19:22.609044] [Train][Ep-16/100]  [ 900/1435]  eta: 0:11:11  lr: 0.0005 (0.0005)  loss: 1459.6355 (1427.8771)  grad_norm: 760.4693 (753.5061)  amp_scale: 1.0000 (1.0000)  time: 1.2186  data: 0.2241  max mem: 13797
[02:21:26.258677] [Train][Ep-16/100]  [1000/1435]  eta: 0:09:05  lr: 0.0005 (0.0005)  loss: 1407.3016 (1429.9448)  grad_norm: 743.2432 (753.0379)  amp_scale: 1.0000 (1.0000)  time: 1.2967  data: 0.7171  max mem: 13797
[02:23:34.525594] [Train][Ep-16/100]  [1100/1435]  eta: 0:07:00  lr: 0.0005 (0.0005)  loss: 1424.4149 (1428.0300)  grad_norm: 730.6454 (752.6220)  amp_scale: 1.0000 (1.0000)  time: 1.3620  data: 0.0007  max mem: 13797
[02:25:48.811248] [Train][Ep-16/100]  [1200/1435]  eta: 0:04:56  lr: 0.0005 (0.0005)  loss: 1385.4115 (1425.2808)  grad_norm: 715.7953 (750.7590)  amp_scale: 1.0000 (1.0000)  time: 1.3785  data: 0.0004  max mem: 13797
[02:28:01.077265] [Train][Ep-16/100]  [1300/1435]  eta: 0:02:51  lr: 0.0005 (0.0005)  loss: 1441.0746 (1424.7020)  grad_norm: 725.0102 (749.4801)  amp_scale: 1.0000 (1.0000)  time: 1.3388  data: 0.0003  max mem: 13797
[02:30:12.746744] [Train][Ep-16/100]  [1400/1435]  eta: 0:00:44  lr: 0.0005 (0.0005)  loss: 1382.7091 (1422.4276)  grad_norm: 740.3203 (749.3692)  amp_scale: 1.0000 (1.0000)  time: 1.2778  data: 0.0005  max mem: 13797
[02:30:54.286171] [Train][Ep-16/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1382.7091 (1422.2919)  grad_norm: 742.7236 (749.3763)  amp_scale: 1.0000 (1.0000)  time: 1.2803  data: 0.0004  max mem: 13797
[02:30:54.287326] [Train][Ep-16/100] Total time: 0:30:23 (1.2706 s / it)
[02:30:54.287774] Syncing meters...
[02:30:55.035026] Averaged stats: lr: 0.0005 (0.0005)  loss: 1382.7091 (1426.9739)  grad_norm: 742.7236 (749.3763)  amp_scale: 1.0000 (1.0000)
[02:31:05.032182] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 17)
[02:31:07.500588] [Train][Ep-17/100]  [   0/1435]  eta: 0:54:10  lr: 0.0005 (0.0005)  time: 2.2654  data: 1.7717  max mem: 13797
[02:33:12.604345] [Train][Ep-17/100]  [ 100/1435]  eta: 0:28:03  lr: 0.0005 (0.0005)  loss: 1367.1855 (1428.8445)  grad_norm: 731.9271 (740.5413)  amp_scale: 1.0000 (1.0000)  time: 1.1584  data: 0.5003  max mem: 13797
[02:35:25.162141] [Train][Ep-17/100]  [ 200/1435]  eta: 0:26:37  lr: 0.0005 (0.0005)  loss: 1361.7229 (1409.8711)  grad_norm: 755.3615 (748.3526)  amp_scale: 1.0000 (1.0000)  time: 1.2604  data: 0.0012  max mem: 13797
[02:37:34.360113] [Train][Ep-17/100]  [ 300/1435]  eta: 0:24:27  lr: 0.0005 (0.0005)  loss: 1434.2981 (1420.1570)  grad_norm: 720.0236 (742.1157)  amp_scale: 1.0000 (1.0000)  time: 1.3027  data: 0.0005  max mem: 13797
[02:39:38.458647] [Train][Ep-17/100]  [ 400/1435]  eta: 0:22:04  lr: 0.0005 (0.0005)  loss: 1427.2863 (1416.9913)  grad_norm: 757.0819 (745.4736)  amp_scale: 1.0000 (1.0000)  time: 1.2641  data: 0.0004  max mem: 13797
[02:41:51.783902] [Train][Ep-17/100]  [ 500/1435]  eta: 0:20:06  lr: 0.0005 (0.0005)  loss: 1370.5842 (1409.6482)  grad_norm: 723.1989 (744.4372)  amp_scale: 1.0000 (1.0000)  time: 1.3054  data: 0.0006  max mem: 13797
[02:44:05.125356] [Train][Ep-17/100]  [ 600/1435]  eta: 0:18:03  lr: 0.0005 (0.0005)  loss: 1431.2524 (1411.2821)  grad_norm: 756.7441 (745.5242)  amp_scale: 1.0000 (1.0000)  time: 1.3524  data: 0.0003  max mem: 13797
[02:46:19.091878] [Train][Ep-17/100]  [ 700/1435]  eta: 0:15:58  lr: 0.0005 (0.0005)  loss: 1423.3590 (1411.9022)  grad_norm: 738.0228 (745.9182)  amp_scale: 1.0000 (1.0000)  time: 1.3546  data: 0.0004  max mem: 13797
[02:48:30.236909] [Train][Ep-17/100]  [ 800/1435]  eta: 0:13:48  lr: 0.0005 (0.0005)  loss: 1455.5017 (1415.5614)  grad_norm: 734.1177 (744.1673)  amp_scale: 1.0000 (1.0000)  time: 1.2949  data: 0.0003  max mem: 13797
[02:50:40.683858] [Train][Ep-17/100]  [ 900/1435]  eta: 0:11:37  lr: 0.0005 (0.0005)  loss: 1416.1338 (1413.3691)  grad_norm: 754.5798 (745.4111)  amp_scale: 1.0000 (1.0000)  time: 1.3064  data: 0.0007  max mem: 13797
[02:52:46.804759] [Train][Ep-17/100]  [1000/1435]  eta: 0:09:25  lr: 0.0005 (0.0005)  loss: 1431.3143 (1415.0377)  grad_norm: 747.1964 (745.5049)  amp_scale: 1.0000 (1.0000)  time: 1.3025  data: 0.0013  max mem: 13797
[02:54:56.611820] [Train][Ep-17/100]  [1100/1435]  eta: 0:07:15  lr: 0.0005 (0.0005)  loss: 1411.6992 (1417.4865)  grad_norm: 758.0740 (747.2499)  amp_scale: 1.0000 (1.0000)  time: 1.3141  data: 0.0006  max mem: 13797
[02:57:06.239285] [Train][Ep-17/100]  [1200/1435]  eta: 0:05:05  lr: 0.0005 (0.0005)  loss: 1456.3604 (1417.6940)  grad_norm: 717.0089 (745.3024)  amp_scale: 1.0000 (1.0000)  time: 1.2880  data: 0.0003  max mem: 13797
[02:59:09.698136] [Train][Ep-17/100]  [1300/1435]  eta: 0:02:54  lr: 0.0005 (0.0005)  loss: 1409.0731 (1418.8781)  grad_norm: 722.1779 (743.4034)  amp_scale: 1.0000 (1.0000)  time: 1.2894  data: 0.0004  max mem: 13797
[03:01:26.835862] [Train][Ep-17/100]  [1400/1435]  eta: 0:00:45  lr: 0.0005 (0.0005)  loss: 1362.7252 (1416.3993)  grad_norm: 719.5458 (743.5853)  amp_scale: 1.0000 (1.0000)  time: 1.3838  data: 0.0006  max mem: 13797
[03:02:08.378219] [Train][Ep-17/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1439.4043 (1416.4290)  grad_norm: 726.6055 (743.1598)  amp_scale: 1.0000 (1.0000)  time: 1.2250  data: 0.0444  max mem: 13797
[03:02:08.379403] [Train][Ep-17/100] Total time: 0:31:03 (1.2984 s / it)
[03:02:08.379888] Syncing meters...
[03:02:08.960785] Averaged stats: lr: 0.0005 (0.0005)  loss: 1439.4043 (1414.5411)  grad_norm: 726.6055 (743.1598)  amp_scale: 1.0000 (1.0000)
[03:02:18.788332] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 18)
[03:02:21.083712] [Train][Ep-18/100]  [   0/1435]  eta: 0:50:21  lr: 0.0005 (0.0005)  time: 2.1054  data: 1.6104  max mem: 13797
[03:04:26.140551] [Train][Ep-18/100]  [ 100/1435]  eta: 0:28:00  lr: 0.0005 (0.0005)  loss: 1397.9489 (1423.9286)  grad_norm: 729.3333 (721.3131)  amp_scale: 1.0000 (1.0000)  time: 1.3166  data: 0.0004  max mem: 13797
[03:06:30.980323] [Train][Ep-18/100]  [ 200/1435]  eta: 0:25:48  lr: 0.0005 (0.0005)  loss: 1394.8976 (1419.7948)  grad_norm: 719.3282 (727.6784)  amp_scale: 1.0000 (1.0000)  time: 1.1985  data: 0.0004  max mem: 13797
[03:08:36.462333] [Train][Ep-18/100]  [ 300/1435]  eta: 0:23:43  lr: 0.0005 (0.0005)  loss: 1400.5665 (1412.7144)  grad_norm: 747.2356 (734.1726)  amp_scale: 1.0000 (1.0000)  time: 1.3102  data: 0.3776  max mem: 13797
[03:10:37.934529] [Train][Ep-18/100]  [ 400/1435]  eta: 0:21:27  lr: 0.0005 (0.0005)  loss: 1396.8915 (1413.5773)  grad_norm: 712.6251 (730.6503)  amp_scale: 1.0000 (1.0000)  time: 1.1914  data: 0.0005  max mem: 13797
[03:12:51.547649] [Train][Ep-18/100]  [ 500/1435]  eta: 0:19:40  lr: 0.0005 (0.0005)  loss: 1382.2980 (1407.3560)  grad_norm: 728.9003 (730.4378)  amp_scale: 1.0000 (1.0000)  time: 1.3677  data: 0.0859  max mem: 13797
[03:14:59.366049] [Train][Ep-18/100]  [ 600/1435]  eta: 0:17:36  lr: 0.0006 (0.0005)  loss: 1412.6493 (1409.8632)  grad_norm: 739.8579 (731.4708)  amp_scale: 1.0000 (1.0000)  time: 1.2655  data: 0.6888  max mem: 13797
[03:17:15.433615] [Train][Ep-18/100]  [ 700/1435]  eta: 0:15:39  lr: 0.0006 (0.0005)  loss: 1437.9230 (1409.6549)  grad_norm: 743.1160 (733.8465)  amp_scale: 1.0000 (1.0000)  time: 1.3576  data: 0.7731  max mem: 13797
[03:19:30.752848] [Train][Ep-18/100]  [ 800/1435]  eta: 0:13:37  lr: 0.0006 (0.0005)  loss: 1459.3867 (1413.1416)  grad_norm: 729.2146 (733.7183)  amp_scale: 1.0000 (1.0000)  time: 1.4098  data: 0.8284  max mem: 13797
[03:21:43.484502] [Train][Ep-18/100]  [ 900/1435]  eta: 0:11:31  lr: 0.0006 (0.0005)  loss: 1375.2551 (1411.3218)  grad_norm: 732.0192 (735.3772)  amp_scale: 1.0000 (1.0000)  time: 1.3369  data: 0.7547  max mem: 13797
[03:23:53.567075] [Train][Ep-18/100]  [1000/1435]  eta: 0:09:22  lr: 0.0006 (0.0006)  loss: 1410.9958 (1412.9575)  grad_norm: 723.1030 (735.7641)  amp_scale: 1.0000 (1.0000)  time: 1.3070  data: 0.7270  max mem: 13797
[03:26:04.727097] [Train][Ep-18/100]  [1100/1435]  eta: 0:07:13  lr: 0.0006 (0.0006)  loss: 1422.0117 (1415.5628)  grad_norm: 728.2025 (736.0245)  amp_scale: 1.0000 (1.0000)  time: 1.2800  data: 0.3122  max mem: 13797
[03:28:17.027474] [Train][Ep-18/100]  [1200/1435]  eta: 0:05:04  lr: 0.0006 (0.0006)  loss: 1401.4232 (1415.9741)  grad_norm: 726.4688 (735.4531)  amp_scale: 1.0000 (1.0000)  time: 1.2882  data: 0.0802  max mem: 13797
[03:30:26.853789] [Train][Ep-18/100]  [1300/1435]  eta: 0:02:55  lr: 0.0006 (0.0006)  loss: 1418.6481 (1417.2735)  grad_norm: 718.6636 (734.1497)  amp_scale: 1.0000 (1.0000)  time: 1.2724  data: 0.3214  max mem: 13797
[03:32:40.007811] [Train][Ep-18/100]  [1400/1435]  eta: 0:00:45  lr: 0.0006 (0.0006)  loss: 1436.9760 (1418.6803)  grad_norm: 734.2667 (734.1536)  amp_scale: 1.0000 (1.0000)  time: 1.3174  data: 0.0918  max mem: 13797
[03:33:24.714654] [Train][Ep-18/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1432.5437 (1418.0472)  grad_norm: 736.6348 (733.6911)  amp_scale: 1.0000 (1.0000)  time: 1.2952  data: 0.3017  max mem: 13797
[03:33:24.715836] [Train][Ep-18/100] Total time: 0:31:05 (1.3002 s / it)
[03:33:24.716388] Syncing meters...
[03:33:24.718124] Averaged stats: lr: 0.0006 (0.0006)  loss: 1432.5437 (1407.4757)  grad_norm: 736.6348 (733.6911)  amp_scale: 1.0000 (1.0000)
[03:33:34.549245] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 19)
[03:33:36.736914] [Train][Ep-19/100]  [   0/1435]  eta: 0:46:10  lr: 0.0006 (0.0006)  time: 1.9307  data: 1.4327  max mem: 13797
[03:35:47.192536] [Train][Ep-19/100]  [ 100/1435]  eta: 0:29:09  lr: 0.0006 (0.0006)  loss: 1390.2919 (1397.5016)  grad_norm: 727.3441 (728.2168)  amp_scale: 1.0000 (1.0000)  time: 1.3943  data: 0.8124  max mem: 13797
[03:37:50.902300] [Train][Ep-19/100]  [ 200/1435]  eta: 0:26:13  lr: 0.0006 (0.0006)  loss: 1406.5328 (1400.3679)  grad_norm: 731.7256 (731.1059)  amp_scale: 1.0000 (1.0000)  time: 1.2289  data: 0.6481  max mem: 13797
[03:39:57.102419] [Train][Ep-19/100]  [ 300/1435]  eta: 0:24:01  lr: 0.0006 (0.0006)  loss: 1382.3007 (1393.9124)  grad_norm: 736.2490 (730.8314)  amp_scale: 1.0000 (1.0000)  time: 1.3042  data: 0.6996  max mem: 13797
[03:42:02.770428] [Train][Ep-19/100]  [ 400/1435]  eta: 0:21:51  lr: 0.0006 (0.0006)  loss: 1353.3004 (1385.6171)  grad_norm: 724.7512 (731.6342)  amp_scale: 1.0000 (1.0000)  time: 1.3097  data: 0.7256  max mem: 13797
[03:44:12.428935] [Train][Ep-19/100]  [ 500/1435]  eta: 0:19:49  lr: 0.0006 (0.0006)  loss: 1410.2859 (1391.1882)  grad_norm: 722.8953 (730.9341)  amp_scale: 1.0000 (1.0000)  time: 1.3903  data: 0.7972  max mem: 13797
[03:46:21.117744] [Train][Ep-19/100]  [ 600/1435]  eta: 0:17:44  lr: 0.0006 (0.0006)  loss: 1368.6398 (1392.9957)  grad_norm: 721.3125 (729.7535)  amp_scale: 1.0000 (1.0000)  time: 1.2598  data: 0.6773  max mem: 13797
[03:48:26.394347] [Train][Ep-19/100]  [ 700/1435]  eta: 0:15:34  lr: 0.0006 (0.0006)  loss: 1405.8175 (1397.4556)  grad_norm: 721.5827 (729.8641)  amp_scale: 1.0000 (1.0000)  time: 1.2277  data: 0.6107  max mem: 13797
[03:50:31.445881] [Train][Ep-19/100]  [ 800/1435]  eta: 0:13:25  lr: 0.0006 (0.0006)  loss: 1420.5616 (1398.5676)  grad_norm: 727.8864 (729.5238)  amp_scale: 1.0000 (1.0000)  time: 1.3233  data: 0.7420  max mem: 13797
[03:52:35.570302] [Train][Ep-19/100]  [ 900/1435]  eta: 0:11:17  lr: 0.0006 (0.0006)  loss: 1446.8239 (1398.4515)  grad_norm: 726.4822 (729.6093)  amp_scale: 1.0000 (1.0000)  time: 1.2999  data: 0.0693  max mem: 13797
[03:54:47.763603] [Train][Ep-19/100]  [1000/1435]  eta: 0:09:13  lr: 0.0006 (0.0006)  loss: 1370.9623 (1396.3597)  grad_norm: 710.9531 (729.2912)  amp_scale: 1.0000 (1.0000)  time: 1.3284  data: 0.7451  max mem: 13797
[03:57:04.073553] [Train][Ep-19/100]  [1100/1435]  eta: 0:07:08  lr: 0.0006 (0.0006)  loss: 1368.9119 (1396.4319)  grad_norm: 726.5286 (728.8261)  amp_scale: 1.0000 (1.0000)  time: 1.3626  data: 0.7780  max mem: 13797
[03:59:13.492338] [Train][Ep-19/100]  [1200/1435]  eta: 0:05:01  lr: 0.0006 (0.0006)  loss: 1355.7499 (1395.7731)  grad_norm: 731.9506 (728.8545)  amp_scale: 1.0000 (1.0000)  time: 1.2475  data: 0.4027  max mem: 13797
[04:01:21.152655] [Train][Ep-19/100]  [1300/1435]  eta: 0:02:52  lr: 0.0006 (0.0006)  loss: 1409.0874 (1394.5756)  grad_norm: 702.8667 (727.4369)  amp_scale: 1.0000 (1.0000)  time: 1.2955  data: 0.4545  max mem: 13797
[04:03:31.894507] [Train][Ep-19/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1415.8630 (1396.1639)  grad_norm: 727.8146 (727.7483)  amp_scale: 1.0000 (1.0000)  time: 1.2765  data: 0.6966  max mem: 13797
[04:04:13.015815] [Train][Ep-19/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1373.5566 (1395.2196)  grad_norm: 726.6154 (727.5049)  amp_scale: 1.0000 (1.0000)  time: 1.1763  data: 0.5185  max mem: 13797
[04:04:13.016698] [Train][Ep-19/100] Total time: 0:30:38 (1.2810 s / it)
[04:04:13.017119] Syncing meters...
[04:04:13.018783] Averaged stats: lr: 0.0006 (0.0006)  loss: 1373.5566 (1401.0591)  grad_norm: 726.6154 (727.5049)  amp_scale: 1.0000 (1.0000)
[04:04:23.243549] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 20)
[04:04:25.871749] [Train][Ep-20/100]  [   0/1435]  eta: 0:57:40  lr: 0.0006 (0.0006)  time: 2.4115  data: 1.9192  max mem: 13797
[04:06:38.615660] [Train][Ep-20/100]  [ 100/1435]  eta: 0:29:46  lr: 0.0006 (0.0006)  loss: 1445.5254 (1432.9531)  grad_norm: 708.3949 (716.0106)  amp_scale: 1.0000 (1.0000)  time: 1.3773  data: 0.7991  max mem: 13797
[04:08:42.126491] [Train][Ep-20/100]  [ 200/1435]  eta: 0:26:29  lr: 0.0006 (0.0006)  loss: 1385.7789 (1416.8543)  grad_norm: 704.3674 (719.8399)  amp_scale: 1.0000 (1.0000)  time: 1.2351  data: 0.6325  max mem: 13797
[04:10:49.710817] [Train][Ep-20/100]  [ 300/1435]  eta: 0:24:16  lr: 0.0006 (0.0006)  loss: 1391.5302 (1411.8911)  grad_norm: 699.7454 (716.7054)  amp_scale: 1.0000 (1.0000)  time: 1.2810  data: 0.6942  max mem: 13797
[04:13:02.967554] [Train][Ep-20/100]  [ 400/1435]  eta: 0:22:20  lr: 0.0006 (0.0006)  loss: 1365.9636 (1401.7453)  grad_norm: 716.6255 (715.7921)  amp_scale: 1.0000 (1.0000)  time: 1.3660  data: 0.7880  max mem: 13797
[04:15:13.853261] [Train][Ep-20/100]  [ 500/1435]  eta: 0:20:13  lr: 0.0006 (0.0006)  loss: 1368.6184 (1395.4924)  grad_norm: 687.4905 (712.4658)  amp_scale: 1.0000 (1.0000)  time: 1.3032  data: 0.7231  max mem: 13797
[04:17:22.670080] [Train][Ep-20/100]  [ 600/1435]  eta: 0:18:02  lr: 0.0006 (0.0006)  loss: 1351.6312 (1394.7872)  grad_norm: 710.3006 (711.6131)  amp_scale: 1.0000 (1.0000)  time: 1.3360  data: 0.7531  max mem: 13797
[04:19:30.714070] [Train][Ep-20/100]  [ 700/1435]  eta: 0:15:51  lr: 0.0006 (0.0006)  loss: 1406.1301 (1400.0719)  grad_norm: 722.2868 (715.0871)  amp_scale: 1.0000 (1.0000)  time: 1.3257  data: 0.7427  max mem: 13797
[04:21:40.291866] [Train][Ep-20/100]  [ 800/1435]  eta: 0:13:41  lr: 0.0006 (0.0006)  loss: 1375.7853 (1399.3889)  grad_norm: 719.8014 (715.9221)  amp_scale: 1.0000 (1.0000)  time: 1.1999  data: 0.6037  max mem: 13797
[04:23:45.647305] [Train][Ep-20/100]  [ 900/1435]  eta: 0:11:30  lr: 0.0006 (0.0006)  loss: 1369.8077 (1396.6663)  grad_norm: 713.6434 (715.8006)  amp_scale: 1.0000 (1.0000)  time: 1.3028  data: 0.7212  max mem: 13797
[04:25:49.677020] [Train][Ep-20/100]  [1000/1435]  eta: 0:09:18  lr: 0.0006 (0.0006)  loss: 1356.3431 (1393.8158)  grad_norm: 719.6962 (717.5213)  amp_scale: 1.0000 (1.0000)  time: 1.2814  data: 0.6445  max mem: 13797
[04:27:58.701787] [Train][Ep-20/100]  [1100/1435]  eta: 0:07:10  lr: 0.0006 (0.0006)  loss: 1409.1044 (1394.2048)  grad_norm: 719.2581 (718.3532)  amp_scale: 1.0000 (1.0000)  time: 1.3511  data: 0.7760  max mem: 13797
[04:30:09.179308] [Train][Ep-20/100]  [1200/1435]  eta: 0:05:02  lr: 0.0006 (0.0006)  loss: 1361.2123 (1393.7912)  grad_norm: 721.8015 (718.7783)  amp_scale: 1.0000 (1.0000)  time: 1.2855  data: 0.2163  max mem: 13797
[04:32:19.339418] [Train][Ep-20/100]  [1300/1435]  eta: 0:02:53  lr: 0.0006 (0.0006)  loss: 1427.4902 (1394.7937)  grad_norm: 707.6220 (718.2731)  amp_scale: 1.0000 (1.0000)  time: 1.2818  data: 0.7042  max mem: 13797
[04:34:32.007466] [Train][Ep-20/100]  [1400/1435]  eta: 0:00:45  lr: 0.0006 (0.0006)  loss: 1337.2194 (1393.0549)  grad_norm: 738.2100 (719.9952)  amp_scale: 1.0000 (1.0000)  time: 1.3193  data: 0.7355  max mem: 13797
[04:35:15.020672] [Train][Ep-20/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1344.9174 (1392.9111)  grad_norm: 731.0849 (719.9273)  amp_scale: 1.0000 (1.0000)  time: 1.2210  data: 0.6460  max mem: 13797
[04:35:15.021936] [Train][Ep-20/100] Total time: 0:30:51 (1.2903 s / it)
[04:35:15.022420] Syncing meters...
[04:35:15.026692] Averaged stats: lr: 0.0006 (0.0006)  loss: 1344.9174 (1389.8253)  grad_norm: 731.0849 (719.9273)  amp_scale: 1.0000 (1.0000)
[04:35:18.019796] [Eval][Ep-20/100]  [  0/121]  eta: 0:06:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.9846  data: 2.8216  max mem: 13797
[04:37:19.281493] [Eval][Ep-20/100]  [100/121]  eta: 0:00:25  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.2155  data: 1.0520  max mem: 13797
[04:37:41.531903] [Eval][Ep-20/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1124  data: 0.9517  max mem: 13797
[04:37:41.533049] [Eval][Ep-20/100] Total time: 0:02:26 (1.2107 s / it)
[04:37:42.104604] [Eval][Ep-20/100] val_acc1_image=24.54 | val_acc1_audio=37.10 | val_acc1_fusion=36.16 | val_acc1_all=47.75
[04:37:51.944133] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 21)
[04:37:54.406810] [Train][Ep-21/100]  [   0/1435]  eta: 0:53:32  lr: 0.0006 (0.0006)  time: 2.2389  data: 1.5296  max mem: 13797
[04:39:58.926082] [Train][Ep-21/100]  [ 100/1435]  eta: 0:27:55  lr: 0.0006 (0.0006)  loss: 1362.0852 (1354.5539)  grad_norm: 714.8815 (717.0743)  amp_scale: 1.0000 (1.0000)  time: 1.2245  data: 0.6494  max mem: 13797
[04:42:03.908822] [Train][Ep-21/100]  [ 200/1435]  eta: 0:25:46  lr: 0.0006 (0.0006)  loss: 1383.4148 (1358.4967)  grad_norm: 705.9896 (716.2302)  amp_scale: 1.0000 (1.0000)  time: 1.2983  data: 0.0007  max mem: 13797
[04:44:11.715993] [Train][Ep-21/100]  [ 300/1435]  eta: 0:23:51  lr: 0.0006 (0.0006)  loss: 1314.9062 (1354.6081)  grad_norm: 706.2369 (713.4600)  amp_scale: 1.0000 (1.0000)  time: 1.2534  data: 0.0008  max mem: 13797
[04:46:15.202636] [Train][Ep-21/100]  [ 400/1435]  eta: 0:21:38  lr: 0.0006 (0.0006)  loss: 1416.4323 (1366.9970)  grad_norm: 711.3734 (712.8344)  amp_scale: 1.0000 (1.0000)  time: 1.1879  data: 0.0003  max mem: 13797
[04:48:21.141011] [Train][Ep-21/100]  [ 500/1435]  eta: 0:19:33  lr: 0.0006 (0.0006)  loss: 1300.7554 (1366.6291)  grad_norm: 722.6025 (716.9729)  amp_scale: 1.0000 (1.0000)  time: 1.2183  data: 0.0144  max mem: 13797
[04:50:28.637083] [Train][Ep-21/100]  [ 600/1435]  eta: 0:17:30  lr: 0.0006 (0.0006)  loss: 1359.3861 (1367.9205)  grad_norm: 717.9526 (716.7845)  amp_scale: 1.0000 (1.0000)  time: 1.3807  data: 0.0005  max mem: 13797
[04:52:39.105333] [Train][Ep-21/100]  [ 700/1435]  eta: 0:15:29  lr: 0.0006 (0.0006)  loss: 1293.6184 (1365.2904)  grad_norm: 705.6800 (716.6540)  amp_scale: 1.0000 (1.0000)  time: 1.2952  data: 0.0006  max mem: 13797
[04:54:45.891677] [Train][Ep-21/100]  [ 800/1435]  eta: 0:13:23  lr: 0.0006 (0.0006)  loss: 1379.7150 (1367.6482)  grad_norm: 720.7465 (715.8690)  amp_scale: 1.0000 (1.0000)  time: 1.2313  data: 0.0006  max mem: 13797
[04:56:54.973866] [Train][Ep-21/100]  [ 900/1435]  eta: 0:11:18  lr: 0.0006 (0.0006)  loss: 1320.9657 (1365.4101)  grad_norm: 722.5251 (714.7385)  amp_scale: 1.0000 (1.0000)  time: 1.3466  data: 0.0005  max mem: 13797
[04:59:08.606249] [Train][Ep-21/100]  [1000/1435]  eta: 0:09:14  lr: 0.0006 (0.0006)  loss: 1331.2731 (1365.3930)  grad_norm: 701.1558 (714.4938)  amp_scale: 1.0000 (1.0000)  time: 1.3438  data: 0.0005  max mem: 13797
[05:01:17.190116] [Train][Ep-21/100]  [1100/1435]  eta: 0:07:07  lr: 0.0006 (0.0006)  loss: 1343.8065 (1367.1756)  grad_norm: 694.3049 (713.6775)  amp_scale: 1.0000 (1.0000)  time: 1.2719  data: 0.0009  max mem: 13797
[05:03:31.496031] [Train][Ep-21/100]  [1200/1435]  eta: 0:05:01  lr: 0.0006 (0.0006)  loss: 1360.1840 (1368.6241)  grad_norm: 724.2335 (714.2938)  amp_scale: 1.0000 (1.0000)  time: 1.3766  data: 0.0002  max mem: 13797
[05:05:47.906586] [Train][Ep-21/100]  [1300/1435]  eta: 0:02:53  lr: 0.0006 (0.0006)  loss: 1389.6849 (1369.9950)  grad_norm: 703.2125 (713.9346)  amp_scale: 1.0000 (1.0000)  time: 1.3518  data: 0.0009  max mem: 13797
[05:08:01.940788] [Train][Ep-21/100]  [1400/1435]  eta: 0:00:45  lr: 0.0006 (0.0006)  loss: 1396.2098 (1370.9024)  grad_norm: 707.6587 (713.4234)  amp_scale: 1.0000 (1.0000)  time: 1.3492  data: 0.0003  max mem: 13797
[05:08:45.569436] [Train][Ep-21/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1428.1954 (1372.9391)  grad_norm: 710.5388 (713.5528)  amp_scale: 1.0000 (1.0000)  time: 1.3034  data: 0.0009  max mem: 13797
[05:08:45.570625] [Train][Ep-21/100] Total time: 0:30:53 (1.2916 s / it)
[05:08:45.571091] Syncing meters...
[05:08:46.240818] Averaged stats: lr: 0.0006 (0.0006)  loss: 1428.1954 (1373.8645)  grad_norm: 710.5388 (713.5528)  amp_scale: 1.0000 (1.0000)
[05:08:55.943795] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 22)
[05:08:58.081757] [Train][Ep-22/100]  [   0/1435]  eta: 0:45:46  lr: 0.0006 (0.0006)  time: 1.9141  data: 1.4208  max mem: 13797
[05:11:03.052245] [Train][Ep-22/100]  [ 100/1435]  eta: 0:27:57  lr: 0.0006 (0.0006)  loss: 1325.2286 (1341.7727)  grad_norm: 703.4701 (719.7605)  amp_scale: 1.0000 (1.0000)  time: 1.3213  data: 0.0007  max mem: 13797
[05:13:13.628047] [Train][Ep-22/100]  [ 200/1435]  eta: 0:26:21  lr: 0.0006 (0.0006)  loss: 1354.3990 (1361.2985)  grad_norm: 697.5756 (706.3909)  amp_scale: 1.0000 (1.0000)  time: 1.3568  data: 0.0013  max mem: 13797
[05:15:14.455260] [Train][Ep-22/100]  [ 300/1435]  eta: 0:23:46  lr: 0.0006 (0.0006)  loss: 1345.5135 (1367.8427)  grad_norm: 704.5162 (705.4077)  amp_scale: 1.0000 (1.0000)  time: 1.1548  data: 0.4749  max mem: 13797
[05:17:14.313234] [Train][Ep-22/100]  [ 400/1435]  eta: 0:21:25  lr: 0.0006 (0.0006)  loss: 1371.3715 (1371.6814)  grad_norm: 701.6584 (706.0540)  amp_scale: 1.0000 (1.0000)  time: 1.2237  data: 0.1615  max mem: 13797
[05:19:19.687973] [Train][Ep-22/100]  [ 500/1435]  eta: 0:19:23  lr: 0.0006 (0.0006)  loss: 1352.1678 (1372.8636)  grad_norm: 705.7733 (706.6286)  amp_scale: 1.0000 (1.0000)  time: 1.2959  data: 0.0169  max mem: 13797
[05:21:28.036596] [Train][Ep-22/100]  [ 600/1435]  eta: 0:17:24  lr: 0.0006 (0.0006)  loss: 1383.2866 (1372.3097)  grad_norm: 697.0007 (706.4550)  amp_scale: 1.0000 (1.0000)  time: 1.2802  data: 0.0013  max mem: 13797
[05:23:28.901100] [Train][Ep-22/100]  [ 700/1435]  eta: 0:15:15  lr: 0.0006 (0.0006)  loss: 1397.1073 (1377.1327)  grad_norm: 689.0888 (706.7839)  amp_scale: 1.0000 (1.0000)  time: 1.2544  data: 0.0005  max mem: 13797
[05:25:36.176070] [Train][Ep-22/100]  [ 800/1435]  eta: 0:13:12  lr: 0.0006 (0.0006)  loss: 1432.1671 (1379.1452)  grad_norm: 702.9324 (707.9891)  amp_scale: 1.0000 (1.0000)  time: 1.2388  data: 0.0794  max mem: 13797
[05:27:45.928756] [Train][Ep-22/100]  [ 900/1435]  eta: 0:11:10  lr: 0.0006 (0.0006)  loss: 1396.1377 (1380.7490)  grad_norm: 698.5825 (706.6247)  amp_scale: 1.0000 (1.0000)  time: 1.2899  data: 0.0012  max mem: 13797
[05:29:52.112726] [Train][Ep-22/100]  [1000/1435]  eta: 0:09:05  lr: 0.0006 (0.0006)  loss: 1365.1222 (1379.7098)  grad_norm: 700.3878 (705.9420)  amp_scale: 1.0000 (1.0000)  time: 1.2615  data: 0.0005  max mem: 13797
[05:31:57.973723] [Train][Ep-22/100]  [1100/1435]  eta: 0:07:00  lr: 0.0006 (0.0006)  loss: 1345.1948 (1376.5128)  grad_norm: 711.0381 (706.2794)  amp_scale: 1.0000 (1.0000)  time: 1.2085  data: 0.0005  max mem: 13797
[05:34:01.707424] [Train][Ep-22/100]  [1200/1435]  eta: 0:04:54  lr: 0.0006 (0.0006)  loss: 1348.5146 (1376.1346)  grad_norm: 718.9502 (706.9516)  amp_scale: 1.0000 (1.0000)  time: 1.2991  data: 0.0004  max mem: 13797
[05:36:12.676756] [Train][Ep-22/100]  [1300/1435]  eta: 0:02:49  lr: 0.0006 (0.0006)  loss: 1407.2798 (1375.5185)  grad_norm: 687.0542 (706.2647)  amp_scale: 1.0000 (1.0000)  time: 1.2772  data: 0.0006  max mem: 13797
[05:38:17.411594] [Train][Ep-22/100]  [1400/1435]  eta: 0:00:43  lr: 0.0006 (0.0006)  loss: 1355.1683 (1374.7853)  grad_norm: 687.2967 (705.5305)  amp_scale: 1.0000 (1.0000)  time: 1.2913  data: 0.0119  max mem: 13797
[05:39:00.231426] [Train][Ep-22/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1418.3615 (1375.2135)  grad_norm: 694.0429 (705.4476)  amp_scale: 1.0000 (1.0000)  time: 1.3422  data: 0.0003  max mem: 13797
[05:39:00.232567] [Train][Ep-22/100] Total time: 0:30:04 (1.2572 s / it)
[05:39:00.233074] Syncing meters...
[05:39:00.419784] Averaged stats: lr: 0.0006 (0.0006)  loss: 1418.3615 (1371.4677)  grad_norm: 694.0429 (705.4476)  amp_scale: 1.0000 (1.0000)
[05:39:10.769608] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 23)
[05:39:12.970275] [Train][Ep-23/100]  [   0/1435]  eta: 0:47:24  lr: 0.0006 (0.0006)  time: 1.9825  data: 1.4887  max mem: 13797
[05:41:13.995244] [Train][Ep-23/100]  [ 100/1435]  eta: 0:27:05  lr: 0.0006 (0.0006)  loss: 1316.0117 (1368.2445)  grad_norm: 698.3636 (699.6944)  amp_scale: 1.0000 (1.0000)  time: 1.3100  data: 0.0004  max mem: 13797
[05:43:17.425900] [Train][Ep-23/100]  [ 200/1435]  eta: 0:25:14  lr: 0.0006 (0.0006)  loss: 1353.5848 (1352.7818)  grad_norm: 705.3793 (705.8751)  amp_scale: 1.0000 (1.0000)  time: 1.1978  data: 0.0013  max mem: 13797
[05:45:26.694827] [Train][Ep-23/100]  [ 300/1435]  eta: 0:23:36  lr: 0.0006 (0.0006)  loss: 1353.8459 (1355.4595)  grad_norm: 695.2051 (703.7555)  amp_scale: 1.0000 (1.0000)  time: 1.3031  data: 0.0241  max mem: 13797
[05:47:36.015120] [Train][Ep-23/100]  [ 400/1435]  eta: 0:21:43  lr: 0.0006 (0.0006)  loss: 1330.2212 (1353.5182)  grad_norm: 715.4941 (705.3565)  amp_scale: 1.0000 (1.0000)  time: 1.2899  data: 0.0002  max mem: 13797
[05:49:42.621162] [Train][Ep-23/100]  [ 500/1435]  eta: 0:19:38  lr: 0.0006 (0.0006)  loss: 1365.7424 (1359.0881)  grad_norm: 701.7741 (707.2284)  amp_scale: 1.0000 (1.0000)  time: 1.2767  data: 0.2269  max mem: 13797
[05:51:47.975854] [Train][Ep-23/100]  [ 600/1435]  eta: 0:17:31  lr: 0.0006 (0.0006)  loss: 1351.0042 (1360.4949)  grad_norm: 707.0224 (708.5987)  amp_scale: 1.0000 (1.0000)  time: 1.3011  data: 0.0540  max mem: 13797
[05:53:50.320064] [Train][Ep-23/100]  [ 700/1435]  eta: 0:15:21  lr: 0.0006 (0.0006)  loss: 1344.0519 (1357.7807)  grad_norm: 692.7677 (707.4153)  amp_scale: 1.0000 (1.0000)  time: 1.2177  data: 0.6303  max mem: 13797
[05:55:55.643168] [Train][Ep-23/100]  [ 800/1435]  eta: 0:13:16  lr: 0.0006 (0.0006)  loss: 1347.0209 (1357.1502)  grad_norm: 693.5288 (705.9640)  amp_scale: 1.0000 (1.0000)  time: 1.2378  data: 0.0019  max mem: 13797
[05:58:02.915017] [Train][Ep-23/100]  [ 900/1435]  eta: 0:11:12  lr: 0.0006 (0.0006)  loss: 1367.7911 (1356.5189)  grad_norm: 701.8286 (705.5100)  amp_scale: 1.0000 (1.0000)  time: 1.2956  data: 0.0010  max mem: 13797
[06:00:15.633235] [Train][Ep-23/100]  [1000/1435]  eta: 0:09:09  lr: 0.0006 (0.0006)  loss: 1326.7640 (1356.3281)  grad_norm: 696.1091 (704.7809)  amp_scale: 1.0000 (1.0000)  time: 1.3110  data: 0.0008  max mem: 13797
[06:02:30.288101] [Train][Ep-23/100]  [1100/1435]  eta: 0:07:05  lr: 0.0006 (0.0006)  loss: 1370.4189 (1355.1725)  grad_norm: 676.3326 (703.7840)  amp_scale: 1.0000 (1.0000)  time: 1.3143  data: 0.0005  max mem: 13797
[06:04:39.901905] [Train][Ep-23/100]  [1200/1435]  eta: 0:04:59  lr: 0.0006 (0.0006)  loss: 1338.0642 (1353.3894)  grad_norm: 720.1816 (705.3449)  amp_scale: 1.0000 (1.0000)  time: 1.2717  data: 0.0009  max mem: 13797
[06:06:47.017546] [Train][Ep-23/100]  [1300/1435]  eta: 0:02:51  lr: 0.0006 (0.0006)  loss: 1370.2477 (1355.2730)  grad_norm: 686.2378 (704.6573)  amp_scale: 1.0000 (1.0000)  time: 1.2612  data: 0.0045  max mem: 13797
[06:08:57.895727] [Train][Ep-23/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1397.9521 (1357.1479)  grad_norm: 669.8342 (703.8123)  amp_scale: 1.0000 (1.0000)  time: 1.2224  data: 0.0427  max mem: 13797
[06:09:40.673455] [Train][Ep-23/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1403.0082 (1358.2779)  grad_norm: 703.6078 (703.8193)  amp_scale: 1.0000 (1.0000)  time: 1.3262  data: 0.0004  max mem: 13797
[06:09:40.674597] [Train][Ep-23/100] Total time: 0:30:29 (1.2750 s / it)
[06:09:40.675138] Syncing meters...
[06:09:41.045324] Averaged stats: lr: 0.0006 (0.0006)  loss: 1403.0082 (1358.6596)  grad_norm: 703.6078 (703.8193)  amp_scale: 1.0000 (1.0000)
[06:09:51.401559] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 24)
[06:09:53.619388] [Train][Ep-24/100]  [   0/1435]  eta: 0:48:03  lr: 0.0006 (0.0006)  time: 2.0096  data: 1.5152  max mem: 13797
[06:11:50.232161] [Train][Ep-24/100]  [ 100/1435]  eta: 0:26:07  lr: 0.0006 (0.0006)  loss: 1322.8016 (1368.0821)  grad_norm: 681.9154 (695.7708)  amp_scale: 1.0000 (1.0000)  time: 1.2295  data: 0.4915  max mem: 13797
[06:13:48.327638] [Train][Ep-24/100]  [ 200/1435]  eta: 0:24:14  lr: 0.0006 (0.0006)  loss: 1334.0757 (1348.8420)  grad_norm: 712.4099 (703.7871)  amp_scale: 1.0000 (1.0000)  time: 1.1387  data: 0.0707  max mem: 13797
[06:15:52.594568] [Train][Ep-24/100]  [ 300/1435]  eta: 0:22:41  lr: 0.0006 (0.0006)  loss: 1348.5625 (1351.5384)  grad_norm: 706.5443 (702.5348)  amp_scale: 1.0000 (1.0000)  time: 1.3410  data: 0.0009  max mem: 13797
[06:17:56.720528] [Train][Ep-24/100]  [ 400/1435]  eta: 0:20:52  lr: 0.0006 (0.0006)  loss: 1357.3750 (1348.7905)  grad_norm: 698.8262 (703.2369)  amp_scale: 1.0000 (1.0000)  time: 1.2379  data: 0.0003  max mem: 13797
[06:20:04.877752] [Train][Ep-24/100]  [ 500/1435]  eta: 0:19:04  lr: 0.0006 (0.0006)  loss: 1358.1259 (1350.7119)  grad_norm: 712.0513 (704.1933)  amp_scale: 1.0000 (1.0000)  time: 1.1966  data: 0.0004  max mem: 13797
[06:22:06.424511] [Train][Ep-24/100]  [ 600/1435]  eta: 0:17:00  lr: 0.0006 (0.0006)  loss: 1324.4463 (1343.8183)  grad_norm: 688.2764 (702.3958)  amp_scale: 1.0000 (1.0000)  time: 1.2214  data: 0.2325  max mem: 13797
[06:24:08.864719] [Train][Ep-24/100]  [ 700/1435]  eta: 0:14:58  lr: 0.0006 (0.0006)  loss: 1331.1750 (1347.0247)  grad_norm: 695.1201 (704.1081)  amp_scale: 1.0000 (1.0000)  time: 1.2342  data: 0.2938  max mem: 13797
[06:26:14.856644] [Train][Ep-24/100]  [ 800/1435]  eta: 0:12:59  lr: 0.0006 (0.0006)  loss: 1370.3107 (1346.8770)  grad_norm: 706.7139 (703.7907)  amp_scale: 1.0000 (1.0000)  time: 1.2833  data: 0.0011  max mem: 13797
[06:28:19.139039] [Train][Ep-24/100]  [ 900/1435]  eta: 0:10:57  lr: 0.0006 (0.0006)  loss: 1346.3517 (1348.4943)  grad_norm: 682.9810 (702.1784)  amp_scale: 1.0000 (1.0000)  time: 1.1938  data: 0.0448  max mem: 13797
[06:30:20.416917] [Train][Ep-24/100]  [1000/1435]  eta: 0:08:53  lr: 0.0006 (0.0006)  loss: 1330.3149 (1347.3237)  grad_norm: 701.4651 (702.6067)  amp_scale: 1.0000 (1.0000)  time: 1.2069  data: 0.0010  max mem: 13797
[06:32:29.844807] [Train][Ep-24/100]  [1100/1435]  eta: 0:06:53  lr: 0.0006 (0.0006)  loss: 1375.3928 (1349.0991)  grad_norm: 684.5209 (702.0974)  amp_scale: 1.0000 (1.0000)  time: 1.2867  data: 0.0004  max mem: 13797
[06:34:40.282046] [Train][Ep-24/100]  [1200/1435]  eta: 0:04:51  lr: 0.0006 (0.0006)  loss: 1356.3585 (1351.5279)  grad_norm: 688.6214 (701.4139)  amp_scale: 1.0000 (1.0000)  time: 1.2935  data: 0.0013  max mem: 13797
[06:36:48.729914] [Train][Ep-24/100]  [1300/1435]  eta: 0:02:47  lr: 0.0006 (0.0006)  loss: 1315.7859 (1350.0903)  grad_norm: 696.9329 (700.4229)  amp_scale: 1.0000 (1.0000)  time: 1.2569  data: 0.0007  max mem: 13797
[06:38:58.431027] [Train][Ep-24/100]  [1400/1435]  eta: 0:00:43  lr: 0.0006 (0.0006)  loss: 1308.6108 (1351.5155)  grad_norm: 705.2776 (701.2459)  amp_scale: 1.0000 (1.0000)  time: 1.3689  data: 0.0525  max mem: 13797
[06:39:41.119518] [Train][Ep-24/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1330.9528 (1350.7761)  grad_norm: 709.6448 (701.1824)  amp_scale: 1.0000 (1.0000)  time: 1.3220  data: 0.0015  max mem: 13797
[06:39:41.121299] [Train][Ep-24/100] Total time: 0:29:49 (1.2470 s / it)
[06:39:41.122166] Syncing meters...
[06:39:41.124229] Averaged stats: lr: 0.0006 (0.0006)  loss: 1330.9528 (1349.0395)  grad_norm: 709.6448 (701.1824)  amp_scale: 1.0000 (1.0000)
[06:39:51.093071] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 25)
[06:39:53.510227] [Train][Ep-25/100]  [   0/1435]  eta: 0:52:30  lr: 0.0006 (0.0006)  time: 2.1951  data: 1.7014  max mem: 13797
[06:41:50.236612] [Train][Ep-25/100]  [ 100/1435]  eta: 0:26:11  lr: 0.0006 (0.0006)  loss: 1336.1572 (1346.8673)  grad_norm: 692.6169 (689.3338)  amp_scale: 1.0000 (1.0000)  time: 1.2457  data: 0.0007  max mem: 13797
[06:43:47.550605] [Train][Ep-25/100]  [ 200/1435]  eta: 0:24:11  lr: 0.0006 (0.0006)  loss: 1340.2733 (1343.7999)  grad_norm: 689.3295 (694.8786)  amp_scale: 1.0000 (1.0000)  time: 1.1449  data: 0.2120  max mem: 13797
[06:45:54.302262] [Train][Ep-25/100]  [ 300/1435]  eta: 0:22:48  lr: 0.0006 (0.0006)  loss: 1354.3441 (1350.0991)  grad_norm: 697.8939 (698.1620)  amp_scale: 1.0000 (1.0000)  time: 1.2853  data: 0.4098  max mem: 13797
[06:47:59.035049] [Train][Ep-25/100]  [ 400/1435]  eta: 0:20:58  lr: 0.0006 (0.0006)  loss: 1339.0101 (1342.7685)  grad_norm: 692.9496 (698.9095)  amp_scale: 1.0000 (1.0000)  time: 1.3163  data: 0.7338  max mem: 13797
[06:50:09.951439] [Train][Ep-25/100]  [ 500/1435]  eta: 0:19:14  lr: 0.0006 (0.0006)  loss: 1315.6556 (1340.2059)  grad_norm: 707.5155 (701.3875)  amp_scale: 1.0000 (1.0000)  time: 1.2588  data: 0.6850  max mem: 13797
[06:52:22.803955] [Train][Ep-25/100]  [ 600/1435]  eta: 0:17:24  lr: 0.0006 (0.0006)  loss: 1330.3239 (1338.5756)  grad_norm: 702.5269 (701.2266)  amp_scale: 1.0000 (1.0000)  time: 1.3192  data: 0.7398  max mem: 13797
[06:54:38.384163] [Train][Ep-25/100]  [ 700/1435]  eta: 0:15:30  lr: 0.0006 (0.0006)  loss: 1312.3389 (1336.1041)  grad_norm: 685.0436 (698.7454)  amp_scale: 1.0000 (1.0000)  time: 1.3850  data: 0.8100  max mem: 13797
[06:56:52.266409] [Train][Ep-25/100]  [ 800/1435]  eta: 0:13:29  lr: 0.0006 (0.0006)  loss: 1318.5294 (1335.5527)  grad_norm: 681.9791 (698.7634)  amp_scale: 1.0000 (1.0000)  time: 1.3098  data: 0.7234  max mem: 13797
[06:59:06.032573] [Train][Ep-25/100]  [ 900/1435]  eta: 0:11:25  lr: 0.0006 (0.0006)  loss: 1331.7430 (1334.7616)  grad_norm: 692.6425 (698.1506)  amp_scale: 1.0000 (1.0000)  time: 1.2734  data: 0.6944  max mem: 13797
[07:01:08.023420] [Train][Ep-25/100]  [1000/1435]  eta: 0:09:14  lr: 0.0006 (0.0006)  loss: 1333.1580 (1335.2609)  grad_norm: 698.6835 (698.6670)  amp_scale: 1.0000 (1.0000)  time: 1.1503  data: 0.5604  max mem: 13797
[07:03:08.927127] [Train][Ep-25/100]  [1100/1435]  eta: 0:07:05  lr: 0.0006 (0.0006)  loss: 1326.1786 (1337.6193)  grad_norm: 697.7595 (698.6968)  amp_scale: 1.0000 (1.0000)  time: 1.2234  data: 0.6475  max mem: 13797
[07:05:18.353865] [Train][Ep-25/100]  [1200/1435]  eta: 0:04:58  lr: 0.0006 (0.0006)  loss: 1368.6558 (1338.7390)  grad_norm: 691.9464 (699.1776)  amp_scale: 1.0000 (1.0000)  time: 1.3146  data: 0.7394  max mem: 13797
[07:07:26.527501] [Train][Ep-25/100]  [1300/1435]  eta: 0:02:51  lr: 0.0006 (0.0006)  loss: 1320.2404 (1338.5289)  grad_norm: 697.8571 (698.8110)  amp_scale: 1.0000 (1.0000)  time: 1.2516  data: 0.6775  max mem: 13797
[07:09:28.257386] [Train][Ep-25/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1359.6542 (1340.3639)  grad_norm: 698.0629 (699.5731)  amp_scale: 1.0000 (1.0000)  time: 1.1981  data: 0.5248  max mem: 13797
[07:10:09.466709] [Train][Ep-25/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1348.7805 (1339.7592)  grad_norm: 688.8096 (698.9189)  amp_scale: 1.0000 (1.0000)  time: 1.2324  data: 0.6522  max mem: 13797
[07:10:09.467956] [Train][Ep-25/100] Total time: 0:30:18 (1.2670 s / it)
[07:10:09.468451] Syncing meters...
[07:10:09.470175] Averaged stats: lr: 0.0006 (0.0006)  loss: 1348.7805 (1340.6418)  grad_norm: 688.8096 (698.9189)  amp_scale: 1.0000 (1.0000)
[07:10:19.789431] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 26)
[07:10:22.127426] [Train][Ep-26/100]  [   0/1435]  eta: 0:50:30  lr: 0.0006 (0.0006)  time: 2.1122  data: 1.6182  max mem: 13797
[07:12:28.212797] [Train][Ep-26/100]  [ 100/1435]  eta: 0:28:14  lr: 0.0006 (0.0006)  loss: 1362.7288 (1363.4099)  grad_norm: 718.7322 (706.9823)  amp_scale: 1.0000 (1.0000)  time: 1.3225  data: 0.7371  max mem: 13797
[07:14:31.149384] [Train][Ep-26/100]  [ 200/1435]  eta: 0:25:42  lr: 0.0006 (0.0006)  loss: 1315.7931 (1354.6543)  grad_norm: 686.7222 (698.9215)  amp_scale: 1.0000 (1.0000)  time: 1.2373  data: 0.6588  max mem: 13797
[07:16:40.363969] [Train][Ep-26/100]  [ 300/1435]  eta: 0:23:54  lr: 0.0006 (0.0006)  loss: 1278.9315 (1354.3895)  grad_norm: 698.4731 (696.2936)  amp_scale: 1.0000 (1.0000)  time: 1.3000  data: 0.7155  max mem: 13797
[07:18:51.872459] [Train][Ep-26/100]  [ 400/1435]  eta: 0:22:01  lr: 0.0006 (0.0006)  loss: 1355.9491 (1354.8172)  grad_norm: 699.1368 (697.2659)  amp_scale: 1.0000 (1.0000)  time: 1.2673  data: 0.6899  max mem: 13797
[07:21:00.046969] [Train][Ep-26/100]  [ 500/1435]  eta: 0:19:54  lr: 0.0006 (0.0006)  loss: 1347.6934 (1353.3441)  grad_norm: 697.8776 (699.8242)  amp_scale: 1.0000 (1.0000)  time: 1.2812  data: 0.7042  max mem: 13797
[07:23:03.863074] [Train][Ep-26/100]  [ 600/1435]  eta: 0:17:41  lr: 0.0006 (0.0006)  loss: 1339.3667 (1352.3399)  grad_norm: 685.1798 (698.4526)  amp_scale: 1.0000 (1.0000)  time: 1.2694  data: 0.3314  max mem: 13797
[07:25:11.383577] [Train][Ep-26/100]  [ 700/1435]  eta: 0:15:34  lr: 0.0006 (0.0006)  loss: 1326.2849 (1347.2256)  grad_norm: 695.7719 (698.9229)  amp_scale: 1.0000 (1.0000)  time: 1.2485  data: 0.4273  max mem: 13797
[07:27:20.540701] [Train][Ep-26/100]  [ 800/1435]  eta: 0:13:28  lr: 0.0006 (0.0006)  loss: 1323.3854 (1345.4842)  grad_norm: 695.5386 (697.7879)  amp_scale: 1.0000 (1.0000)  time: 1.2623  data: 0.6075  max mem: 13797
[07:29:27.396015] [Train][Ep-26/100]  [ 900/1435]  eta: 0:11:21  lr: 0.0006 (0.0006)  loss: 1300.4396 (1342.8182)  grad_norm: 689.3133 (696.8464)  amp_scale: 1.0000 (1.0000)  time: 1.2902  data: 0.7158  max mem: 13797
[07:31:35.840364] [Train][Ep-26/100]  [1000/1435]  eta: 0:09:14  lr: 0.0006 (0.0006)  loss: 1279.0066 (1341.3990)  grad_norm: 690.2142 (695.7092)  amp_scale: 1.0000 (1.0000)  time: 1.3399  data: 0.7587  max mem: 13797
[07:33:39.528853] [Train][Ep-26/100]  [1100/1435]  eta: 0:07:05  lr: 0.0006 (0.0006)  loss: 1351.1405 (1342.3845)  grad_norm: 700.8228 (695.6138)  amp_scale: 1.0000 (1.0000)  time: 1.1619  data: 0.5819  max mem: 13797
[07:35:39.193909] [Train][Ep-26/100]  [1200/1435]  eta: 0:04:57  lr: 0.0006 (0.0006)  loss: 1322.6381 (1342.2221)  grad_norm: 685.6873 (695.3192)  amp_scale: 1.0000 (1.0000)  time: 1.2700  data: 0.4781  max mem: 13797
[07:37:45.374411] [Train][Ep-26/100]  [1300/1435]  eta: 0:02:50  lr: 0.0006 (0.0006)  loss: 1335.5621 (1340.8957)  grad_norm: 675.3842 (694.0789)  amp_scale: 1.0000 (1.0000)  time: 1.3277  data: 0.7485  max mem: 13797
[07:39:54.481315] [Train][Ep-26/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1321.9237 (1340.3073)  grad_norm: 682.3012 (693.7286)  amp_scale: 1.0000 (1.0000)  time: 1.2487  data: 0.6696  max mem: 13797
[07:40:33.759550] [Train][Ep-26/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1340.1511 (1340.2317)  grad_norm: 682.3012 (693.5901)  amp_scale: 1.0000 (1.0000)  time: 1.1085  data: 0.5025  max mem: 13797
[07:40:33.760568] [Train][Ep-26/100] Total time: 0:30:13 (1.2639 s / it)
[07:40:33.761069] Syncing meters...
[07:40:34.037689] Averaged stats: lr: 0.0006 (0.0006)  loss: 1340.1511 (1334.0861)  grad_norm: 682.3012 (693.5901)  amp_scale: 1.0000 (1.0000)
[07:40:43.950402] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 27)
[07:40:46.139803] [Train][Ep-27/100]  [   0/1435]  eta: 0:47:20  lr: 0.0006 (0.0006)  time: 1.9791  data: 1.4861  max mem: 13797
[07:42:52.930333] [Train][Ep-27/100]  [ 100/1435]  eta: 0:28:22  lr: 0.0006 (0.0006)  loss: 1298.8252 (1295.3370)  grad_norm: 685.2883 (693.4012)  amp_scale: 1.0000 (1.0000)  time: 1.3359  data: 0.7586  max mem: 13797
[07:45:01.940602] [Train][Ep-27/100]  [ 200/1435]  eta: 0:26:23  lr: 0.0006 (0.0006)  loss: 1294.5692 (1307.9814)  grad_norm: 696.1277 (693.1376)  amp_scale: 1.0000 (1.0000)  time: 1.3841  data: 0.1807  max mem: 13797
[07:47:12.393950] [Train][Ep-27/100]  [ 300/1435]  eta: 0:24:23  lr: 0.0006 (0.0006)  loss: 1298.2371 (1309.9254)  grad_norm: 702.6791 (696.2636)  amp_scale: 1.0000 (1.0000)  time: 1.2309  data: 0.1752  max mem: 13797
[07:49:15.213310] [Train][Ep-27/100]  [ 400/1435]  eta: 0:21:58  lr: 0.0006 (0.0006)  loss: 1280.9972 (1311.0524)  grad_norm: 707.7262 (698.8195)  amp_scale: 1.0000 (1.0000)  time: 1.2315  data: 0.4179  max mem: 13797
[07:51:28.182429] [Train][Ep-27/100]  [ 500/1435]  eta: 0:20:01  lr: 0.0006 (0.0006)  loss: 1340.0686 (1319.7996)  grad_norm: 685.4095 (698.0812)  amp_scale: 1.0000 (1.0000)  time: 1.2991  data: 0.0796  max mem: 13797
[07:53:42.666160] [Train][Ep-27/100]  [ 600/1435]  eta: 0:18:01  lr: 0.0006 (0.0006)  loss: 1290.3673 (1314.3263)  grad_norm: 679.3402 (698.8915)  amp_scale: 1.0000 (1.0000)  time: 1.2845  data: 0.0855  max mem: 13797
[07:55:48.974063] [Train][Ep-27/100]  [ 700/1435]  eta: 0:15:48  lr: 0.0006 (0.0006)  loss: 1285.4932 (1313.7321)  grad_norm: 707.0855 (700.7039)  amp_scale: 1.0000 (1.0000)  time: 1.2527  data: 0.1329  max mem: 13797
[07:57:55.849939] [Train][Ep-27/100]  [ 800/1435]  eta: 0:13:37  lr: 0.0006 (0.0006)  loss: 1312.5288 (1317.9337)  grad_norm: 691.0443 (699.6949)  amp_scale: 1.0000 (1.0000)  time: 1.2831  data: 0.0919  max mem: 13797
[08:00:04.706072] [Train][Ep-27/100]  [ 900/1435]  eta: 0:11:29  lr: 0.0006 (0.0006)  loss: 1327.3004 (1320.0763)  grad_norm: 686.9170 (699.4139)  amp_scale: 1.0000 (1.0000)  time: 1.2438  data: 0.6654  max mem: 13797
[08:02:06.987985] [Train][Ep-27/100]  [1000/1435]  eta: 0:09:17  lr: 0.0006 (0.0006)  loss: 1336.3180 (1323.5659)  grad_norm: 703.8085 (699.5277)  amp_scale: 1.0000 (1.0000)  time: 1.2177  data: 0.1893  max mem: 13797
[08:04:13.859787] [Train][Ep-27/100]  [1100/1435]  eta: 0:07:08  lr: 0.0006 (0.0006)  loss: 1283.4402 (1320.0657)  grad_norm: 673.2534 (697.6794)  amp_scale: 1.0000 (1.0000)  time: 1.2761  data: 0.0007  max mem: 13797
[08:06:23.172329] [Train][Ep-27/100]  [1200/1435]  eta: 0:05:01  lr: 0.0006 (0.0006)  loss: 1353.2638 (1321.4972)  grad_norm: 681.3325 (696.9214)  amp_scale: 1.0000 (1.0000)  time: 1.2996  data: 0.7211  max mem: 13797
[08:08:30.061411] [Train][Ep-27/100]  [1300/1435]  eta: 0:02:52  lr: 0.0006 (0.0006)  loss: 1283.0260 (1322.2534)  grad_norm: 673.3244 (696.6137)  amp_scale: 1.0000 (1.0000)  time: 1.3094  data: 0.7291  max mem: 13797
[08:10:43.549150] [Train][Ep-27/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1332.1243 (1323.0879)  grad_norm: 698.8290 (697.4899)  amp_scale: 1.0000 (1.0000)  time: 1.3272  data: 0.7505  max mem: 13797
[08:11:27.034879] [Train][Ep-27/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1332.1243 (1323.0770)  grad_norm: 703.8113 (697.7280)  amp_scale: 1.0000 (1.0000)  time: 1.2260  data: 0.6467  max mem: 13797
[08:11:27.035717] [Train][Ep-27/100] Total time: 0:30:42 (1.2842 s / it)
[08:11:27.036105] Syncing meters...
[08:11:27.039892] Averaged stats: lr: 0.0006 (0.0006)  loss: 1332.1243 (1321.2567)  grad_norm: 703.8113 (697.7280)  amp_scale: 1.0000 (1.0000)
[08:11:37.401646] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 28)
[08:11:39.587458] [Train][Ep-28/100]  [   0/1435]  eta: 0:46:36  lr: 0.0006 (0.0006)  time: 1.9485  data: 1.4556  max mem: 13797
[08:13:48.824691] [Train][Ep-28/100]  [ 100/1435]  eta: 0:28:53  lr: 0.0006 (0.0006)  loss: 1279.4015 (1288.4273)  grad_norm: 715.2131 (699.2226)  amp_scale: 1.0000 (1.0000)  time: 1.3345  data: 0.7497  max mem: 13797
[08:15:57.978604] [Train][Ep-28/100]  [ 200/1435]  eta: 0:26:39  lr: 0.0006 (0.0006)  loss: 1274.2312 (1288.4202)  grad_norm: 703.4890 (699.2390)  amp_scale: 1.0000 (1.0000)  time: 1.2452  data: 0.6464  max mem: 13797
[08:18:01.556275] [Train][Ep-28/100]  [ 300/1435]  eta: 0:24:07  lr: 0.0006 (0.0006)  loss: 1276.3634 (1301.8643)  grad_norm: 706.1899 (701.0482)  amp_scale: 1.0000 (1.0000)  time: 1.2355  data: 0.6076  max mem: 13797
[08:20:09.912420] [Train][Ep-28/100]  [ 400/1435]  eta: 0:22:02  lr: 0.0006 (0.0006)  loss: 1288.7861 (1302.4869)  grad_norm: 692.6319 (698.1716)  amp_scale: 1.0000 (1.0000)  time: 1.2854  data: 0.7051  max mem: 13797
[08:22:19.517912] [Train][Ep-28/100]  [ 500/1435]  eta: 0:19:57  lr: 0.0006 (0.0006)  loss: 1280.6237 (1298.8217)  grad_norm: 698.4415 (697.9454)  amp_scale: 1.0000 (1.0000)  time: 1.2449  data: 0.6612  max mem: 13797
[08:24:30.880257] [Train][Ep-28/100]  [ 600/1435]  eta: 0:17:54  lr: 0.0006 (0.0006)  loss: 1288.3319 (1301.1473)  grad_norm: 692.7603 (696.9982)  amp_scale: 1.0000 (1.0000)  time: 1.3181  data: 0.7428  max mem: 13797
[08:26:40.611018] [Train][Ep-28/100]  [ 700/1435]  eta: 0:15:46  lr: 0.0006 (0.0006)  loss: 1271.2466 (1300.1795)  grad_norm: 683.8862 (696.1952)  amp_scale: 1.0000 (1.0000)  time: 1.2596  data: 0.6765  max mem: 13797
[08:28:48.543347] [Train][Ep-28/100]  [ 800/1435]  eta: 0:13:37  lr: 0.0006 (0.0006)  loss: 1275.6809 (1303.6171)  grad_norm: 696.1049 (697.2687)  amp_scale: 1.0000 (1.0000)  time: 1.2691  data: 0.6873  max mem: 13797
[08:30:59.432064] [Train][Ep-28/100]  [ 900/1435]  eta: 0:11:29  lr: 0.0006 (0.0006)  loss: 1288.9949 (1303.6036)  grad_norm: 696.6799 (698.4587)  amp_scale: 1.0000 (1.0000)  time: 1.2921  data: 0.6802  max mem: 13797
[08:33:11.044322] [Train][Ep-28/100]  [1000/1435]  eta: 0:09:22  lr: 0.0006 (0.0006)  loss: 1284.9691 (1304.9827)  grad_norm: 684.9320 (698.6370)  amp_scale: 1.0000 (1.0000)  time: 1.3675  data: 0.0004  max mem: 13797
[08:35:21.648908] [Train][Ep-28/100]  [1100/1435]  eta: 0:07:13  lr: 0.0006 (0.0006)  loss: 1313.4711 (1305.0462)  grad_norm: 692.1276 (698.3946)  amp_scale: 1.0000 (1.0000)  time: 1.3246  data: 0.0514  max mem: 13797
[08:37:31.916648] [Train][Ep-28/100]  [1200/1435]  eta: 0:05:04  lr: 0.0006 (0.0006)  loss: 1293.6991 (1306.6975)  grad_norm: 694.6954 (698.1201)  amp_scale: 1.0000 (1.0000)  time: 1.2590  data: 0.2323  max mem: 13797
[08:39:34.512847] [Train][Ep-28/100]  [1300/1435]  eta: 0:02:53  lr: 0.0006 (0.0006)  loss: 1258.0237 (1305.3479)  grad_norm: 680.0273 (697.5217)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.3959  max mem: 13797
[08:41:35.159550] [Train][Ep-28/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1291.8556 (1307.4394)  grad_norm: 688.3328 (697.7548)  amp_scale: 1.0000 (1.0000)  time: 1.2367  data: 0.4946  max mem: 13797
[08:42:15.773008] [Train][Ep-28/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1318.2894 (1308.0346)  grad_norm: 697.3962 (697.6248)  amp_scale: 1.0000 (1.0000)  time: 1.1495  data: 0.2940  max mem: 13797
[08:42:15.773846] [Train][Ep-28/100] Total time: 0:30:38 (1.2809 s / it)
[08:42:15.774342] Syncing meters...
[08:42:16.077331] Averaged stats: lr: 0.0006 (0.0006)  loss: 1318.2894 (1316.0434)  grad_norm: 697.3962 (697.6248)  amp_scale: 1.0000 (1.0000)
[08:42:26.796183] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 29)
[08:42:29.043516] [Train][Ep-29/100]  [   0/1435]  eta: 0:47:21  lr: 0.0006 (0.0006)  time: 1.9802  data: 1.4837  max mem: 13797
[08:44:39.543042] [Train][Ep-29/100]  [ 100/1435]  eta: 0:29:11  lr: 0.0006 (0.0006)  loss: 1272.1779 (1283.3761)  grad_norm: 678.7488 (678.5544)  amp_scale: 1.0000 (1.0000)  time: 1.3556  data: 0.7738  max mem: 13797
[08:46:45.616836] [Train][Ep-29/100]  [ 200/1435]  eta: 0:26:28  lr: 0.0006 (0.0006)  loss: 1289.3560 (1285.6807)  grad_norm: 709.3203 (690.4805)  amp_scale: 1.0000 (1.0000)  time: 1.2564  data: 0.6737  max mem: 13797
[08:48:52.038752] [Train][Ep-29/100]  [ 300/1435]  eta: 0:24:11  lr: 0.0006 (0.0006)  loss: 1313.8228 (1298.1412)  grad_norm: 692.0237 (692.7741)  amp_scale: 1.0000 (1.0000)  time: 1.2726  data: 0.4332  max mem: 13797
[08:51:00.685055] [Train][Ep-29/100]  [ 400/1435]  eta: 0:22:05  lr: 0.0006 (0.0006)  loss: 1296.9127 (1302.8067)  grad_norm: 692.6017 (691.3639)  amp_scale: 1.0000 (1.0000)  time: 1.3177  data: 0.0454  max mem: 13797
[08:53:12.159750] [Train][Ep-29/100]  [ 500/1435]  eta: 0:20:03  lr: 0.0006 (0.0006)  loss: 1293.1830 (1303.1820)  grad_norm: 698.0278 (694.5346)  amp_scale: 1.0000 (1.0000)  time: 1.3023  data: 0.1143  max mem: 13797
[08:55:21.998690] [Train][Ep-29/100]  [ 600/1435]  eta: 0:17:56  lr: 0.0006 (0.0006)  loss: 1273.1641 (1302.6044)  grad_norm: 694.3255 (693.1557)  amp_scale: 1.0000 (1.0000)  time: 1.2564  data: 0.5474  max mem: 13797
[08:57:30.047982] [Train][Ep-29/100]  [ 700/1435]  eta: 0:15:46  lr: 0.0006 (0.0006)  loss: 1268.7518 (1299.9879)  grad_norm: 688.8686 (692.6931)  amp_scale: 1.0000 (1.0000)  time: 1.2906  data: 0.7072  max mem: 13797
[08:59:40.857049] [Train][Ep-29/100]  [ 800/1435]  eta: 0:13:39  lr: 0.0006 (0.0006)  loss: 1325.9381 (1301.2092)  grad_norm: 671.1318 (693.3675)  amp_scale: 1.0000 (1.0000)  time: 1.2871  data: 0.7080  max mem: 13797
[09:01:49.277474] [Train][Ep-29/100]  [ 900/1435]  eta: 0:11:30  lr: 0.0006 (0.0006)  loss: 1274.7515 (1300.6177)  grad_norm: 685.1367 (693.0216)  amp_scale: 1.0000 (1.0000)  time: 1.3157  data: 0.7375  max mem: 13797
[09:03:56.338141] [Train][Ep-29/100]  [1000/1435]  eta: 0:09:20  lr: 0.0006 (0.0006)  loss: 1328.4233 (1302.8634)  grad_norm: 698.8659 (693.3995)  amp_scale: 1.0000 (1.0000)  time: 1.2347  data: 0.6442  max mem: 13797
[09:06:03.788961] [Train][Ep-29/100]  [1100/1435]  eta: 0:07:11  lr: 0.0006 (0.0006)  loss: 1297.0187 (1302.7360)  grad_norm: 689.4170 (693.2684)  amp_scale: 1.0000 (1.0000)  time: 1.2959  data: 0.7194  max mem: 13797
[09:08:15.587897] [Train][Ep-29/100]  [1200/1435]  eta: 0:05:02  lr: 0.0006 (0.0006)  loss: 1292.4045 (1302.6792)  grad_norm: 711.2850 (693.8568)  amp_scale: 1.0000 (1.0000)  time: 1.3407  data: 0.7519  max mem: 13797
[09:10:27.647914] [Train][Ep-29/100]  [1300/1435]  eta: 0:02:54  lr: 0.0006 (0.0006)  loss: 1271.0730 (1301.6325)  grad_norm: 706.0130 (695.0009)  amp_scale: 1.0000 (1.0000)  time: 1.3385  data: 0.7612  max mem: 13797
[09:12:40.081845] [Train][Ep-29/100]  [1400/1435]  eta: 0:00:45  lr: 0.0006 (0.0006)  loss: 1275.8881 (1302.5116)  grad_norm: 690.6648 (694.4982)  amp_scale: 1.0000 (1.0000)  time: 1.3625  data: 0.7781  max mem: 13797
[09:13:24.246699] [Train][Ep-29/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1283.2369 (1303.1503)  grad_norm: 692.6614 (694.5622)  amp_scale: 1.0000 (1.0000)  time: 1.2454  data: 0.6591  max mem: 13797
[09:13:24.247567] [Train][Ep-29/100] Total time: 0:30:57 (1.2942 s / it)
[09:13:24.248325] Syncing meters...
[09:13:24.249818] Averaged stats: lr: 0.0006 (0.0006)  loss: 1283.2369 (1306.1114)  grad_norm: 692.6614 (694.5622)  amp_scale: 1.0000 (1.0000)
[09:13:34.345064] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 30)
[09:13:36.408834] [Train][Ep-30/100]  [   0/1435]  eta: 0:43:57  lr: 0.0006 (0.0006)  time: 1.8377  data: 1.3445  max mem: 13797
[09:15:47.016295] [Train][Ep-30/100]  [ 100/1435]  eta: 0:29:10  lr: 0.0006 (0.0006)  loss: 1211.4454 (1264.6265)  grad_norm: 693.6863 (705.4216)  amp_scale: 1.0000 (1.0000)  time: 1.3065  data: 0.7305  max mem: 13797
[09:17:52.034343] [Train][Ep-30/100]  [ 200/1435]  eta: 0:26:21  lr: 0.0006 (0.0006)  loss: 1307.0541 (1286.5021)  grad_norm: 697.1610 (701.9163)  amp_scale: 1.0000 (1.0000)  time: 1.3391  data: 0.0260  max mem: 13797
[09:19:58.577666] [Train][Ep-30/100]  [ 300/1435]  eta: 0:24:07  lr: 0.0006 (0.0006)  loss: 1294.4086 (1286.3407)  grad_norm: 687.2325 (697.4941)  amp_scale: 1.0000 (1.0000)  time: 1.2425  data: 0.6640  max mem: 13797
[09:22:05.018463] [Train][Ep-30/100]  [ 400/1435]  eta: 0:21:57  lr: 0.0006 (0.0006)  loss: 1260.5726 (1289.7002)  grad_norm: 691.4857 (697.7513)  amp_scale: 1.0000 (1.0000)  time: 1.2755  data: 0.3633  max mem: 13797
[09:24:05.122688] [Train][Ep-30/100]  [ 500/1435]  eta: 0:19:36  lr: 0.0006 (0.0006)  loss: 1263.4600 (1290.5576)  grad_norm: 691.1644 (697.1317)  amp_scale: 1.0000 (1.0000)  time: 1.1726  data: 0.3537  max mem: 13797
[09:26:05.789404] [Train][Ep-30/100]  [ 600/1435]  eta: 0:17:23  lr: 0.0006 (0.0006)  loss: 1257.5405 (1289.5203)  grad_norm: 699.5947 (696.9467)  amp_scale: 1.0000 (1.0000)  time: 1.2765  data: 0.1947  max mem: 13797
[09:28:14.112749] [Train][Ep-30/100]  [ 700/1435]  eta: 0:15:22  lr: 0.0006 (0.0006)  loss: 1285.5746 (1293.1358)  grad_norm: 666.1672 (695.2281)  amp_scale: 1.0000 (1.0000)  time: 1.2076  data: 0.6297  max mem: 13797
[09:30:19.419995] [Train][Ep-30/100]  [ 800/1435]  eta: 0:13:16  lr: 0.0006 (0.0006)  loss: 1259.7511 (1292.1473)  grad_norm: 676.0125 (695.0141)  amp_scale: 1.0000 (1.0000)  time: 1.2386  data: 0.6547  max mem: 13797
[09:32:24.803452] [Train][Ep-30/100]  [ 900/1435]  eta: 0:11:11  lr: 0.0006 (0.0006)  loss: 1300.7222 (1292.7405)  grad_norm: 701.8588 (695.8802)  amp_scale: 1.0000 (1.0000)  time: 1.2070  data: 0.1090  max mem: 13797
[09:34:33.319303] [Train][Ep-30/100]  [1000/1435]  eta: 0:09:06  lr: 0.0006 (0.0006)  loss: 1227.8577 (1289.3960)  grad_norm: 663.2354 (693.9588)  amp_scale: 1.0000 (1.0000)  time: 1.2177  data: 0.4678  max mem: 13797
[09:36:38.560669] [Train][Ep-30/100]  [1100/1435]  eta: 0:07:01  lr: 0.0006 (0.0006)  loss: 1231.7893 (1287.2010)  grad_norm: 679.3174 (692.0508)  amp_scale: 1.0000 (1.0000)  time: 1.2580  data: 0.0002  max mem: 13797
[09:38:39.208955] [Train][Ep-30/100]  [1200/1435]  eta: 0:04:54  lr: 0.0006 (0.0006)  loss: 1316.3033 (1289.6571)  grad_norm: 700.4695 (692.1620)  amp_scale: 1.0000 (1.0000)  time: 1.2540  data: 0.6683  max mem: 13797
[09:40:38.118185] [Train][Ep-30/100]  [1300/1435]  eta: 0:02:48  lr: 0.0006 (0.0006)  loss: 1248.9377 (1288.6184)  grad_norm: 674.6523 (691.5619)  amp_scale: 1.0000 (1.0000)  time: 1.2383  data: 0.6550  max mem: 13797
[09:42:39.855287] [Train][Ep-30/100]  [1400/1435]  eta: 0:00:43  lr: 0.0006 (0.0006)  loss: 1242.6445 (1286.7192)  grad_norm: 689.4743 (692.7970)  amp_scale: 1.0000 (1.0000)  time: 1.2385  data: 0.6489  max mem: 13797
[09:43:18.880755] [Train][Ep-30/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1264.8762 (1287.9755)  grad_norm: 689.2372 (693.0023)  amp_scale: 1.0000 (1.0000)  time: 1.1246  data: 0.5274  max mem: 13797
[09:43:18.881783] [Train][Ep-30/100] Total time: 0:29:44 (1.2434 s / it)
[09:43:18.882249] Syncing meters...
[09:43:18.884388] Averaged stats: lr: 0.0006 (0.0006)  loss: 1264.8762 (1299.4002)  grad_norm: 689.2372 (693.0023)  amp_scale: 1.0000 (1.0000)
[09:43:20.983332] [Eval][Ep-30/100]  [  0/121]  eta: 0:04:13  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0912  data: 1.9273  max mem: 13797
[09:45:16.573710] [Eval][Ep-30/100]  [100/121]  eta: 0:00:24  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1918  data: 1.0281  max mem: 13797
[09:45:38.878478] [Eval][Ep-30/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1151  data: 0.9540  max mem: 13797
[09:45:38.880172] [Eval][Ep-30/100] Total time: 0:02:19 (1.1569 s / it)
[09:45:39.140780] [Eval][Ep-30/100] val_acc1_image=27.62 | val_acc1_audio=39.69 | val_acc1_fusion=37.32 | val_acc1_all=50.58
[09:45:49.191758] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 31)
[09:45:51.511181] [Train][Ep-31/100]  [   0/1435]  eta: 0:49:04  lr: 0.0006 (0.0006)  time: 2.0518  data: 1.5606  max mem: 13797
[09:47:56.278768] [Train][Ep-31/100]  [ 100/1435]  eta: 0:27:56  lr: 0.0006 (0.0006)  loss: 1270.3875 (1279.1752)  grad_norm: 677.4722 (674.7887)  amp_scale: 1.0000 (1.0000)  time: 1.2790  data: 0.6953  max mem: 13797
[09:50:02.185551] [Train][Ep-31/100]  [ 200/1435]  eta: 0:25:52  lr: 0.0006 (0.0006)  loss: 1312.1031 (1286.1567)  grad_norm: 676.2303 (678.2391)  amp_scale: 1.0000 (1.0000)  time: 1.2923  data: 0.7104  max mem: 13797
[09:52:09.769508] [Train][Ep-31/100]  [ 300/1435]  eta: 0:23:53  lr: 0.0006 (0.0006)  loss: 1259.6250 (1275.4559)  grad_norm: 708.1808 (683.9732)  amp_scale: 1.0000 (1.0000)  time: 1.2635  data: 0.6487  max mem: 13797
[09:54:15.783738] [Train][Ep-31/100]  [ 400/1435]  eta: 0:21:46  lr: 0.0006 (0.0006)  loss: 1264.0470 (1272.2834)  grad_norm: 681.1801 (686.9456)  amp_scale: 1.0000 (1.0000)  time: 1.2117  data: 0.3520  max mem: 13797
[09:56:21.891260] [Train][Ep-31/100]  [ 500/1435]  eta: 0:19:40  lr: 0.0006 (0.0006)  loss: 1271.8059 (1272.4985)  grad_norm: 672.5840 (683.9353)  amp_scale: 1.0000 (1.0000)  time: 1.2488  data: 0.1047  max mem: 13797
[09:58:19.010398] [Train][Ep-31/100]  [ 600/1435]  eta: 0:17:21  lr: 0.0006 (0.0006)  loss: 1264.8361 (1274.4646)  grad_norm: 674.7084 (685.7106)  amp_scale: 1.0000 (1.0000)  time: 1.1724  data: 0.1523  max mem: 13797
[10:00:23.703513] [Train][Ep-31/100]  [ 700/1435]  eta: 0:15:16  lr: 0.0006 (0.0006)  loss: 1297.4098 (1276.7371)  grad_norm: 691.4112 (687.6488)  amp_scale: 1.0000 (1.0000)  time: 1.1884  data: 0.3568  max mem: 13797
[10:02:28.669798] [Train][Ep-31/100]  [ 800/1435]  eta: 0:13:12  lr: 0.0006 (0.0006)  loss: 1307.4279 (1283.2215)  grad_norm: 704.8314 (691.1755)  amp_scale: 1.0000 (1.0000)  time: 1.2950  data: 0.0150  max mem: 13797
[10:04:32.676305] [Train][Ep-31/100]  [ 900/1435]  eta: 0:11:06  lr: 0.0006 (0.0006)  loss: 1231.6132 (1281.1856)  grad_norm: 680.5192 (691.2472)  amp_scale: 1.0000 (1.0000)  time: 1.2111  data: 0.6088  max mem: 13797
[10:06:32.060950] [Train][Ep-31/100]  [1000/1435]  eta: 0:08:59  lr: 0.0006 (0.0006)  loss: 1268.5642 (1280.9036)  grad_norm: 694.2621 (691.5937)  amp_scale: 1.0000 (1.0000)  time: 1.2716  data: 0.2108  max mem: 13797
[10:08:41.057137] [Train][Ep-31/100]  [1100/1435]  eta: 0:06:57  lr: 0.0006 (0.0006)  loss: 1228.8458 (1280.1435)  grad_norm: 670.0285 (691.0384)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.5941  max mem: 13797
[10:10:45.857861] [Train][Ep-31/100]  [1200/1435]  eta: 0:04:52  lr: 0.0006 (0.0006)  loss: 1306.6321 (1281.3388)  grad_norm: 682.0444 (690.0409)  amp_scale: 1.0000 (1.0000)  time: 1.1546  data: 0.5415  max mem: 13797
[10:13:00.491398] [Train][Ep-31/100]  [1300/1435]  eta: 0:02:49  lr: 0.0006 (0.0006)  loss: 1257.6501 (1281.3329)  grad_norm: 679.4733 (689.3866)  amp_scale: 1.0000 (1.0000)  time: 1.3558  data: 0.7758  max mem: 13797
[10:15:05.400532] [Train][Ep-31/100]  [1400/1435]  eta: 0:00:43  lr: 0.0006 (0.0006)  loss: 1302.6367 (1282.9715)  grad_norm: 673.6922 (689.0887)  amp_scale: 1.0000 (1.0000)  time: 1.1382  data: 0.5068  max mem: 13797
[10:15:45.632976] [Train][Ep-31/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1259.2551 (1282.2840)  grad_norm: 673.6922 (689.1296)  amp_scale: 1.0000 (1.0000)  time: 1.2197  data: 0.0803  max mem: 13797
[10:15:45.634026] [Train][Ep-31/100] Total time: 0:29:56 (1.2517 s / it)
[10:15:45.634487] Syncing meters...
[10:15:46.094917] Averaged stats: lr: 0.0006 (0.0006)  loss: 1259.2551 (1292.7652)  grad_norm: 673.6922 (689.1296)  amp_scale: 1.0000 (1.0000)
[10:15:57.020271] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 32)
[10:15:59.446243] [Train][Ep-32/100]  [   0/1435]  eta: 0:52:56  lr: 0.0006 (0.0006)  time: 2.2137  data: 1.7199  max mem: 13797
[10:17:58.237730] [Train][Ep-32/100]  [ 100/1435]  eta: 0:26:39  lr: 0.0006 (0.0006)  loss: 1260.3591 (1272.6323)  grad_norm: 676.5451 (674.9621)  amp_scale: 1.0000 (1.0000)  time: 1.1808  data: 0.4976  max mem: 13797
[10:20:03.698646] [Train][Ep-32/100]  [ 200/1435]  eta: 0:25:14  lr: 0.0006 (0.0006)  loss: 1331.2671 (1285.4606)  grad_norm: 691.2226 (682.2123)  amp_scale: 1.0000 (1.0000)  time: 1.2178  data: 0.3442  max mem: 13797
[10:22:09.943070] [Train][Ep-32/100]  [ 300/1435]  eta: 0:23:25  lr: 0.0006 (0.0006)  loss: 1224.3651 (1275.3178)  grad_norm: 675.0799 (680.4640)  amp_scale: 1.0000 (1.0000)  time: 1.2876  data: 0.0916  max mem: 13797
[10:24:18.772249] [Train][Ep-32/100]  [ 400/1435]  eta: 0:21:34  lr: 0.0006 (0.0006)  loss: 1313.6476 (1281.3349)  grad_norm: 671.3259 (679.3310)  amp_scale: 1.0000 (1.0000)  time: 1.3170  data: 0.7325  max mem: 13797
[10:26:27.268285] [Train][Ep-32/100]  [ 500/1435]  eta: 0:19:35  lr: 0.0006 (0.0006)  loss: 1220.1576 (1273.7562)  grad_norm: 679.8087 (681.7984)  amp_scale: 1.0000 (1.0000)  time: 1.2272  data: 0.6458  max mem: 13797
[10:28:33.652052] [Train][Ep-32/100]  [ 600/1435]  eta: 0:17:30  lr: 0.0006 (0.0006)  loss: 1264.7263 (1274.7404)  grad_norm: 690.5056 (683.0766)  amp_scale: 1.0000 (1.0000)  time: 1.2486  data: 0.6698  max mem: 13797
[10:30:35.068312] [Train][Ep-32/100]  [ 700/1435]  eta: 0:15:20  lr: 0.0006 (0.0006)  loss: 1268.8955 (1277.5265)  grad_norm: 672.6749 (683.3188)  amp_scale: 1.0000 (1.0000)  time: 1.1829  data: 0.3599  max mem: 13797
[10:32:39.361827] [Train][Ep-32/100]  [ 800/1435]  eta: 0:13:14  lr: 0.0006 (0.0006)  loss: 1271.0757 (1277.9116)  grad_norm: 684.3636 (684.6725)  amp_scale: 1.0000 (1.0000)  time: 1.2183  data: 0.4850  max mem: 13797
[10:34:41.782962] [Train][Ep-32/100]  [ 900/1435]  eta: 0:11:07  lr: 0.0006 (0.0006)  loss: 1276.3328 (1281.2607)  grad_norm: 706.9252 (687.3317)  amp_scale: 1.0000 (1.0000)  time: 1.2155  data: 0.6392  max mem: 13797
[10:36:51.134012] [Train][Ep-32/100]  [1000/1435]  eta: 0:09:04  lr: 0.0006 (0.0006)  loss: 1267.1615 (1281.7735)  grad_norm: 688.7453 (687.8832)  amp_scale: 1.0000 (1.0000)  time: 1.3351  data: 0.7577  max mem: 13797
[10:39:01.307134] [Train][Ep-32/100]  [1100/1435]  eta: 0:07:01  lr: 0.0006 (0.0006)  loss: 1253.2494 (1284.6956)  grad_norm: 689.3316 (688.8235)  amp_scale: 1.0000 (1.0000)  time: 1.3212  data: 0.7375  max mem: 13797
[10:41:01.625715] [Train][Ep-32/100]  [1200/1435]  eta: 0:04:54  lr: 0.0006 (0.0006)  loss: 1301.4045 (1284.7045)  grad_norm: 677.0171 (688.9498)  amp_scale: 1.0000 (1.0000)  time: 1.1281  data: 0.3580  max mem: 13797
[10:43:02.436752] [Train][Ep-32/100]  [1300/1435]  eta: 0:02:48  lr: 0.0006 (0.0006)  loss: 1293.4587 (1286.0346)  grad_norm: 680.5129 (687.9550)  amp_scale: 1.0000 (1.0000)  time: 1.2614  data: 0.1756  max mem: 13797
[10:45:09.262026] [Train][Ep-32/100]  [1400/1435]  eta: 0:00:43  lr: 0.0006 (0.0006)  loss: 1259.5551 (1283.9461)  grad_norm: 684.2231 (688.6130)  amp_scale: 1.0000 (1.0000)  time: 1.2008  data: 0.0766  max mem: 13797
[10:45:50.176791] [Train][Ep-32/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1279.2384 (1284.0010)  grad_norm: 680.9458 (688.3888)  amp_scale: 1.0000 (1.0000)  time: 1.2033  data: 0.3029  max mem: 13797
[10:45:50.177789] [Train][Ep-32/100] Total time: 0:29:52 (1.2494 s / it)
[10:45:50.178302] Syncing meters...
[10:45:50.181715] Averaged stats: lr: 0.0006 (0.0006)  loss: 1279.2384 (1279.3608)  grad_norm: 680.9458 (688.3888)  amp_scale: 1.0000 (1.0000)
[10:46:00.182789] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 33)
[10:46:02.646666] [Train][Ep-33/100]  [   0/1435]  eta: 0:54:48  lr: 0.0006 (0.0006)  time: 2.2915  data: 1.7985  max mem: 13797
[10:48:07.186546] [Train][Ep-33/100]  [ 100/1435]  eta: 0:27:56  lr: 0.0006 (0.0006)  loss: 1239.7531 (1276.5435)  grad_norm: 676.7100 (688.2506)  amp_scale: 1.0000 (1.0000)  time: 1.2668  data: 0.2743  max mem: 13797
[10:50:08.884477] [Train][Ep-33/100]  [ 200/1435]  eta: 0:25:26  lr: 0.0006 (0.0006)  loss: 1270.1587 (1267.7925)  grad_norm: 655.4807 (682.6079)  amp_scale: 1.0000 (1.0000)  time: 1.1695  data: 0.3530  max mem: 13797
[10:52:15.056900] [Train][Ep-33/100]  [ 300/1435]  eta: 0:23:32  lr: 0.0006 (0.0006)  loss: 1204.4645 (1259.6125)  grad_norm: 678.4701 (680.8871)  amp_scale: 1.0000 (1.0000)  time: 1.3177  data: 0.0010  max mem: 13797
[10:54:23.924005] [Train][Ep-33/100]  [ 400/1435]  eta: 0:21:39  lr: 0.0006 (0.0006)  loss: 1278.8490 (1264.9139)  grad_norm: 697.1068 (685.0791)  amp_scale: 1.0000 (1.0000)  time: 1.2844  data: 0.0007  max mem: 13797
[10:56:33.730437] [Train][Ep-33/100]  [ 500/1435]  eta: 0:19:41  lr: 0.0006 (0.0006)  loss: 1327.5273 (1266.4466)  grad_norm: 687.7031 (686.9151)  amp_scale: 1.0000 (1.0000)  time: 1.3040  data: 0.0011  max mem: 13797
[10:58:41.255441] [Train][Ep-33/100]  [ 600/1435]  eta: 0:17:37  lr: 0.0006 (0.0006)  loss: 1279.2009 (1274.1367)  grad_norm: 694.8694 (688.5079)  amp_scale: 1.0000 (1.0000)  time: 1.1876  data: 0.0115  max mem: 13797
[11:00:47.727106] [Train][Ep-33/100]  [ 700/1435]  eta: 0:15:30  lr: 0.0006 (0.0006)  loss: 1296.0178 (1278.1325)  grad_norm: 692.8362 (689.1541)  amp_scale: 1.0000 (1.0000)  time: 1.2341  data: 0.0026  max mem: 13797
[11:02:58.248745] [Train][Ep-33/100]  [ 800/1435]  eta: 0:13:26  lr: 0.0006 (0.0006)  loss: 1249.6127 (1277.4733)  grad_norm: 681.6412 (687.9514)  amp_scale: 1.0000 (1.0000)  time: 1.3328  data: 0.0006  max mem: 13797
[11:05:09.484705] [Train][Ep-33/100]  [ 900/1435]  eta: 0:11:22  lr: 0.0006 (0.0006)  loss: 1307.4250 (1279.2559)  grad_norm: 685.9004 (689.1604)  amp_scale: 1.0000 (1.0000)  time: 1.3264  data: 0.0010  max mem: 13797
[11:07:20.802740] [Train][Ep-33/100]  [1000/1435]  eta: 0:09:16  lr: 0.0006 (0.0006)  loss: 1286.6917 (1278.9555)  grad_norm: 671.5604 (688.8560)  amp_scale: 1.0000 (1.0000)  time: 1.3813  data: 0.0003  max mem: 13797
[11:09:27.092318] [Train][Ep-33/100]  [1100/1435]  eta: 0:07:08  lr: 0.0006 (0.0006)  loss: 1285.2386 (1279.2128)  grad_norm: 691.0168 (690.1591)  amp_scale: 1.0000 (1.0000)  time: 1.2347  data: 0.0005  max mem: 13797
[11:11:37.775788] [Train][Ep-33/100]  [1200/1435]  eta: 0:05:00  lr: 0.0006 (0.0006)  loss: 1259.3986 (1278.1908)  grad_norm: 681.9478 (689.5996)  amp_scale: 1.0000 (1.0000)  time: 1.3064  data: 0.1086  max mem: 13797
[11:13:46.755102] [Train][Ep-33/100]  [1300/1435]  eta: 0:02:52  lr: 0.0006 (0.0006)  loss: 1328.7919 (1278.5394)  grad_norm: 699.4519 (690.2342)  amp_scale: 1.0000 (1.0000)  time: 1.2866  data: 0.1355  max mem: 13797
[11:15:55.297065] [Train][Ep-33/100]  [1400/1435]  eta: 0:00:44  lr: 0.0006 (0.0006)  loss: 1204.3593 (1276.8141)  grad_norm: 693.8258 (690.9969)  amp_scale: 1.0000 (1.0000)  time: 1.3402  data: 0.0005  max mem: 13797
[11:16:39.042574] [Train][Ep-33/100]  [1434/1435]  eta: 0:00:01  lr: 0.0006 (0.0006)  loss: 1242.2622 (1276.9383)  grad_norm: 688.7782 (690.8203)  amp_scale: 1.0000 (1.0000)  time: 1.3362  data: 0.0006  max mem: 13797
[11:16:39.043631] [Train][Ep-33/100] Total time: 0:30:38 (1.2813 s / it)
[11:16:39.044041] Syncing meters...
[11:16:39.652059] Averaged stats: lr: 0.0006 (0.0006)  loss: 1242.2622 (1277.4710)  grad_norm: 688.7782 (690.8203)  amp_scale: 1.0000 (1.0000)
[11:16:51.281097] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 34)
[11:16:53.731584] [Train][Ep-34/100]  [   0/1435]  eta: 0:53:44  lr: 0.0006 (0.0006)  time: 2.2472  data: 1.7535  max mem: 13797
[11:19:01.031674] [Train][Ep-34/100]  [ 100/1435]  eta: 0:28:32  lr: 0.0006 (0.0006)  loss: 1285.9659 (1275.5924)  grad_norm: 684.7055 (689.1292)  amp_scale: 1.0000 (1.0000)  time: 1.3258  data: 0.7484  max mem: 13797
[11:21:11.722410] [Train][Ep-34/100]  [ 200/1435]  eta: 0:26:38  lr: 0.0006 (0.0006)  loss: 1313.4719 (1278.7885)  grad_norm: 687.6011 (686.8028)  amp_scale: 1.0000 (1.0000)  time: 1.3129  data: 0.7291  max mem: 13797
[11:23:16.443343] [Train][Ep-34/100]  [ 300/1435]  eta: 0:24:11  lr: 0.0006 (0.0006)  loss: 1239.9984 (1268.0458)  grad_norm: 722.3700 (695.1211)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.5308  max mem: 13797
[11:25:24.533013] [Train][Ep-34/100]  [ 400/1435]  eta: 0:22:04  lr: 0.0006 (0.0006)  loss: 1302.7023 (1271.7832)  grad_norm: 673.2371 (690.5035)  amp_scale: 1.0000 (1.0000)  time: 1.2791  data: 0.6614  max mem: 13797
[11:27:29.595777] [Train][Ep-34/100]  [ 500/1435]  eta: 0:19:50  lr: 0.0006 (0.0006)  loss: 1215.7423 (1265.0337)  grad_norm: 669.6983 (692.4932)  amp_scale: 1.0000 (1.0000)  time: 1.2487  data: 0.6726  max mem: 13797
[11:29:30.149242] [Train][Ep-34/100]  [ 600/1435]  eta: 0:17:33  lr: 0.0006 (0.0006)  loss: 1243.2749 (1263.9968)  grad_norm: 666.2769 (691.4154)  amp_scale: 1.0000 (1.0000)  time: 1.1931  data: 0.0008  max mem: 13797
[11:31:36.650683] [Train][Ep-34/100]  [ 700/1435]  eta: 0:15:28  lr: 0.0006 (0.0006)  loss: 1248.6295 (1265.3057)  grad_norm: 680.5613 (690.6232)  amp_scale: 1.0000 (1.0000)  time: 1.3235  data: 0.0006  max mem: 13797
[11:33:45.085209] [Train][Ep-34/100]  [ 800/1435]  eta: 0:13:23  lr: 0.0006 (0.0006)  loss: 1242.7552 (1264.0885)  grad_norm: 695.8754 (691.2177)  amp_scale: 1.0000 (1.0000)  time: 1.2663  data: 0.0006  max mem: 13797
[11:35:50.779827] [Train][Ep-34/100]  [ 900/1435]  eta: 0:11:16  lr: 0.0006 (0.0006)  loss: 1234.8136 (1262.0011)  grad_norm: 671.3368 (690.7622)  amp_scale: 1.0000 (1.0000)  time: 1.2948  data: 0.0008  max mem: 13797
[11:37:59.641470] [Train][Ep-34/100]  [1000/1435]  eta: 0:09:11  lr: 0.0006 (0.0006)  loss: 1296.0317 (1263.0158)  grad_norm: 683.9363 (690.4946)  amp_scale: 1.0000 (1.0000)  time: 1.2807  data: 0.0018  max mem: 13797
[11:40:06.805489] [Train][Ep-34/100]  [1100/1435]  eta: 0:07:04  lr: 0.0006 (0.0006)  loss: 1294.1527 (1265.6227)  grad_norm: 669.4713 (689.2486)  amp_scale: 1.0000 (1.0000)  time: 1.3190  data: 0.0053  max mem: 13797
[11:42:16.655841] [Train][Ep-34/100]  [1200/1435]  eta: 0:04:58  lr: 0.0006 (0.0006)  loss: 1300.2729 (1267.4739)  grad_norm: 661.0552 (687.5159)  amp_scale: 1.0000 (1.0000)  time: 1.2892  data: 0.0006  max mem: 13797
[11:44:23.511009] [Train][Ep-34/100]  [1300/1435]  eta: 0:02:51  lr: 0.0006 (0.0006)  loss: 1304.6919 (1270.8009)  grad_norm: 680.5441 (687.2347)  amp_scale: 1.0000 (1.0000)  time: 1.2553  data: 0.0003  max mem: 13797
[11:46:28.000433] [Train][Ep-34/100]  [1400/1435]  eta: 0:00:44  lr: 0.0005 (0.0006)  loss: 1293.5625 (1273.6963)  grad_norm: 674.1171 (687.5437)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.0003  max mem: 13797
[11:47:10.261780] [Train][Ep-34/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0006)  loss: 1264.9386 (1274.2081)  grad_norm: 687.1289 (687.4804)  amp_scale: 1.0000 (1.0000)  time: 1.2633  data: 0.0019  max mem: 13797
[11:47:10.262779] [Train][Ep-34/100] Total time: 0:30:18 (1.2674 s / it)
[11:47:10.263272] Syncing meters...
[11:47:11.425343] Averaged stats: lr: 0.0005 (0.0006)  loss: 1264.9386 (1269.8790)  grad_norm: 687.1289 (687.4804)  amp_scale: 1.0000 (1.0000)
[11:47:21.619713] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 35)
[11:47:23.761790] [Train][Ep-35/100]  [   0/1435]  eta: 0:46:32  lr: 0.0005 (0.0005)  time: 1.9459  data: 1.4522  max mem: 13797
[11:49:20.003080] [Train][Ep-35/100]  [ 100/1435]  eta: 0:26:02  lr: 0.0005 (0.0005)  loss: 1253.4316 (1245.3362)  grad_norm: 698.7333 (694.7795)  amp_scale: 1.0000 (1.0000)  time: 1.2112  data: 0.0006  max mem: 13797
[11:51:23.767811] [Train][Ep-35/100]  [ 200/1435]  eta: 0:24:46  lr: 0.0005 (0.0005)  loss: 1215.8130 (1252.8974)  grad_norm: 672.1859 (689.6912)  amp_scale: 1.0000 (1.0000)  time: 1.2297  data: 0.0094  max mem: 13797
[11:53:27.349146] [Train][Ep-35/100]  [ 300/1435]  eta: 0:22:58  lr: 0.0005 (0.0005)  loss: 1248.3259 (1248.3849)  grad_norm: 682.2589 (687.8230)  amp_scale: 1.0000 (1.0000)  time: 1.2424  data: 0.1118  max mem: 13797
[11:55:27.697299] [Train][Ep-35/100]  [ 400/1435]  eta: 0:20:54  lr: 0.0005 (0.0005)  loss: 1309.2052 (1250.5250)  grad_norm: 671.7872 (686.4739)  amp_scale: 1.0000 (1.0000)  time: 1.1947  data: 0.0012  max mem: 13797
[11:57:32.329952] [Train][Ep-35/100]  [ 500/1435]  eta: 0:18:59  lr: 0.0005 (0.0005)  loss: 1319.8413 (1261.4701)  grad_norm: 670.6055 (684.3274)  amp_scale: 1.0000 (1.0000)  time: 1.2896  data: 0.0005  max mem: 13797
[11:59:31.412519] [Train][Ep-35/100]  [ 600/1435]  eta: 0:16:53  lr: 0.0005 (0.0005)  loss: 1244.2654 (1262.7855)  grad_norm: 685.5494 (683.5978)  amp_scale: 1.0000 (1.0000)  time: 1.2411  data: 0.0351  max mem: 13797
[12:01:35.854821] [Train][Ep-35/100]  [ 700/1435]  eta: 0:14:55  lr: 0.0005 (0.0005)  loss: 1261.9464 (1264.4082)  grad_norm: 678.2693 (683.7917)  amp_scale: 1.0000 (1.0000)  time: 1.2753  data: 0.6947  max mem: 13797
[12:03:52.444276] [Train][Ep-35/100]  [ 800/1435]  eta: 0:13:05  lr: 0.0005 (0.0005)  loss: 1282.6610 (1264.5573)  grad_norm: 711.0338 (687.4762)  amp_scale: 1.0000 (1.0000)  time: 1.4323  data: 0.8523  max mem: 13797
[12:06:06.981566] [Train][Ep-35/100]  [ 900/1435]  eta: 0:11:08  lr: 0.0005 (0.0005)  loss: 1298.1779 (1267.0665)  grad_norm: 677.0106 (686.6658)  amp_scale: 1.0000 (1.0000)  time: 1.3809  data: 0.8037  max mem: 13797
[12:08:16.696953] [Train][Ep-35/100]  [1000/1435]  eta: 0:09:05  lr: 0.0005 (0.0005)  loss: 1305.1285 (1269.7116)  grad_norm: 684.6118 (687.8035)  amp_scale: 1.0000 (1.0000)  time: 1.2483  data: 0.6711  max mem: 13797
[12:10:22.506983] [Train][Ep-35/100]  [1100/1435]  eta: 0:07:00  lr: 0.0005 (0.0005)  loss: 1204.5125 (1267.4171)  grad_norm: 684.9717 (687.7856)  amp_scale: 1.0000 (1.0000)  time: 1.2458  data: 0.6690  max mem: 13797
[12:12:28.609140] [Train][Ep-35/100]  [1200/1435]  eta: 0:04:54  lr: 0.0005 (0.0005)  loss: 1263.0171 (1266.7870)  grad_norm: 696.7161 (688.2489)  amp_scale: 1.0000 (1.0000)  time: 1.2504  data: 0.6714  max mem: 13797
[12:14:33.827436] [Train][Ep-35/100]  [1300/1435]  eta: 0:02:49  lr: 0.0005 (0.0005)  loss: 1248.4088 (1267.1967)  grad_norm: 686.9103 (688.0646)  amp_scale: 1.0000 (1.0000)  time: 1.2645  data: 0.6235  max mem: 13797
[12:16:39.628995] [Train][Ep-35/100]  [1400/1435]  eta: 0:00:43  lr: 0.0005 (0.0005)  loss: 1280.7571 (1266.9636)  grad_norm: 694.5826 (688.8501)  amp_scale: 1.0000 (1.0000)  time: 1.2795  data: 0.5154  max mem: 13797
[12:17:22.412133] [Train][Ep-35/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1255.6398 (1267.1624)  grad_norm: 686.1613 (689.0082)  amp_scale: 1.0000 (1.0000)  time: 1.3086  data: 0.0122  max mem: 13797
[12:17:22.413271] [Train][Ep-35/100] Total time: 0:30:00 (1.2548 s / it)
[12:17:22.413774] Syncing meters...
[12:17:22.785596] Averaged stats: lr: 0.0005 (0.0005)  loss: 1255.6398 (1268.3230)  grad_norm: 686.1613 (689.0082)  amp_scale: 1.0000 (1.0000)
[12:17:32.786448] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 36)
[12:17:35.101637] [Train][Ep-36/100]  [   0/1435]  eta: 0:49:40  lr: 0.0005 (0.0005)  time: 2.0767  data: 1.5844  max mem: 13797
[12:19:44.501444] [Train][Ep-36/100]  [ 100/1435]  eta: 0:28:57  lr: 0.0005 (0.0005)  loss: 1207.4304 (1226.5819)  grad_norm: 688.1593 (691.8258)  amp_scale: 1.0000 (1.0000)  time: 1.2946  data: 0.7118  max mem: 13797
[12:21:50.678732] [Train][Ep-36/100]  [ 200/1435]  eta: 0:26:23  lr: 0.0005 (0.0005)  loss: 1207.8756 (1228.2701)  grad_norm: 673.8258 (685.9460)  amp_scale: 1.0000 (1.0000)  time: 1.1835  data: 0.6036  max mem: 13797
[12:23:50.610304] [Train][Ep-36/100]  [ 300/1435]  eta: 0:23:43  lr: 0.0005 (0.0005)  loss: 1233.3448 (1232.5790)  grad_norm: 679.1901 (686.7813)  amp_scale: 1.0000 (1.0000)  time: 1.2498  data: 0.4550  max mem: 13797
[12:25:59.245315] [Train][Ep-36/100]  [ 400/1435]  eta: 0:21:46  lr: 0.0005 (0.0005)  loss: 1201.0472 (1232.6443)  grad_norm: 681.3522 (688.4360)  amp_scale: 1.0000 (1.0000)  time: 1.2940  data: 0.7165  max mem: 13797
[12:27:59.797009] [Train][Ep-36/100]  [ 500/1435]  eta: 0:19:29  lr: 0.0005 (0.0005)  loss: 1269.2316 (1238.9609)  grad_norm: 671.2955 (684.6863)  amp_scale: 1.0000 (1.0000)  time: 1.1897  data: 0.4446  max mem: 13797
[12:30:00.267817] [Train][Ep-36/100]  [ 600/1435]  eta: 0:17:18  lr: 0.0005 (0.0005)  loss: 1282.1141 (1246.5134)  grad_norm: 670.2457 (683.1372)  amp_scale: 1.0000 (1.0000)  time: 1.2462  data: 0.6329  max mem: 13797
[12:32:07.017127] [Train][Ep-36/100]  [ 700/1435]  eta: 0:15:16  lr: 0.0005 (0.0005)  loss: 1236.8413 (1244.4304)  grad_norm: 674.5737 (682.5831)  amp_scale: 1.0000 (1.0000)  time: 1.2812  data: 0.7021  max mem: 13797
[12:34:12.454475] [Train][Ep-36/100]  [ 800/1435]  eta: 0:13:12  lr: 0.0005 (0.0005)  loss: 1239.2408 (1248.0449)  grad_norm: 696.1796 (683.6348)  amp_scale: 1.0000 (1.0000)  time: 1.2949  data: 0.7142  max mem: 13797
[12:36:15.490456] [Train][Ep-36/100]  [ 900/1435]  eta: 0:11:06  lr: 0.0005 (0.0005)  loss: 1263.7794 (1252.4371)  grad_norm: 681.7761 (682.9437)  amp_scale: 1.0000 (1.0000)  time: 1.3451  data: 0.7705  max mem: 13797
[12:38:27.151287] [Train][Ep-36/100]  [1000/1435]  eta: 0:09:04  lr: 0.0005 (0.0005)  loss: 1247.4968 (1252.7044)  grad_norm: 708.1599 (683.9329)  amp_scale: 1.0000 (1.0000)  time: 1.2967  data: 0.7217  max mem: 13797
[12:40:33.839938] [Train][Ep-36/100]  [1100/1435]  eta: 0:07:00  lr: 0.0005 (0.0005)  loss: 1274.8876 (1255.2754)  grad_norm: 695.3653 (684.4300)  amp_scale: 1.0000 (1.0000)  time: 1.2272  data: 0.6487  max mem: 13797
[12:42:39.816844] [Train][Ep-36/100]  [1200/1435]  eta: 0:04:54  lr: 0.0005 (0.0005)  loss: 1272.4355 (1258.6784)  grad_norm: 678.6080 (684.9339)  amp_scale: 1.0000 (1.0000)  time: 1.1855  data: 0.5207  max mem: 13797
[12:44:43.948352] [Train][Ep-36/100]  [1300/1435]  eta: 0:02:49  lr: 0.0005 (0.0005)  loss: 1247.8145 (1256.0258)  grad_norm: 676.6914 (684.2042)  amp_scale: 1.0000 (1.0000)  time: 1.2547  data: 0.6708  max mem: 13797
[12:46:52.323019] [Train][Ep-36/100]  [1400/1435]  eta: 0:00:43  lr: 0.0005 (0.0005)  loss: 1287.8019 (1258.3948)  grad_norm: 705.3151 (685.2415)  amp_scale: 1.0000 (1.0000)  time: 1.3595  data: 0.7820  max mem: 13797
[12:47:33.379255] [Train][Ep-36/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1257.2302 (1258.4288)  grad_norm: 704.2365 (685.8926)  amp_scale: 1.0000 (1.0000)  time: 1.1576  data: 0.5777  max mem: 13797
[12:47:33.380584] [Train][Ep-36/100] Total time: 0:30:00 (1.2546 s / it)
[12:47:33.381013] Syncing meters...
[12:47:33.382917] Averaged stats: lr: 0.0005 (0.0005)  loss: 1257.2302 (1257.3142)  grad_norm: 704.2365 (685.8926)  amp_scale: 1.0000 (1.0000)
[12:47:44.010028] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 37)
[12:47:46.302474] [Train][Ep-37/100]  [   0/1435]  eta: 0:49:11  lr: 0.0005 (0.0005)  time: 2.0570  data: 1.5654  max mem: 13797
[12:49:48.397956] [Train][Ep-37/100]  [ 100/1435]  eta: 0:27:20  lr: 0.0005 (0.0005)  loss: 1249.1859 (1229.1736)  grad_norm: 670.9127 (671.8821)  amp_scale: 1.0000 (1.0000)  time: 1.1878  data: 0.5862  max mem: 13797
[12:51:50.788639] [Train][Ep-37/100]  [ 200/1435]  eta: 0:25:14  lr: 0.0005 (0.0005)  loss: 1234.6262 (1234.7976)  grad_norm: 685.5646 (690.6808)  amp_scale: 1.0000 (1.0000)  time: 1.2270  data: 0.2215  max mem: 13797
[12:53:49.276657] [Train][Ep-37/100]  [ 300/1435]  eta: 0:22:56  lr: 0.0005 (0.0005)  loss: 1219.2778 (1235.5558)  grad_norm: 663.7913 (690.6986)  amp_scale: 1.0000 (1.0000)  time: 1.1913  data: 0.3941  max mem: 13797
[12:55:50.473825] [Train][Ep-37/100]  [ 400/1435]  eta: 0:20:54  lr: 0.0005 (0.0005)  loss: 1270.3584 (1246.1043)  grad_norm: 695.0602 (691.5921)  amp_scale: 1.0000 (1.0000)  time: 1.1798  data: 0.5943  max mem: 13797
[12:57:45.088559] [Train][Ep-37/100]  [ 500/1435]  eta: 0:18:41  lr: 0.0005 (0.0005)  loss: 1180.0111 (1242.6591)  grad_norm: 671.2687 (689.0444)  amp_scale: 1.0000 (1.0000)  time: 1.1785  data: 0.4961  max mem: 13797
[12:59:41.803982] [Train][Ep-37/100]  [ 600/1435]  eta: 0:16:36  lr: 0.0005 (0.0005)  loss: 1254.6685 (1243.5675)  grad_norm: 673.8687 (687.3433)  amp_scale: 1.0000 (1.0000)  time: 1.1721  data: 0.1036  max mem: 13797
[13:01:40.073454] [Train][Ep-37/100]  [ 700/1435]  eta: 0:14:36  lr: 0.0005 (0.0005)  loss: 1296.6659 (1250.8206)  grad_norm: 691.3239 (687.3812)  amp_scale: 1.0000 (1.0000)  time: 1.1509  data: 0.0053  max mem: 13797
[13:03:39.617066] [Train][Ep-37/100]  [ 800/1435]  eta: 0:12:37  lr: 0.0005 (0.0005)  loss: 1218.7019 (1249.6122)  grad_norm: 672.6356 (687.5057)  amp_scale: 1.0000 (1.0000)  time: 1.1851  data: 0.0632  max mem: 13797
[13:05:43.105284] [Train][Ep-37/100]  [ 900/1435]  eta: 0:10:40  lr: 0.0005 (0.0005)  loss: 1243.2706 (1250.4751)  grad_norm: 680.1220 (687.0164)  amp_scale: 1.0000 (1.0000)  time: 1.2157  data: 0.3443  max mem: 13797
[13:07:42.719585] [Train][Ep-37/100]  [1000/1435]  eta: 0:08:40  lr: 0.0005 (0.0005)  loss: 1250.9182 (1252.5808)  grad_norm: 682.5895 (687.6638)  amp_scale: 1.0000 (1.0000)  time: 1.1877  data: 0.0118  max mem: 13797
[13:09:44.149914] [Train][Ep-37/100]  [1100/1435]  eta: 0:06:41  lr: 0.0005 (0.0005)  loss: 1260.0819 (1254.2554)  grad_norm: 684.1370 (688.0424)  amp_scale: 1.0000 (1.0000)  time: 1.1588  data: 0.0230  max mem: 13797
[13:11:45.619842] [Train][Ep-37/100]  [1200/1435]  eta: 0:04:42  lr: 0.0005 (0.0005)  loss: 1232.1477 (1254.1774)  grad_norm: 675.1982 (687.9868)  amp_scale: 1.0000 (1.0000)  time: 1.1436  data: 0.0584  max mem: 13797
[13:13:50.583527] [Train][Ep-37/100]  [1300/1435]  eta: 0:02:42  lr: 0.0005 (0.0005)  loss: 1291.3956 (1256.3276)  grad_norm: 704.3731 (688.6644)  amp_scale: 1.0000 (1.0000)  time: 1.2601  data: 0.2670  max mem: 13797
[13:15:58.711119] [Train][Ep-37/100]  [1400/1435]  eta: 0:00:42  lr: 0.0005 (0.0005)  loss: 1208.5946 (1255.0897)  grad_norm: 677.6418 (687.9373)  amp_scale: 1.0000 (1.0000)  time: 1.2112  data: 0.6313  max mem: 13797
[13:16:39.554492] [Train][Ep-37/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1220.2821 (1254.6981)  grad_norm: 681.4896 (687.9400)  amp_scale: 1.0000 (1.0000)  time: 1.2013  data: 0.6258  max mem: 13797
[13:16:39.555844] [Train][Ep-37/100] Total time: 0:28:55 (1.2093 s / it)
[13:16:39.556342] Syncing meters...
[13:16:39.558460] Averaged stats: lr: 0.0005 (0.0005)  loss: 1220.2821 (1253.0696)  grad_norm: 681.4896 (687.9400)  amp_scale: 1.0000 (1.0000)
[13:16:50.181146] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 38)
[13:16:52.591772] [Train][Ep-38/100]  [   0/1435]  eta: 0:52:03  lr: 0.0005 (0.0005)  time: 2.1769  data: 1.6820  max mem: 13797
[13:18:54.237291] [Train][Ep-38/100]  [ 100/1435]  eta: 0:27:16  lr: 0.0005 (0.0005)  loss: 1199.4380 (1213.0226)  grad_norm: 664.8676 (670.1869)  amp_scale: 1.0000 (1.0000)  time: 1.2523  data: 0.6629  max mem: 13797
[13:21:03.512895] [Train][Ep-38/100]  [ 200/1435]  eta: 0:25:55  lr: 0.0005 (0.0005)  loss: 1269.8177 (1229.9415)  grad_norm: 698.1812 (682.2796)  amp_scale: 1.0000 (1.0000)  time: 1.2754  data: 0.6961  max mem: 13797
[13:23:08.403503] [Train][Ep-38/100]  [ 300/1435]  eta: 0:23:45  lr: 0.0005 (0.0005)  loss: 1203.3748 (1224.4285)  grad_norm: 670.3426 (680.2935)  amp_scale: 1.0000 (1.0000)  time: 1.2756  data: 0.6944  max mem: 13797
[13:25:17.922394] [Train][Ep-38/100]  [ 400/1435]  eta: 0:21:49  lr: 0.0005 (0.0005)  loss: 1260.1404 (1229.8562)  grad_norm: 677.9650 (680.5468)  amp_scale: 1.0000 (1.0000)  time: 1.3385  data: 0.7624  max mem: 13797
[13:27:22.597527] [Train][Ep-38/100]  [ 500/1435]  eta: 0:19:39  lr: 0.0005 (0.0005)  loss: 1215.8979 (1231.6535)  grad_norm: 671.6846 (680.1306)  amp_scale: 1.0000 (1.0000)  time: 1.2010  data: 0.6180  max mem: 13797
[13:29:28.159267] [Train][Ep-38/100]  [ 600/1435]  eta: 0:17:32  lr: 0.0005 (0.0005)  loss: 1238.6475 (1232.0197)  grad_norm: 683.5335 (680.6934)  amp_scale: 1.0000 (1.0000)  time: 1.2494  data: 0.6709  max mem: 13797
[13:31:34.571989] [Train][Ep-38/100]  [ 700/1435]  eta: 0:15:26  lr: 0.0005 (0.0005)  loss: 1275.3346 (1237.0954)  grad_norm: 693.9789 (682.7229)  amp_scale: 1.0000 (1.0000)  time: 1.1730  data: 0.5941  max mem: 13797
[13:33:39.453581] [Train][Ep-38/100]  [ 800/1435]  eta: 0:13:19  lr: 0.0005 (0.0005)  loss: 1204.2458 (1238.2832)  grad_norm: 685.5272 (682.8638)  amp_scale: 1.0000 (1.0000)  time: 1.2962  data: 0.7164  max mem: 13797
[13:35:46.368133] [Train][Ep-38/100]  [ 900/1435]  eta: 0:11:14  lr: 0.0005 (0.0005)  loss: 1243.8997 (1238.2546)  grad_norm: 702.6178 (684.8769)  amp_scale: 1.0000 (1.0000)  time: 1.2624  data: 0.6779  max mem: 13797
[13:37:52.170911] [Train][Ep-38/100]  [1000/1435]  eta: 0:09:08  lr: 0.0005 (0.0005)  loss: 1211.6056 (1235.3032)  grad_norm: 688.6047 (685.5415)  amp_scale: 1.0000 (1.0000)  time: 1.2857  data: 0.4681  max mem: 13797
[13:39:57.753476] [Train][Ep-38/100]  [1100/1435]  eta: 0:07:02  lr: 0.0005 (0.0005)  loss: 1265.3007 (1240.0361)  grad_norm: 687.7584 (686.1712)  amp_scale: 1.0000 (1.0000)  time: 1.2201  data: 0.6385  max mem: 13797
[13:42:04.577697] [Train][Ep-38/100]  [1200/1435]  eta: 0:04:56  lr: 0.0005 (0.0005)  loss: 1232.8217 (1238.4016)  grad_norm: 691.1376 (686.5486)  amp_scale: 1.0000 (1.0000)  time: 1.2649  data: 0.6884  max mem: 13797
[13:44:04.944759] [Train][Ep-38/100]  [1300/1435]  eta: 0:02:49  lr: 0.0005 (0.0005)  loss: 1269.7346 (1241.4240)  grad_norm: 695.5483 (687.1568)  amp_scale: 1.0000 (1.0000)  time: 1.2824  data: 0.1200  max mem: 13797
[13:46:07.331789] [Train][Ep-38/100]  [1400/1435]  eta: 0:00:43  lr: 0.0005 (0.0005)  loss: 1262.4799 (1242.1047)  grad_norm: 670.7213 (686.4788)  amp_scale: 1.0000 (1.0000)  time: 1.1758  data: 0.4218  max mem: 13797
[13:46:46.968256] [Train][Ep-38/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1255.9620 (1242.4634)  grad_norm: 670.4893 (685.9372)  amp_scale: 1.0000 (1.0000)  time: 1.1396  data: 0.5618  max mem: 13797
[13:46:46.969179] [Train][Ep-38/100] Total time: 0:29:56 (1.2520 s / it)
[13:46:46.969654] Syncing meters...
[13:46:46.971127] Averaged stats: lr: 0.0005 (0.0005)  loss: 1255.9620 (1245.0694)  grad_norm: 670.4893 (685.9372)  amp_scale: 1.0000 (1.0000)
[13:46:57.609909] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 39)
[13:46:59.962072] [Train][Ep-39/100]  [   0/1435]  eta: 0:52:20  lr: 0.0005 (0.0005)  time: 2.1884  data: 1.6967  max mem: 13797
[13:49:06.689693] [Train][Ep-39/100]  [ 100/1435]  eta: 0:28:23  lr: 0.0005 (0.0005)  loss: 1276.5156 (1279.9622)  grad_norm: 668.2611 (668.0844)  amp_scale: 1.0000 (1.0000)  time: 1.3216  data: 0.7443  max mem: 13797
[13:51:11.898611] [Train][Ep-39/100]  [ 200/1435]  eta: 0:26:01  lr: 0.0005 (0.0005)  loss: 1239.0731 (1268.8490)  grad_norm: 696.2599 (685.3611)  amp_scale: 1.0000 (1.0000)  time: 1.2304  data: 0.6449  max mem: 13797
[13:53:14.722657] [Train][Ep-39/100]  [ 300/1435]  eta: 0:23:41  lr: 0.0005 (0.0005)  loss: 1262.4473 (1256.6241)  grad_norm: 682.4824 (685.0653)  amp_scale: 1.0000 (1.0000)  time: 1.2794  data: 0.6975  max mem: 13797
[13:55:16.136116] [Train][Ep-39/100]  [ 400/1435]  eta: 0:21:26  lr: 0.0005 (0.0005)  loss: 1171.4954 (1251.2203)  grad_norm: 662.7900 (683.1944)  amp_scale: 1.0000 (1.0000)  time: 1.1769  data: 0.2896  max mem: 13797
[13:57:24.373017] [Train][Ep-39/100]  [ 500/1435]  eta: 0:19:29  lr: 0.0005 (0.0005)  loss: 1268.4688 (1252.9073)  grad_norm: 687.7574 (683.6046)  amp_scale: 1.0000 (1.0000)  time: 1.3271  data: 0.1791  max mem: 13797
[13:59:35.129175] [Train][Ep-39/100]  [ 600/1435]  eta: 0:17:32  lr: 0.0005 (0.0005)  loss: 1238.4408 (1253.6845)  grad_norm: 673.7575 (683.2009)  amp_scale: 1.0000 (1.0000)  time: 1.2741  data: 0.1352  max mem: 13797
[14:01:38.883123] [Train][Ep-39/100]  [ 700/1435]  eta: 0:15:23  lr: 0.0005 (0.0005)  loss: 1191.6642 (1247.8713)  grad_norm: 689.7701 (684.8589)  amp_scale: 1.0000 (1.0000)  time: 1.1955  data: 0.4733  max mem: 13797
[14:03:46.955429] [Train][Ep-39/100]  [ 800/1435]  eta: 0:13:19  lr: 0.0005 (0.0005)  loss: 1189.8783 (1241.5245)  grad_norm: 680.2831 (685.9578)  amp_scale: 1.0000 (1.0000)  time: 1.3275  data: 0.7466  max mem: 13797
[14:05:52.127141] [Train][Ep-39/100]  [ 900/1435]  eta: 0:11:13  lr: 0.0005 (0.0005)  loss: 1209.2120 (1241.7028)  grad_norm: 694.1448 (687.1927)  amp_scale: 1.0000 (1.0000)  time: 1.2072  data: 0.6295  max mem: 13797
[14:07:50.849373] [Train][Ep-39/100]  [1000/1435]  eta: 0:09:04  lr: 0.0005 (0.0005)  loss: 1209.6432 (1244.1045)  grad_norm: 666.2892 (686.5008)  amp_scale: 1.0000 (1.0000)  time: 1.1735  data: 0.5831  max mem: 13797
[14:09:52.585645] [Train][Ep-39/100]  [1100/1435]  eta: 0:06:58  lr: 0.0005 (0.0005)  loss: 1203.7017 (1243.8796)  grad_norm: 666.1516 (685.1896)  amp_scale: 1.0000 (1.0000)  time: 1.1475  data: 0.3805  max mem: 13797
[14:11:52.005494] [Train][Ep-39/100]  [1200/1435]  eta: 0:04:52  lr: 0.0005 (0.0005)  loss: 1214.7635 (1242.1379)  grad_norm: 679.5368 (685.9592)  amp_scale: 1.0000 (1.0000)  time: 1.1881  data: 0.3858  max mem: 13797
[14:13:57.962577] [Train][Ep-39/100]  [1300/1435]  eta: 0:02:48  lr: 0.0005 (0.0005)  loss: 1241.3977 (1243.9257)  grad_norm: 671.0602 (685.3847)  amp_scale: 1.0000 (1.0000)  time: 1.2429  data: 0.6564  max mem: 13797
[14:16:08.730310] [Train][Ep-39/100]  [1400/1435]  eta: 0:00:43  lr: 0.0005 (0.0005)  loss: 1246.0190 (1245.8021)  grad_norm: 661.4219 (684.4420)  amp_scale: 1.0000 (1.0000)  time: 1.3471  data: 0.7635  max mem: 13797
[14:16:51.272989] [Train][Ep-39/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1243.1382 (1245.2117)  grad_norm: 660.2484 (687.6575)  amp_scale: 1.0000 (1.0000)  time: 1.2228  data: 0.6429  max mem: 13797
[14:16:51.273983] [Train][Ep-39/100] Total time: 0:29:53 (1.2498 s / it)
[14:16:51.274460] Syncing meters...
[14:16:51.276713] Averaged stats: lr: 0.0005 (0.0005)  loss: 1243.1382 (1239.2551)  grad_norm: 660.2484 (687.6575)  amp_scale: 1.0000 (1.0000)
[14:17:01.060383] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 40)
[14:17:03.329868] [Train][Ep-40/100]  [   0/1435]  eta: 0:50:29  lr: 0.0005 (0.0005)  time: 2.1109  data: 1.6187  max mem: 13797
[14:19:09.616488] [Train][Ep-40/100]  [ 100/1435]  eta: 0:28:17  lr: 0.0005 (0.0005)  loss: 1199.4244 (1219.3680)  grad_norm: 675.4667 (696.6177)  amp_scale: 1.0000 (1.0000)  time: 1.1750  data: 0.5970  max mem: 13797
[14:21:14.252424] [Train][Ep-40/100]  [ 200/1435]  eta: 0:25:54  lr: 0.0005 (0.0005)  loss: 1197.2629 (1218.1009)  grad_norm: 673.7546 (689.9196)  amp_scale: 1.0000 (1.0000)  time: 1.1988  data: 0.6217  max mem: 13797
[14:23:19.768300] [Train][Ep-40/100]  [ 300/1435]  eta: 0:23:47  lr: 0.0005 (0.0005)  loss: 1207.1277 (1216.2679)  grad_norm: 689.0256 (686.9935)  amp_scale: 1.0000 (1.0000)  time: 1.3166  data: 0.7373  max mem: 13797
[14:25:27.535987] [Train][Ep-40/100]  [ 400/1435]  eta: 0:21:46  lr: 0.0005 (0.0005)  loss: 1208.4073 (1217.0714)  grad_norm: 685.9828 (686.5533)  amp_scale: 1.0000 (1.0000)  time: 1.2139  data: 0.6198  max mem: 13797
[14:27:34.210851] [Train][Ep-40/100]  [ 500/1435]  eta: 0:19:41  lr: 0.0005 (0.0005)  loss: 1241.6165 (1228.0695)  grad_norm: 687.9211 (692.1667)  amp_scale: 1.0000 (1.0000)  time: 1.2306  data: 0.6434  max mem: 13797
[14:29:42.382971] [Train][Ep-40/100]  [ 600/1435]  eta: 0:17:37  lr: 0.0005 (0.0005)  loss: 1280.5356 (1234.2705)  grad_norm: 683.7275 (692.3657)  amp_scale: 1.0000 (1.0000)  time: 1.2395  data: 0.6397  max mem: 13797
[14:31:45.560539] [Train][Ep-40/100]  [ 700/1435]  eta: 0:15:27  lr: 0.0005 (0.0005)  loss: 1227.0630 (1234.4770)  grad_norm: 685.8651 (691.7200)  amp_scale: 1.0000 (1.0000)  time: 1.2235  data: 0.6450  max mem: 13797
[14:33:55.206439] [Train][Ep-40/100]  [ 800/1435]  eta: 0:13:23  lr: 0.0005 (0.0005)  loss: 1215.5874 (1233.9646)  grad_norm: 686.7591 (690.8414)  amp_scale: 1.0000 (1.0000)  time: 1.2567  data: 0.6777  max mem: 13797
[14:36:03.801612] [Train][Ep-40/100]  [ 900/1435]  eta: 0:11:18  lr: 0.0005 (0.0005)  loss: 1213.4082 (1231.5106)  grad_norm: 686.7786 (689.7208)  amp_scale: 1.0000 (1.0000)  time: 1.2738  data: 0.6943  max mem: 13797
[14:38:12.294840] [Train][Ep-40/100]  [1000/1435]  eta: 0:09:12  lr: 0.0005 (0.0005)  loss: 1179.1329 (1232.7254)  grad_norm: 672.0549 (689.1527)  amp_scale: 1.0000 (1.0000)  time: 1.2194  data: 0.6401  max mem: 13797
[14:40:12.368673] [Train][Ep-40/100]  [1100/1435]  eta: 0:07:03  lr: 0.0005 (0.0005)  loss: 1233.6477 (1235.4956)  grad_norm: 661.6573 (688.3121)  amp_scale: 1.0000 (1.0000)  time: 1.2351  data: 0.6594  max mem: 13797
[14:42:22.597054] [Train][Ep-40/100]  [1200/1435]  eta: 0:04:57  lr: 0.0005 (0.0005)  loss: 1240.5862 (1237.2269)  grad_norm: 689.9284 (689.0497)  amp_scale: 1.0000 (1.0000)  time: 1.3274  data: 0.7468  max mem: 13797
[14:44:26.256320] [Train][Ep-40/100]  [1300/1435]  eta: 0:02:50  lr: 0.0005 (0.0005)  loss: 1207.8126 (1238.1628)  grad_norm: 675.4453 (688.1228)  amp_scale: 1.0000 (1.0000)  time: 1.1768  data: 0.4311  max mem: 13797
[14:46:28.647540] [Train][Ep-40/100]  [1400/1435]  eta: 0:00:44  lr: 0.0005 (0.0005)  loss: 1230.4128 (1238.0088)  grad_norm: 677.7664 (688.4396)  amp_scale: 1.0000 (1.0000)  time: 1.2506  data: 0.4162  max mem: 13797
[14:47:09.902665] [Train][Ep-40/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1221.7253 (1236.6842)  grad_norm: 665.2083 (687.5138)  amp_scale: 1.0000 (1.0000)  time: 1.2333  data: 0.6548  max mem: 13797
[14:47:09.903720] [Train][Ep-40/100] Total time: 0:30:08 (1.2604 s / it)
[14:47:09.904157] Syncing meters...
[14:47:10.100316] Averaged stats: lr: 0.0005 (0.0005)  loss: 1221.7253 (1231.2917)  grad_norm: 665.2083 (687.5138)  amp_scale: 1.0000 (1.0000)
[14:47:12.290561] [Eval][Ep-40/100]  [  0/121]  eta: 0:04:23  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.1800  data: 2.0160  max mem: 13797
[14:49:08.356632] [Eval][Ep-40/100]  [100/121]  eta: 0:00:24  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1781  data: 1.0158  max mem: 13797
[14:49:30.458040] [Eval][Ep-40/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 1.1049  data: 0.9448  max mem: 13797
[14:49:30.459216] [Eval][Ep-40/100] Total time: 0:02:20 (1.1599 s / it)
[14:49:30.682493] [Eval][Ep-40/100] val_acc1_image=27.18 | val_acc1_audio=40.57 | val_acc1_fusion=37.50 | val_acc1_all=51.68
[14:49:40.117772] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 41)
[14:49:42.242479] [Train][Ep-41/100]  [   0/1435]  eta: 0:48:18  lr: 0.0005 (0.0005)  time: 2.0198  data: 1.5319  max mem: 13797
[14:51:46.017816] [Train][Ep-41/100]  [ 100/1435]  eta: 0:27:42  lr: 0.0005 (0.0005)  loss: 1220.8069 (1239.4951)  grad_norm: 668.1113 (676.9170)  amp_scale: 1.0000 (1.0000)  time: 1.3035  data: 0.3379  max mem: 13797
[14:53:51.322690] [Train][Ep-41/100]  [ 200/1435]  eta: 0:25:42  lr: 0.0005 (0.0005)  loss: 1216.4236 (1246.3721)  grad_norm: 679.1883 (678.0976)  amp_scale: 1.0000 (1.0000)  time: 1.1946  data: 0.4677  max mem: 13797
[14:55:56.459816] [Train][Ep-41/100]  [ 300/1435]  eta: 0:23:38  lr: 0.0005 (0.0005)  loss: 1191.0729 (1232.6179)  grad_norm: 678.7150 (681.9177)  amp_scale: 1.0000 (1.0000)  time: 1.2721  data: 0.6626  max mem: 13797
[14:58:03.615048] [Train][Ep-41/100]  [ 400/1435]  eta: 0:21:39  lr: 0.0005 (0.0005)  loss: 1151.4303 (1218.6305)  grad_norm: 668.9537 (681.5537)  amp_scale: 1.0000 (1.0000)  time: 1.2447  data: 0.6081  max mem: 13797
[15:00:11.665223] [Train][Ep-41/100]  [ 500/1435]  eta: 0:19:38  lr: 0.0005 (0.0005)  loss: 1248.6366 (1222.4360)  grad_norm: 697.3782 (681.7597)  amp_scale: 1.0000 (1.0000)  time: 1.3043  data: 0.7271  max mem: 13797
[15:02:22.180443] [Train][Ep-41/100]  [ 600/1435]  eta: 0:17:38  lr: 0.0005 (0.0005)  loss: 1198.1841 (1219.0497)  grad_norm: 674.1552 (682.5108)  amp_scale: 1.0000 (1.0000)  time: 1.2335  data: 0.6527  max mem: 13797
[15:04:24.569975] [Train][Ep-41/100]  [ 700/1435]  eta: 0:15:27  lr: 0.0005 (0.0005)  loss: 1248.3112 (1221.0206)  grad_norm: 681.5651 (683.4057)  amp_scale: 1.0000 (1.0000)  time: 1.2910  data: 0.1414  max mem: 13797
[15:06:30.104280] [Train][Ep-41/100]  [ 800/1435]  eta: 0:13:20  lr: 0.0005 (0.0005)  loss: 1228.0670 (1224.1270)  grad_norm: 677.1769 (685.8312)  amp_scale: 1.0000 (1.0000)  time: 1.3521  data: 0.0002  max mem: 13797
[15:08:39.481382] [Train][Ep-41/100]  [ 900/1435]  eta: 0:11:16  lr: 0.0005 (0.0005)  loss: 1234.1375 (1226.8358)  grad_norm: 673.1640 (684.7581)  amp_scale: 1.0000 (1.0000)  time: 1.2334  data: 0.0670  max mem: 13797
[15:10:39.832368] [Train][Ep-41/100]  [1000/1435]  eta: 0:09:07  lr: 0.0005 (0.0005)  loss: 1182.7252 (1226.6563)  grad_norm: 669.2594 (684.2200)  amp_scale: 1.0000 (1.0000)  time: 1.2657  data: 0.0013  max mem: 13797
[15:12:49.413625] [Train][Ep-41/100]  [1100/1435]  eta: 0:07:02  lr: 0.0005 (0.0005)  loss: 1239.7513 (1229.9496)  grad_norm: 673.9529 (683.5637)  amp_scale: 1.0000 (1.0000)  time: 1.2592  data: 0.0207  max mem: 13797
[15:14:58.770321] [Train][Ep-41/100]  [1200/1435]  eta: 0:04:57  lr: 0.0005 (0.0005)  loss: 1205.9274 (1229.6398)  grad_norm: 679.8827 (684.2048)  amp_scale: 1.0000 (1.0000)  time: 1.2395  data: 0.0485  max mem: 13797
[15:17:01.944900] [Train][Ep-41/100]  [1300/1435]  eta: 0:02:50  lr: 0.0005 (0.0005)  loss: 1181.7100 (1228.1591)  grad_norm: 666.9341 (683.2597)  amp_scale: 1.0000 (1.0000)  time: 1.1698  data: 0.2515  max mem: 13797
[15:19:02.676752] [Train][Ep-41/100]  [1400/1435]  eta: 0:00:44  lr: 0.0005 (0.0005)  loss: 1248.3567 (1227.6535)  grad_norm: 688.5570 (683.4946)  amp_scale: 1.0000 (1.0000)  time: 1.2441  data: 0.6446  max mem: 13797
[15:19:44.186171] [Train][Ep-41/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1239.8538 (1227.3362)  grad_norm: 696.8607 (683.7108)  amp_scale: 1.0000 (1.0000)  time: 1.2297  data: 0.6498  max mem: 13797
[15:19:44.187692] [Train][Ep-41/100] Total time: 0:30:03 (1.2571 s / it)
[15:19:44.188162] Syncing meters...
[15:19:44.190133] Averaged stats: lr: 0.0005 (0.0005)  loss: 1239.8538 (1226.9960)  grad_norm: 696.8607 (683.7108)  amp_scale: 1.0000 (1.0000)
[15:19:54.319158] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 42)
[15:19:56.775083] [Train][Ep-42/100]  [   0/1435]  eta: 0:53:57  lr: 0.0005 (0.0005)  time: 2.2560  data: 1.7601  max mem: 13797
[15:22:02.104962] [Train][Ep-42/100]  [ 100/1435]  eta: 0:28:06  lr: 0.0005 (0.0005)  loss: 1197.0079 (1205.3740)  grad_norm: 644.5462 (661.0226)  amp_scale: 1.0000 (1.0000)  time: 1.2457  data: 0.6706  max mem: 13797
[15:24:02.371751] [Train][Ep-42/100]  [ 200/1435]  eta: 0:25:22  lr: 0.0005 (0.0005)  loss: 1220.7235 (1223.5420)  grad_norm: 674.4510 (667.9084)  amp_scale: 1.0000 (1.0000)  time: 1.1842  data: 0.5173  max mem: 13797
[15:26:04.952053] [Train][Ep-42/100]  [ 300/1435]  eta: 0:23:16  lr: 0.0005 (0.0005)  loss: 1220.3610 (1217.4227)  grad_norm: 671.1443 (669.1436)  amp_scale: 1.0000 (1.0000)  time: 1.1963  data: 0.6072  max mem: 13797
[15:28:05.823038] [Train][Ep-42/100]  [ 400/1435]  eta: 0:21:07  lr: 0.0005 (0.0005)  loss: 1249.7699 (1223.7191)  grad_norm: 681.2173 (672.4104)  amp_scale: 1.0000 (1.0000)  time: 1.2112  data: 0.0349  max mem: 13797
[15:30:11.241373] [Train][Ep-42/100]  [ 500/1435]  eta: 0:19:10  lr: 0.0005 (0.0005)  loss: 1217.7225 (1216.8238)  grad_norm: 660.1187 (671.5241)  amp_scale: 1.0000 (1.0000)  time: 1.1981  data: 0.2714  max mem: 13797
[15:32:17.885905] [Train][Ep-42/100]  [ 600/1435]  eta: 0:17:12  lr: 0.0005 (0.0005)  loss: 1205.3378 (1218.7197)  grad_norm: 666.0371 (671.8613)  amp_scale: 1.0000 (1.0000)  time: 1.2703  data: 0.0714  max mem: 13797
[15:34:26.825508] [Train][Ep-42/100]  [ 700/1435]  eta: 0:15:14  lr: 0.0005 (0.0005)  loss: 1226.8256 (1217.9938)  grad_norm: 693.5991 (674.5387)  amp_scale: 1.0000 (1.0000)  time: 1.3302  data: 0.1640  max mem: 13797
[15:36:31.188883] [Train][Ep-42/100]  [ 800/1435]  eta: 0:13:10  lr: 0.0005 (0.0005)  loss: 1206.8861 (1218.1890)  grad_norm: 687.5246 (676.8863)  amp_scale: 1.0000 (1.0000)  time: 1.2679  data: 0.2924  max mem: 13797
[15:38:33.913784] [Train][Ep-42/100]  [ 900/1435]  eta: 0:11:04  lr: 0.0005 (0.0005)  loss: 1205.1727 (1219.4096)  grad_norm: 678.9213 (677.0192)  amp_scale: 1.0000 (1.0000)  time: 1.2587  data: 0.2974  max mem: 13797
[15:40:40.502588] [Train][Ep-42/100]  [1000/1435]  eta: 0:09:01  lr: 0.0005 (0.0005)  loss: 1216.0947 (1220.0997)  grad_norm: 677.0287 (677.4776)  amp_scale: 1.0000 (1.0000)  time: 1.2788  data: 0.0005  max mem: 13797
[15:42:45.549307] [Train][Ep-42/100]  [1100/1435]  eta: 0:06:57  lr: 0.0005 (0.0005)  loss: 1237.2468 (1221.0187)  grad_norm: 696.5491 (678.6150)  amp_scale: 1.0000 (1.0000)  time: 1.2140  data: 0.0009  max mem: 13797
[15:44:58.180446] [Train][Ep-42/100]  [1200/1435]  eta: 0:04:54  lr: 0.0005 (0.0005)  loss: 1259.5071 (1224.2844)  grad_norm: 685.9243 (679.9742)  amp_scale: 1.0000 (1.0000)  time: 1.2790  data: 0.0016  max mem: 13797
[15:47:04.903047] [Train][Ep-42/100]  [1300/1435]  eta: 0:02:49  lr: 0.0005 (0.0005)  loss: 1182.1237 (1222.9132)  grad_norm: 690.1981 (680.7641)  amp_scale: 1.0000 (1.0000)  time: 1.2766  data: 0.0006  max mem: 13797
[15:49:10.577313] [Train][Ep-42/100]  [1400/1435]  eta: 0:00:43  lr: 0.0005 (0.0005)  loss: 1226.5790 (1225.0127)  grad_norm: 703.5128 (681.8832)  amp_scale: 1.0000 (1.0000)  time: 1.2967  data: 0.0003  max mem: 13797
[15:49:52.573234] [Train][Ep-42/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1213.8726 (1224.2698)  grad_norm: 703.5128 (681.7376)  amp_scale: 1.0000 (1.0000)  time: 1.2039  data: 0.0003  max mem: 13797
[15:49:52.574438] [Train][Ep-42/100] Total time: 0:29:58 (1.2530 s / it)
[15:49:52.574978] Syncing meters...
[15:49:53.263247] Averaged stats: lr: 0.0005 (0.0005)  loss: 1213.8726 (1222.4334)  grad_norm: 703.5128 (681.7376)  amp_scale: 1.0000 (1.0000)
[15:50:03.275960] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 43)
[15:50:05.630653] [Train][Ep-43/100]  [   0/1435]  eta: 0:51:23  lr: 0.0005 (0.0005)  time: 2.1487  data: 1.6550  max mem: 13797
[15:52:03.950935] [Train][Ep-43/100]  [ 100/1435]  eta: 0:26:32  lr: 0.0005 (0.0005)  loss: 1225.3438 (1234.8930)  grad_norm: 692.2715 (689.0859)  amp_scale: 1.0000 (1.0000)  time: 1.2011  data: 0.0009  max mem: 13797
[15:54:12.051398] [Train][Ep-43/100]  [ 200/1435]  eta: 0:25:27  lr: 0.0005 (0.0005)  loss: 1190.5295 (1221.2019)  grad_norm: 680.8375 (684.2134)  amp_scale: 1.0000 (1.0000)  time: 1.2689  data: 0.0006  max mem: 13797
[15:56:21.108553] [Train][Ep-43/100]  [ 300/1435]  eta: 0:23:43  lr: 0.0005 (0.0005)  loss: 1189.8372 (1215.3338)  grad_norm: 668.7689 (681.3589)  amp_scale: 1.0000 (1.0000)  time: 1.2588  data: 0.0518  max mem: 13797
[15:58:19.890893] [Train][Ep-43/100]  [ 400/1435]  eta: 0:21:21  lr: 0.0005 (0.0005)  loss: 1190.1234 (1215.0952)  grad_norm: 683.1429 (682.8098)  amp_scale: 1.0000 (1.0000)  time: 1.1827  data: 0.0005  max mem: 13797
[16:00:29.312124] [Train][Ep-43/100]  [ 500/1435]  eta: 0:19:27  lr: 0.0005 (0.0005)  loss: 1223.0963 (1222.0831)  grad_norm: 686.8056 (683.8965)  amp_scale: 1.0000 (1.0000)  time: 1.2149  data: 0.0348  max mem: 13797
[16:02:26.953127] [Train][Ep-43/100]  [ 600/1435]  eta: 0:17:12  lr: 0.0005 (0.0005)  loss: 1186.8977 (1217.3454)  grad_norm: 675.6180 (683.0702)  amp_scale: 1.0000 (1.0000)  time: 1.2816  data: 0.0950  max mem: 13797
[16:04:36.088746] [Train][Ep-43/100]  [ 700/1435]  eta: 0:15:14  lr: 0.0005 (0.0005)  loss: 1189.4734 (1214.6036)  grad_norm: 665.0947 (681.4859)  amp_scale: 1.0000 (1.0000)  time: 1.2496  data: 0.0365  max mem: 13797
[16:06:44.279829] [Train][Ep-43/100]  [ 800/1435]  eta: 0:13:13  lr: 0.0005 (0.0005)  loss: 1252.9116 (1218.1436)  grad_norm: 685.6200 (681.7794)  amp_scale: 1.0000 (1.0000)  time: 1.3291  data: 0.0699  max mem: 13797
[16:08:54.313806] [Train][Ep-43/100]  [ 900/1435]  eta: 0:11:11  lr: 0.0005 (0.0005)  loss: 1180.0039 (1217.8202)  grad_norm: 671.3613 (681.7141)  amp_scale: 1.0000 (1.0000)  time: 1.2903  data: 0.0093  max mem: 13797
[16:11:04.927616] [Train][Ep-43/100]  [1000/1435]  eta: 0:09:08  lr: 0.0005 (0.0005)  loss: 1214.1270 (1216.9298)  grad_norm: 674.9538 (681.9967)  amp_scale: 1.0000 (1.0000)  time: 1.2757  data: 0.0002  max mem: 13797
[16:13:13.525309] [Train][Ep-43/100]  [1100/1435]  eta: 0:07:02  lr: 0.0005 (0.0005)  loss: 1219.9839 (1217.1394)  grad_norm: 693.5788 (684.1973)  amp_scale: 1.0000 (1.0000)  time: 1.2308  data: 0.1654  max mem: 13797
[16:15:19.747289] [Train][Ep-43/100]  [1200/1435]  eta: 0:04:56  lr: 0.0005 (0.0005)  loss: 1207.5887 (1217.4567)  grad_norm: 684.6622 (683.6916)  amp_scale: 1.0000 (1.0000)  time: 1.3170  data: 0.0004  max mem: 13797
[16:17:28.211495] [Train][Ep-43/100]  [1300/1435]  eta: 0:02:50  lr: 0.0005 (0.0005)  loss: 1222.7238 (1217.7505)  grad_norm: 693.0497 (685.1697)  amp_scale: 1.0000 (1.0000)  time: 1.2349  data: 0.0429  max mem: 13797
[16:19:33.586432] [Train][Ep-43/100]  [1400/1435]  eta: 0:00:44  lr: 0.0005 (0.0005)  loss: 1213.6403 (1218.3484)  grad_norm: 681.5819 (685.2853)  amp_scale: 1.0000 (1.0000)  time: 1.2577  data: 0.0007  max mem: 13797
[16:20:15.197898] [Train][Ep-43/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1213.6403 (1217.4495)  grad_norm: 681.8210 (685.4729)  amp_scale: 1.0000 (1.0000)  time: 1.2658  data: 0.0014  max mem: 13797
[16:20:15.198962] [Train][Ep-43/100] Total time: 0:30:11 (1.2625 s / it)
[16:20:15.199418] Syncing meters...
[16:20:16.047682] Averaged stats: lr: 0.0005 (0.0005)  loss: 1213.6403 (1217.0303)  grad_norm: 681.8210 (685.4729)  amp_scale: 1.0000 (1.0000)
[16:20:26.044837] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 44)
[16:20:28.552174] [Train][Ep-44/100]  [   0/1435]  eta: 0:55:07  lr: 0.0005 (0.0005)  time: 2.3047  data: 1.8095  max mem: 13797
[16:22:28.901055] [Train][Ep-44/100]  [ 100/1435]  eta: 0:27:01  lr: 0.0005 (0.0005)  loss: 1186.1871 (1199.2049)  grad_norm: 670.1519 (675.7301)  amp_scale: 1.0000 (1.0000)  time: 1.1825  data: 0.5951  max mem: 13797
[16:24:29.476596] [Train][Ep-44/100]  [ 200/1435]  eta: 0:24:54  lr: 0.0005 (0.0005)  loss: 1182.2900 (1178.6131)  grad_norm: 661.2675 (674.2180)  amp_scale: 1.0000 (1.0000)  time: 1.2130  data: 0.6293  max mem: 13797
[16:26:25.574321] [Train][Ep-44/100]  [ 300/1435]  eta: 0:22:34  lr: 0.0005 (0.0005)  loss: 1227.1919 (1187.2725)  grad_norm: 700.5564 (682.3491)  amp_scale: 1.0000 (1.0000)  time: 1.2297  data: 0.2764  max mem: 13797
[16:28:28.981657] [Train][Ep-44/100]  [ 400/1435]  eta: 0:20:45  lr: 0.0005 (0.0005)  loss: 1205.1058 (1195.1857)  grad_norm: 690.3250 (685.5183)  amp_scale: 1.0000 (1.0000)  time: 1.3269  data: 0.0340  max mem: 13797
[16:30:29.586163] [Train][Ep-44/100]  [ 500/1435]  eta: 0:18:45  lr: 0.0005 (0.0005)  loss: 1197.1378 (1198.7763)  grad_norm: 674.8007 (683.4775)  amp_scale: 1.0000 (1.0000)  time: 1.1805  data: 0.3712  max mem: 13797
[16:32:27.126916] [Train][Ep-44/100]  [ 600/1435]  eta: 0:16:41  lr: 0.0005 (0.0005)  loss: 1264.9589 (1208.2150)  grad_norm: 698.5369 (684.6195)  amp_scale: 1.0000 (1.0000)  time: 1.1437  data: 0.1510  max mem: 13797
[16:34:27.296864] [Train][Ep-44/100]  [ 700/1435]  eta: 0:14:41  lr: 0.0005 (0.0005)  loss: 1160.0448 (1202.6546)  grad_norm: 683.8835 (684.7880)  amp_scale: 1.0000 (1.0000)  time: 1.2859  data: 0.0547  max mem: 13797
[16:36:24.774428] [Train][Ep-44/100]  [ 800/1435]  eta: 0:12:39  lr: 0.0005 (0.0005)  loss: 1167.2842 (1201.2981)  grad_norm: 674.8937 (683.9068)  amp_scale: 1.0000 (1.0000)  time: 1.1783  data: 0.3611  max mem: 13797
[16:38:26.664943] [Train][Ep-44/100]  [ 900/1435]  eta: 0:10:41  lr: 0.0005 (0.0005)  loss: 1208.8022 (1203.1425)  grad_norm: 676.1817 (684.0459)  amp_scale: 1.0000 (1.0000)  time: 1.1833  data: 0.3769  max mem: 13797
[16:40:34.014411] [Train][Ep-44/100]  [1000/1435]  eta: 0:08:44  lr: 0.0005 (0.0005)  loss: 1231.2906 (1207.2900)  grad_norm: 656.0170 (683.4338)  amp_scale: 1.0000 (1.0000)  time: 1.2658  data: 0.0226  max mem: 13797
[16:42:43.310425] [Train][Ep-44/100]  [1100/1435]  eta: 0:06:46  lr: 0.0005 (0.0005)  loss: 1176.4919 (1206.3795)  grad_norm: 673.3933 (682.5688)  amp_scale: 1.0000 (1.0000)  time: 1.3118  data: 0.5147  max mem: 13797
[16:44:48.187859] [Train][Ep-44/100]  [1200/1435]  eta: 0:04:46  lr: 0.0005 (0.0005)  loss: 1237.6577 (1209.1150)  grad_norm: 696.2281 (684.4318)  amp_scale: 1.0000 (1.0000)  time: 1.2764  data: 0.6957  max mem: 13797
[16:46:56.190629] [Train][Ep-44/100]  [1300/1435]  eta: 0:02:44  lr: 0.0005 (0.0005)  loss: 1179.2877 (1207.9334)  grad_norm: 673.6418 (684.4935)  amp_scale: 1.0000 (1.0000)  time: 1.2805  data: 0.7041  max mem: 13797
[16:49:06.413940] [Train][Ep-44/100]  [1400/1435]  eta: 0:00:42  lr: 0.0005 (0.0005)  loss: 1193.9161 (1208.4669)  grad_norm: 672.3787 (683.8192)  amp_scale: 1.0000 (1.0000)  time: 1.3352  data: 0.7558  max mem: 13797
[16:49:49.096016] [Train][Ep-44/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1218.2244 (1208.7708)  grad_norm: 672.6968 (683.9246)  amp_scale: 1.0000 (1.0000)  time: 1.2671  data: 0.6896  max mem: 13797
[16:49:49.097340] [Train][Ep-44/100] Total time: 0:29:22 (1.2285 s / it)
[16:49:49.098036] Syncing meters...
[16:49:49.100216] Averaged stats: lr: 0.0005 (0.0005)  loss: 1218.2244 (1207.6404)  grad_norm: 672.6968 (683.9246)  amp_scale: 1.0000 (1.0000)
[16:49:59.868955] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 45)
[16:50:01.973587] [Train][Ep-45/100]  [   0/1435]  eta: 0:45:28  lr: 0.0005 (0.0005)  time: 1.9014  data: 1.4071  max mem: 13797
[16:52:08.902014] [Train][Ep-45/100]  [ 100/1435]  eta: 0:28:22  lr: 0.0005 (0.0005)  loss: 1191.3611 (1203.1143)  grad_norm: 653.5471 (667.6597)  amp_scale: 1.0000 (1.0000)  time: 1.2619  data: 0.6575  max mem: 13797
[16:54:15.058802] [Train][Ep-45/100]  [ 200/1435]  eta: 0:26:06  lr: 0.0005 (0.0005)  loss: 1169.6772 (1193.1120)  grad_norm: 696.8651 (677.7847)  amp_scale: 1.0000 (1.0000)  time: 1.2670  data: 0.6835  max mem: 13797
[16:56:18.009342] [Train][Ep-45/100]  [ 300/1435]  eta: 0:23:45  lr: 0.0005 (0.0005)  loss: 1168.2729 (1192.6801)  grad_norm: 676.2789 (680.0190)  amp_scale: 1.0000 (1.0000)  time: 1.2127  data: 0.6236  max mem: 13797
[16:58:15.509389] [Train][Ep-45/100]  [ 400/1435]  eta: 0:21:18  lr: 0.0005 (0.0005)  loss: 1169.0818 (1188.9509)  grad_norm: 685.6451 (680.6097)  amp_scale: 1.0000 (1.0000)  time: 1.1219  data: 0.4772  max mem: 13797
[17:00:20.258559] [Train][Ep-45/100]  [ 500/1435]  eta: 0:19:17  lr: 0.0005 (0.0005)  loss: 1178.6500 (1189.9370)  grad_norm: 686.1949 (682.9882)  amp_scale: 1.0000 (1.0000)  time: 1.2307  data: 0.0795  max mem: 13797
[17:02:22.579841] [Train][Ep-45/100]  [ 600/1435]  eta: 0:17:11  lr: 0.0005 (0.0005)  loss: 1138.1857 (1184.7928)  grad_norm: 679.8713 (683.7055)  amp_scale: 1.0000 (1.0000)  time: 1.3181  data: 0.7379  max mem: 13797
[17:04:25.050653] [Train][Ep-45/100]  [ 700/1435]  eta: 0:15:06  lr: 0.0005 (0.0005)  loss: 1243.9180 (1189.5648)  grad_norm: 676.6804 (683.2433)  amp_scale: 1.0000 (1.0000)  time: 1.1763  data: 0.5752  max mem: 13797
[17:06:28.093492] [Train][Ep-45/100]  [ 800/1435]  eta: 0:13:03  lr: 0.0005 (0.0005)  loss: 1223.8782 (1193.5740)  grad_norm: 686.1407 (682.8291)  amp_scale: 1.0000 (1.0000)  time: 1.2879  data: 0.7030  max mem: 13797
[17:08:35.835990] [Train][Ep-45/100]  [ 900/1435]  eta: 0:11:02  lr: 0.0005 (0.0005)  loss: 1226.4208 (1197.7100)  grad_norm: 688.7681 (683.0095)  amp_scale: 1.0000 (1.0000)  time: 1.3348  data: 0.7523  max mem: 13797
[17:10:46.276033] [Train][Ep-45/100]  [1000/1435]  eta: 0:09:01  lr: 0.0005 (0.0005)  loss: 1169.8636 (1196.2582)  grad_norm: 676.0731 (683.2124)  amp_scale: 1.0000 (1.0000)  time: 1.2507  data: 0.6743  max mem: 13797
[17:13:01.953977] job dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound
[17:13:01.955183] env.world_size: 4
[17:13:01.955545] env.rank: 0
[17:13:01.955871] env.dist_url: tcp://localhost:50001
[17:13:01.956188] env.dist_backend: nccl
[17:13:01.956494] env.port: 50001
[17:13:01.956802] env.node: localhost
[17:13:01.957118] env.distributed: True
[17:13:01.957437] env.seed: None
[17:13:01.957690] env.gpu: None
[17:13:01.957922] env.ngpu: 4
[17:13:01.958160] env.mem_gb: 240
[17:13:01.958435] env.workers: 8
[17:13:01.958696] env.slurm: True
[17:13:01.958945] env.slurm_suffix: 
[17:13:01.959183] env.slurm_partition: NORMAL
[17:13:01.959430] env.slurm_comment: DeepAVFusion
[17:13:01.959668] env.slurm_timeout: 1440
[17:13:01.959902] env.nodelist: node14
[17:13:01.960127] env.exclude: 
[17:13:01.960421] log.print_freq: 100
[17:13:01.960680] log.save_freq: 50
[17:13:01.960937] log.eval_freq: 10
[17:13:01.961170] log.wandb_watch_freq: 0
[17:13:01.961405] log.debug: False
[17:13:01.961658] log.use_wandb: True
[17:13:01.962174] log.wandb_entity: audiovisual_diagnostics
[17:13:01.962479] log.wandb_project: efav
[17:13:01.962754] worker: eval_finetune
[17:13:01.962987] output_dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200
[17:13:01.963219] job_name: finetune_vggsound
[17:13:01.963489] pretrain_job_name: deepavfusion_vitb_vggsound_ep200
[17:13:01.963752] checkpoint: None
[17:13:01.963981] encoder_prefix: encoder.
[17:13:01.964227] pretrain_resume_epoch: latest
[17:13:01.964502] eval: False
[17:13:01.964748] debug: False
[17:13:01.965082] model.image.backbone: vit_base
[17:13:01.965342] model.image.pretrained: vit_base_mae_in1k
[17:13:01.965666] model.audio.backbone: vit_base
[17:13:01.965925] model.audio.pretrained: vit_base_audiomae_as2m
[17:13:01.966258] model.fusion.arch: factorized_mmi
[17:13:01.966655] model.fusion.layers: all
[17:13:01.966964] model.fusion.num_fusion_tkns: 16
[17:13:01.967218] model.fusion.num_aggr_image_tkns: 8
[17:13:01.967501] model.fusion.num_aggr_audio_tkns: 8
[17:13:01.967792] model.fusion.mlp_ratio: 1.0
[17:13:01.968081] model.fusion.attn_ratio: 0.25
[17:13:01.968365] model.fusion.num_heads: 12
[17:13:01.968773] opt.resume: True
[17:13:01.969253] opt.joint_loss: True
[17:13:01.969601] opt.batch_size: 32
[17:13:01.969867] opt.epochs: 100
[17:13:01.970117] opt.warmup_epochs: 20
[17:13:01.970359] opt.accum_iter: 4
[17:13:01.970621] opt.clip_grad: None
[17:13:01.970861] opt.weight_decay: 0.05
[17:13:01.971105] opt.layer_decay: 0.75
[17:13:01.971369] opt.smoothing: 0.1
[17:13:01.971609] opt.lr: None
[17:13:01.971865] opt.blr: 0.0003
[17:13:01.972105] opt.min_lr: 0.0
[17:13:01.972357] opt.drop_path: 0.2
[17:13:01.972598] opt.attn_drop: 0.0
[17:13:01.972859] opt.proj_drop: 0.0
[17:13:01.973209] opt.use_amp: False
[17:13:01.973662] data.dataset: vggsound
[17:13:01.974015] data.data_path: /tmp/zverev/vggsound
[17:13:01.974263] data.audio_rate: 16000
[17:13:01.974510] data.audio_dur: 3.0
[17:13:01.974770] data.audio_mels: 128
[17:13:01.975032] data.image_size: 224
[17:13:01.975285] data.crop_min: 0.5
[17:13:01.975531] data.mixup: 1.0
[17:13:01.975794] data.cutmix: 0.0
[17:13:01.976045] data.cutmix_minmax: None
[17:13:01.976345] data.mixup_prob: 1.0
[17:13:01.976632] data.mixup_switch_prob: 0.5
[17:13:01.976880] data.mixup_mode: batch
[17:13:01.977482] base lr: 3.00e-04
[17:13:01.977763] actual lr: 6.00e-04
[17:13:01.978023] accumulate grad iterations: 4
[17:13:01.978252] effective batch size: 512
[17:13:16.338975] Video CN7qu_1k3F8 not found
[17:13:29.487307] Video RflAC1ror3c not found
[17:13:55.083811] Video wvHLlBCYe8c not found
[17:13:59.208520] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 183727
 - Num classes: 310
[17:14:04.582718] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:09.898470] DenseVideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:09.899394] Mixup is activated!
[17:14:10.854010] Loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth
[17:14:10.854811] Error loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth: [Errno 2] No such file or directory: 'https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth' 
 Loading from torch hub ...
[17:14:12.624451] Loading checkpoint from assets/models/vitbase_audiomae_as2m.pth
[17:14:16.728893] Model = AVClassifier(
  (encoder): DeepAVFusion(
    (image): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (audio): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (fusion_blocks): ModuleList(
      (0-11): 12 x FusionBlock_FactorizedAVInteractions(
        (norm1_mm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_aud): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_img): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): CrossAttention_FactorizedAVInteractions(
          (attn_v): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (attn_a): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (q): Linear(in_features=768, out_features=192, bias=True)
          (k): Linear(in_features=1536, out_features=192, bias=True)
          (v): Linear(in_features=1536, out_features=768, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=768, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (fusion_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (image_head): Linear(in_features=768, out_features=310, bias=True)
  (audio_head): Linear(in_features=768, out_features=310, bias=True)
  (fusion_head): Linear(in_features=768, out_features=310, bias=True)
)
[17:14:20.295979] Loaded pre-trained checkpoint: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/checkpoints/checkpoint_latest.pth
[17:14:20.333143] criterion = SoftTargetCrossEntropy()
[17:14:20.372762] Loading /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth
[17:14:41.971050] => loaded checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 45)
[17:14:44.169545] Start training for 100 epochs
[17:14:52.216964] [Train][Ep-45/100]  [   0/1435]  eta: 3:12:21  lr: 0.0005 (0.0005)  time: 8.0429  data: 7.1625  max mem: 12727
[17:17:10.652843] [Train][Ep-45/100]  [ 100/1435]  eta: 0:32:16  lr: 0.0005 (0.0005)  loss: 1193.6354 (1187.2840)  grad_norm: 658.1409 (669.4034)  amp_scale: 1.0000 (1.0000)  time: 1.3046  data: 0.7404  max mem: 13835
[17:19:24.533929] [Train][Ep-45/100]  [ 200/1435]  eta: 0:28:42  lr: 0.0005 (0.0005)  loss: 1204.7122 (1198.7998)  grad_norm: 684.5579 (673.5301)  amp_scale: 1.0000 (1.0000)  time: 1.3741  data: 0.8090  max mem: 13835
[17:21:31.968737] [Train][Ep-45/100]  [ 300/1435]  eta: 0:25:37  lr: 0.0005 (0.0005)  loss: 1214.2588 (1203.6741)  grad_norm: 661.3173 (674.1818)  amp_scale: 1.0000 (1.0000)  time: 1.2970  data: 0.6650  max mem: 13835
[17:23:41.226384] [Train][Ep-45/100]  [ 400/1435]  eta: 0:23:06  lr: 0.0005 (0.0005)  loss: 1179.2662 (1195.1735)  grad_norm: 671.1118 (677.6930)  amp_scale: 1.0000 (1.0000)  time: 1.2792  data: 0.6670  max mem: 13835
[17:25:53.123821] [Train][Ep-45/100]  [ 500/1435]  eta: 0:20:48  lr: 0.0005 (0.0005)  loss: 1206.0934 (1194.9324)  grad_norm: 679.9670 (678.9826)  amp_scale: 1.0000 (1.0000)  time: 1.2589  data: 0.6650  max mem: 13835
[17:27:58.000845] [Train][Ep-45/100]  [ 600/1435]  eta: 0:18:22  lr: 0.0005 (0.0005)  loss: 1177.4435 (1193.1645)  grad_norm: 695.0467 (682.1518)  amp_scale: 1.0000 (1.0000)  time: 1.3016  data: 0.7106  max mem: 13835
[17:30:07.482838] [Train][Ep-45/100]  [ 700/1435]  eta: 0:16:08  lr: 0.0005 (0.0005)  loss: 1179.2419 (1195.1976)  grad_norm: 661.5372 (680.1956)  amp_scale: 1.0000 (1.0000)  time: 1.3344  data: 0.2968  max mem: 13835
[17:32:13.533878] [Train][Ep-45/100]  [ 800/1435]  eta: 0:13:51  lr: 0.0005 (0.0005)  loss: 1213.9032 (1198.8449)  grad_norm: 678.9845 (681.1276)  amp_scale: 1.0000 (1.0000)  time: 1.3229  data: 0.5469  max mem: 13835
[17:34:28.645819] [Train][Ep-45/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0005 (0.0005)  loss: 1226.3505 (1201.1887)  grad_norm: 683.4824 (681.5688)  amp_scale: 1.0000 (1.0000)  time: 1.4579  data: 0.1340  max mem: 13835
[17:36:37.144732] [Train][Ep-45/100]  [1000/1435]  eta: 0:09:30  lr: 0.0005 (0.0005)  loss: 1154.4946 (1200.6666)  grad_norm: 693.7083 (683.0487)  amp_scale: 1.0000 (1.0000)  time: 1.2652  data: 0.2507  max mem: 13835
[17:38:45.851980] [Train][Ep-45/100]  [1100/1435]  eta: 0:07:18  lr: 0.0005 (0.0005)  loss: 1208.2081 (1201.8795)  grad_norm: 692.9937 (683.7569)  amp_scale: 1.0000 (1.0000)  time: 1.3270  data: 0.1577  max mem: 13835
[17:40:53.767450] [Train][Ep-45/100]  [1200/1435]  eta: 0:05:07  lr: 0.0005 (0.0005)  loss: 1164.4685 (1200.1707)  grad_norm: 677.6287 (684.5088)  amp_scale: 1.0000 (1.0000)  time: 1.2120  data: 0.2945  max mem: 13835
[17:43:08.752279] [Train][Ep-45/100]  [1300/1435]  eta: 0:02:56  lr: 0.0005 (0.0005)  loss: 1197.6144 (1202.2862)  grad_norm: 687.8901 (684.7002)  amp_scale: 1.0000 (1.0000)  time: 1.3394  data: 0.0002  max mem: 13835
[17:45:12.723137] [Train][Ep-45/100]  [1400/1435]  eta: 0:00:45  lr: 0.0005 (0.0005)  loss: 1174.5548 (1199.5309)  grad_norm: 698.9792 (685.8236)  amp_scale: 1.0000 (1.0000)  time: 1.2542  data: 0.4884  max mem: 13835
[17:45:54.431709] [Train][Ep-45/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1174.5548 (1199.7695)  grad_norm: 698.9792 (686.1711)  amp_scale: 1.0000 (1.0000)  time: 1.2468  data: 0.2260  max mem: 13835
[17:45:54.432759] [Train][Ep-45/100] Total time: 0:31:10 (1.3033 s / it)
[17:45:54.433218] Syncing meters...
[17:45:55.142134] Averaged stats: lr: 0.0005 (0.0005)  loss: 1174.5548 (1201.6689)  grad_norm: 698.9792 (686.1711)  amp_scale: 1.0000 (1.0000)
[17:46:00.870151] [Eval][Ep-45/100]  [  0/121]  eta: 0:11:31  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 5.7189  data: 5.5571  max mem: 13835
[17:47:45.504725] [Eval][Ep-45/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9350  data: 0.7748  max mem: 13835
[17:48:03.068782] [Eval][Ep-45/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8780  data: 0.7203  max mem: 13835
[17:48:03.069800] [Eval][Ep-45/100] Total time: 0:02:07 (1.0572 s / it)
[17:48:03.645430] [Eval][Ep-45/100] val_acc1_image=26.36 | val_acc1_audio=41.12 | val_acc1_fusion=37.69 | val_acc1_all=51.77
[17:48:12.174247] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 46)
[17:48:14.119110] [Train][Ep-46/100]  [   0/1435]  eta: 0:46:17  lr: 0.0005 (0.0005)  time: 1.9358  data: 1.4542  max mem: 13835
[17:50:15.629140] [Train][Ep-46/100]  [ 100/1435]  eta: 0:27:11  lr: 0.0005 (0.0005)  loss: 1228.4828 (1225.5088)  grad_norm: 686.0037 (681.8229)  amp_scale: 1.0000 (1.0000)  time: 1.1847  data: 0.4554  max mem: 13835
[17:52:19.116142] [Train][Ep-46/100]  [ 200/1435]  eta: 0:25:17  lr: 0.0005 (0.0005)  loss: 1167.1895 (1210.5546)  grad_norm: 690.2693 (688.5104)  amp_scale: 1.0000 (1.0000)  time: 1.1980  data: 0.2381  max mem: 13835
[17:54:29.422712] [Train][Ep-46/100]  [ 300/1435]  eta: 0:23:42  lr: 0.0005 (0.0005)  loss: 1175.6465 (1202.5605)  grad_norm: 697.3036 (689.4743)  amp_scale: 1.0000 (1.0000)  time: 1.2467  data: 0.0713  max mem: 13835
[17:56:36.063688] [Train][Ep-46/100]  [ 400/1435]  eta: 0:21:40  lr: 0.0005 (0.0005)  loss: 1169.6073 (1195.6215)  grad_norm: 673.1312 (685.6529)  amp_scale: 1.0000 (1.0000)  time: 1.2954  data: 0.0006  max mem: 13835
[17:58:40.847439] [Train][Ep-46/100]  [ 500/1435]  eta: 0:19:33  lr: 0.0005 (0.0005)  loss: 1199.3201 (1200.6374)  grad_norm: 667.7916 (684.8019)  amp_scale: 1.0000 (1.0000)  time: 1.2884  data: 0.2711  max mem: 13835
[18:00:48.979819] [Train][Ep-46/100]  [ 600/1435]  eta: 0:17:31  lr: 0.0005 (0.0005)  loss: 1160.8180 (1195.8982)  grad_norm: 680.9992 (685.2981)  amp_scale: 1.0000 (1.0000)  time: 1.2665  data: 0.6089  max mem: 13835
[18:02:55.726815] [Train][Ep-46/100]  [ 700/1435]  eta: 0:15:26  lr: 0.0005 (0.0005)  loss: 1186.8472 (1194.1244)  grad_norm: 684.0165 (685.2403)  amp_scale: 1.0000 (1.0000)  time: 1.2960  data: 0.6213  max mem: 13835
[18:05:10.648259] [Train][Ep-46/100]  [ 800/1435]  eta: 0:13:27  lr: 0.0005 (0.0005)  loss: 1177.4691 (1192.5703)  grad_norm: 670.5023 (684.6091)  amp_scale: 1.0000 (1.0000)  time: 1.3067  data: 0.1062  max mem: 13835
[18:07:25.739676] [Train][Ep-46/100]  [ 900/1435]  eta: 0:11:24  lr: 0.0005 (0.0005)  loss: 1164.7924 (1190.7980)  grad_norm: 662.6602 (682.3754)  amp_scale: 1.0000 (1.0000)  time: 1.2470  data: 0.1592  max mem: 13835
[18:09:41.596507] [Train][Ep-46/100]  [1000/1435]  eta: 0:09:20  lr: 0.0004 (0.0005)  loss: 1194.7854 (1195.7267)  grad_norm: 679.1208 (682.4532)  amp_scale: 1.0000 (1.0000)  time: 1.3434  data: 0.0126  max mem: 13835
[18:11:56.300592] [Train][Ep-46/100]  [1100/1435]  eta: 0:07:13  lr: 0.0004 (0.0005)  loss: 1193.1550 (1197.2954)  grad_norm: 665.2235 (681.6718)  amp_scale: 1.0000 (1.0000)  time: 1.3689  data: 0.0464  max mem: 13835
[18:14:08.155064] [Train][Ep-46/100]  [1200/1435]  eta: 0:05:04  lr: 0.0004 (0.0005)  loss: 1215.6127 (1198.7055)  grad_norm: 679.8477 (682.1389)  amp_scale: 1.0000 (1.0000)  time: 1.3024  data: 0.1671  max mem: 13835
[18:16:14.763681] [Train][Ep-46/100]  [1300/1435]  eta: 0:02:54  lr: 0.0004 (0.0005)  loss: 1210.4060 (1199.1519)  grad_norm: 695.9003 (683.1966)  amp_scale: 1.0000 (1.0000)  time: 1.1952  data: 0.5978  max mem: 13835
[18:18:18.246862] [Train][Ep-46/100]  [1400/1435]  eta: 0:00:45  lr: 0.0004 (0.0005)  loss: 1171.9106 (1198.5852)  grad_norm: 683.6665 (683.5477)  amp_scale: 1.0000 (1.0000)  time: 1.2088  data: 0.6204  max mem: 13835
[18:18:57.413622] [Train][Ep-46/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0005)  loss: 1182.8973 (1198.3674)  grad_norm: 695.7463 (684.0891)  amp_scale: 1.0000 (1.0000)  time: 1.1778  data: 0.3615  max mem: 13835
[18:18:57.414341] [Train][Ep-46/100] Total time: 0:30:45 (1.2859 s / it)
[18:18:57.414743] Syncing meters...
[18:18:58.418314] Averaged stats: lr: 0.0004 (0.0005)  loss: 1182.8973 (1195.1189)  grad_norm: 695.7463 (684.0891)  amp_scale: 1.0000 (1.0000)
[18:19:07.406023] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 47)
[18:19:09.535665] [Train][Ep-47/100]  [   0/1435]  eta: 0:50:42  lr: 0.0004 (0.0004)  time: 2.1203  data: 1.6362  max mem: 13835
[18:21:06.218833] [Train][Ep-47/100]  [ 100/1435]  eta: 0:26:10  lr: 0.0004 (0.0004)  loss: 1168.0980 (1197.6655)  grad_norm: 677.0809 (683.1252)  amp_scale: 1.0000 (1.0000)  time: 1.1182  data: 0.4984  max mem: 13835
[18:23:09.719262] [Train][Ep-47/100]  [ 200/1435]  eta: 0:24:48  lr: 0.0004 (0.0004)  loss: 1135.7834 (1176.1638)  grad_norm: 649.3897 (675.2427)  amp_scale: 1.0000 (1.0000)  time: 1.1420  data: 0.1594  max mem: 13835
[18:25:18.677994] [Train][Ep-47/100]  [ 300/1435]  eta: 0:23:19  lr: 0.0004 (0.0004)  loss: 1163.0836 (1172.5687)  grad_norm: 675.0401 (674.5762)  amp_scale: 1.0000 (1.0000)  time: 1.2294  data: 0.0535  max mem: 13835
[18:27:15.760679] [Train][Ep-47/100]  [ 400/1435]  eta: 0:21:00  lr: 0.0004 (0.0004)  loss: 1122.6229 (1168.6086)  grad_norm: 684.1578 (677.1367)  amp_scale: 1.0000 (1.0000)  time: 1.1008  data: 0.4303  max mem: 13835
[18:29:18.417818] [Train][Ep-47/100]  [ 500/1435]  eta: 0:19:00  lr: 0.0004 (0.0004)  loss: 1182.4274 (1171.6448)  grad_norm: 684.4832 (680.9246)  amp_scale: 1.0000 (1.0000)  time: 1.2887  data: 0.0004  max mem: 13835
[18:31:24.981743] [Train][Ep-47/100]  [ 600/1435]  eta: 0:17:04  lr: 0.0004 (0.0004)  loss: 1150.9177 (1172.6227)  grad_norm: 668.1814 (680.5731)  amp_scale: 1.0000 (1.0000)  time: 1.3119  data: 0.5595  max mem: 13835
[18:33:39.085279] [Train][Ep-47/100]  [ 700/1435]  eta: 0:15:13  lr: 0.0004 (0.0004)  loss: 1214.1384 (1177.3174)  grad_norm: 682.6824 (680.0678)  amp_scale: 1.0000 (1.0000)  time: 1.3438  data: 0.0401  max mem: 13835
[18:35:48.631323] [Train][Ep-47/100]  [ 800/1435]  eta: 0:13:13  lr: 0.0004 (0.0004)  loss: 1216.3441 (1180.1874)  grad_norm: 675.4855 (681.6013)  amp_scale: 1.0000 (1.0000)  time: 1.2030  data: 0.6341  max mem: 13835
[18:37:50.970374] [Train][Ep-47/100]  [ 900/1435]  eta: 0:11:07  lr: 0.0004 (0.0004)  loss: 1196.3054 (1183.1886)  grad_norm: 682.6716 (682.4931)  amp_scale: 1.0000 (1.0000)  time: 1.2509  data: 0.4869  max mem: 13835
[18:40:02.406149] [Train][Ep-47/100]  [1000/1435]  eta: 0:09:05  lr: 0.0004 (0.0004)  loss: 1215.6447 (1184.1263)  grad_norm: 686.7766 (682.2897)  amp_scale: 1.0000 (1.0000)  time: 1.3349  data: 0.7658  max mem: 13835
[18:42:18.569710] [Train][Ep-47/100]  [1100/1435]  eta: 0:07:03  lr: 0.0004 (0.0004)  loss: 1178.0541 (1182.7075)  grad_norm: 664.7325 (682.2073)  amp_scale: 1.0000 (1.0000)  time: 1.4051  data: 0.0180  max mem: 13835
[18:44:30.292941] [Train][Ep-47/100]  [1200/1435]  eta: 0:04:57  lr: 0.0004 (0.0004)  loss: 1164.9772 (1184.3382)  grad_norm: 688.3535 (682.7499)  amp_scale: 1.0000 (1.0000)  time: 1.3473  data: 0.0265  max mem: 13835
[18:46:35.917173] [Train][Ep-47/100]  [1300/1435]  eta: 0:02:51  lr: 0.0004 (0.0004)  loss: 1197.6962 (1185.6099)  grad_norm: 672.4777 (682.4052)  amp_scale: 1.0000 (1.0000)  time: 1.2088  data: 0.2025  max mem: 13835
[18:48:41.357545] [Train][Ep-47/100]  [1400/1435]  eta: 0:00:44  lr: 0.0004 (0.0004)  loss: 1157.4022 (1184.6074)  grad_norm: 678.5596 (683.1336)  amp_scale: 1.0000 (1.0000)  time: 1.1546  data: 0.1898  max mem: 13835
[18:49:19.761873] [Train][Ep-47/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1135.3761 (1183.4797)  grad_norm: 682.9915 (683.1641)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.2448  max mem: 13835
[18:49:19.762815] [Train][Ep-47/100] Total time: 0:30:12 (1.2630 s / it)
[18:49:19.763291] Syncing meters...
[18:49:20.523326] Averaged stats: lr: 0.0004 (0.0004)  loss: 1135.3761 (1190.4785)  grad_norm: 682.9915 (683.1641)  amp_scale: 1.0000 (1.0000)
[18:49:29.189334] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 48)
[18:49:31.312217] [Train][Ep-48/100]  [   0/1435]  eta: 0:50:34  lr: 0.0004 (0.0004)  time: 2.1147  data: 1.6311  max mem: 13835
[18:51:32.919347] [Train][Ep-48/100]  [ 100/1435]  eta: 0:27:15  lr: 0.0004 (0.0004)  loss: 1148.2114 (1174.8337)  grad_norm: 681.6232 (682.7982)  amp_scale: 1.0000 (1.0000)  time: 1.1498  data: 0.3979  max mem: 13835
[18:53:40.386582] [Train][Ep-48/100]  [ 200/1435]  eta: 0:25:43  lr: 0.0004 (0.0004)  loss: 1119.9181 (1154.6968)  grad_norm: 667.8810 (678.1535)  amp_scale: 1.0000 (1.0000)  time: 1.2586  data: 0.1133  max mem: 13835
[18:55:49.734918] [Train][Ep-48/100]  [ 300/1435]  eta: 0:23:54  lr: 0.0004 (0.0004)  loss: 1176.6222 (1165.5200)  grad_norm: 676.3246 (679.6463)  amp_scale: 1.0000 (1.0000)  time: 1.2932  data: 0.0830  max mem: 13835
[18:57:54.391581] [Train][Ep-48/100]  [ 400/1435]  eta: 0:21:43  lr: 0.0004 (0.0004)  loss: 1173.8595 (1164.7076)  grad_norm: 668.7905 (679.6379)  amp_scale: 1.0000 (1.0000)  time: 1.2611  data: 0.0381  max mem: 13835
[18:59:55.219770] [Train][Ep-48/100]  [ 500/1435]  eta: 0:19:28  lr: 0.0004 (0.0004)  loss: 1167.7915 (1165.0962)  grad_norm: 676.4262 (678.3191)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.5133  max mem: 13835
[19:01:54.665037] [Train][Ep-48/100]  [ 600/1435]  eta: 0:17:15  lr: 0.0004 (0.0004)  loss: 1137.9055 (1166.8286)  grad_norm: 686.8761 (682.1758)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.5864  max mem: 13835
[19:04:01.837063] [Train][Ep-48/100]  [ 700/1435]  eta: 0:15:14  lr: 0.0004 (0.0004)  loss: 1197.6586 (1170.6683)  grad_norm: 680.6995 (682.6021)  amp_scale: 1.0000 (1.0000)  time: 1.2543  data: 0.0034  max mem: 13835
[19:06:08.552559] [Train][Ep-48/100]  [ 800/1435]  eta: 0:13:12  lr: 0.0004 (0.0004)  loss: 1181.7994 (1171.0178)  grad_norm: 680.4111 (682.2411)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.0681  max mem: 13835
[19:08:19.062055] [Train][Ep-48/100]  [ 900/1435]  eta: 0:11:10  lr: 0.0004 (0.0004)  loss: 1184.8750 (1175.0137)  grad_norm: 688.0631 (684.3034)  amp_scale: 1.0000 (1.0000)  time: 1.2974  data: 0.0726  max mem: 13835
[19:10:21.775284] [Train][Ep-48/100]  [1000/1435]  eta: 0:09:04  lr: 0.0004 (0.0004)  loss: 1184.3890 (1177.8286)  grad_norm: 663.7953 (684.1836)  amp_scale: 1.0000 (1.0000)  time: 1.1544  data: 0.5017  max mem: 13835
[19:12:26.833058] [Train][Ep-48/100]  [1100/1435]  eta: 0:06:59  lr: 0.0004 (0.0004)  loss: 1218.4907 (1179.2224)  grad_norm: 709.3503 (685.8983)  amp_scale: 1.0000 (1.0000)  time: 1.2135  data: 0.0844  max mem: 13835
[19:14:30.714115] [Train][Ep-48/100]  [1200/1435]  eta: 0:04:53  lr: 0.0004 (0.0004)  loss: 1158.7905 (1179.1218)  grad_norm: 696.9403 (686.6340)  amp_scale: 1.0000 (1.0000)  time: 1.2615  data: 0.0129  max mem: 13835
[19:16:34.232187] [Train][Ep-48/100]  [1300/1435]  eta: 0:02:48  lr: 0.0004 (0.0004)  loss: 1160.1398 (1180.6960)  grad_norm: 700.9007 (687.1563)  amp_scale: 1.0000 (1.0000)  time: 1.2439  data: 0.0610  max mem: 13835
[19:18:40.483469] [Train][Ep-48/100]  [1400/1435]  eta: 0:00:43  lr: 0.0004 (0.0004)  loss: 1215.5453 (1181.7919)  grad_norm: 680.2610 (686.6707)  amp_scale: 1.0000 (1.0000)  time: 1.2168  data: 0.5978  max mem: 13835
[19:19:20.220286] [Train][Ep-48/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1182.5345 (1181.4351)  grad_norm: 666.8382 (686.3996)  amp_scale: 1.0000 (1.0000)  time: 1.1417  data: 0.5714  max mem: 13835
[19:19:20.221253] [Train][Ep-48/100] Total time: 0:29:51 (1.2481 s / it)
[19:19:20.221829] Syncing meters...
[19:19:20.223230] Averaged stats: lr: 0.0004 (0.0004)  loss: 1182.5345 (1184.4790)  grad_norm: 666.8382 (686.3996)  amp_scale: 1.0000 (1.0000)
[19:19:28.490038] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 49)
[19:19:30.458875] [Train][Ep-49/100]  [   0/1435]  eta: 0:46:53  lr: 0.0004 (0.0004)  time: 1.9603  data: 1.4753  max mem: 13835
[19:21:33.335084] [Train][Ep-49/100]  [ 100/1435]  eta: 0:27:30  lr: 0.0004 (0.0004)  loss: 1149.3081 (1161.1749)  grad_norm: 684.8180 (698.9444)  amp_scale: 1.0000 (1.0000)  time: 1.2143  data: 0.6429  max mem: 13835
[19:23:36.382531] [Train][Ep-49/100]  [ 200/1435]  eta: 0:25:23  lr: 0.0004 (0.0004)  loss: 1223.0603 (1185.6793)  grad_norm: 682.3945 (690.8449)  amp_scale: 1.0000 (1.0000)  time: 1.1967  data: 0.6026  max mem: 13835
[19:25:36.582522] [Train][Ep-49/100]  [ 300/1435]  eta: 0:23:07  lr: 0.0004 (0.0004)  loss: 1207.0515 (1185.1419)  grad_norm: 672.7352 (691.6273)  amp_scale: 1.0000 (1.0000)  time: 1.2267  data: 0.4981  max mem: 13835
[19:27:31.522554] [Train][Ep-49/100]  [ 400/1435]  eta: 0:20:46  lr: 0.0004 (0.0004)  loss: 1149.0736 (1181.0626)  grad_norm: 637.8390 (682.1273)  amp_scale: 1.0000 (1.0000)  time: 1.1370  data: 0.5184  max mem: 13835
[19:29:35.486710] [Train][Ep-49/100]  [ 500/1435]  eta: 0:18:52  lr: 0.0004 (0.0004)  loss: 1184.4620 (1179.6595)  grad_norm: 681.7974 (683.5098)  amp_scale: 1.0000 (1.0000)  time: 1.2344  data: 0.2493  max mem: 13835
[19:31:39.301713] [Train][Ep-49/100]  [ 600/1435]  eta: 0:16:55  lr: 0.0004 (0.0004)  loss: 1202.9749 (1181.4998)  grad_norm: 694.2524 (685.3262)  amp_scale: 1.0000 (1.0000)  time: 1.2449  data: 0.0023  max mem: 13835
[19:33:41.809435] [Train][Ep-49/100]  [ 700/1435]  eta: 0:14:54  lr: 0.0004 (0.0004)  loss: 1163.5671 (1179.3153)  grad_norm: 665.1235 (684.2489)  amp_scale: 1.0000 (1.0000)  time: 1.1833  data: 0.0615  max mem: 13835
[19:35:43.913092] [Train][Ep-49/100]  [ 800/1435]  eta: 0:12:53  lr: 0.0004 (0.0004)  loss: 1159.5569 (1179.9609)  grad_norm: 671.5950 (683.8558)  amp_scale: 1.0000 (1.0000)  time: 1.2200  data: 0.0017  max mem: 13835
[19:37:42.364360] [Train][Ep-49/100]  [ 900/1435]  eta: 0:10:49  lr: 0.0004 (0.0004)  loss: 1142.2201 (1178.4143)  grad_norm: 671.6208 (683.7845)  amp_scale: 1.0000 (1.0000)  time: 1.1390  data: 0.5265  max mem: 13835
[19:39:41.891398] [Train][Ep-49/100]  [1000/1435]  eta: 0:08:47  lr: 0.0004 (0.0004)  loss: 1179.4935 (1178.8945)  grad_norm: 673.2661 (683.8745)  amp_scale: 1.0000 (1.0000)  time: 1.1465  data: 0.3518  max mem: 13835
[19:41:38.973381] [Train][Ep-49/100]  [1100/1435]  eta: 0:06:44  lr: 0.0004 (0.0004)  loss: 1177.7185 (1179.2924)  grad_norm: 680.8226 (684.1746)  amp_scale: 1.0000 (1.0000)  time: 1.2510  data: 0.2583  max mem: 13835
[19:43:44.280052] [Train][Ep-49/100]  [1200/1435]  eta: 0:04:44  lr: 0.0004 (0.0004)  loss: 1122.5042 (1176.7715)  grad_norm: 681.1579 (684.7952)  amp_scale: 1.0000 (1.0000)  time: 1.1813  data: 0.1587  max mem: 13835
[19:45:42.163009] [Train][Ep-49/100]  [1300/1435]  eta: 0:02:43  lr: 0.0004 (0.0004)  loss: 1185.9069 (1176.2241)  grad_norm: 654.4052 (682.8214)  amp_scale: 1.0000 (1.0000)  time: 1.1611  data: 0.4213  max mem: 13835
[19:47:40.898585] [Train][Ep-49/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1190.2761 (1177.3174)  grad_norm: 670.1458 (682.1413)  amp_scale: 1.0000 (1.0000)  time: 1.2227  data: 0.1506  max mem: 13835
[19:48:22.754567] [Train][Ep-49/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1193.3210 (1177.6627)  grad_norm: 681.8937 (682.4397)  amp_scale: 1.0000 (1.0000)  time: 1.2332  data: 0.1138  max mem: 13835
[19:48:22.755482] [Train][Ep-49/100] Total time: 0:28:54 (1.2085 s / it)
[19:48:22.755910] Syncing meters...
[19:48:23.798440] Averaged stats: lr: 0.0004 (0.0004)  loss: 1193.3210 (1174.3626)  grad_norm: 681.8937 (682.4397)  amp_scale: 1.0000 (1.0000)
[19:48:33.595860] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 50)
[19:48:40.102778] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_0050.pth' (epoch 50)
[19:48:42.457647] [Train][Ep-50/100]  [   0/1435]  eta: 0:56:06  lr: 0.0004 (0.0004)  time: 2.3461  data: 1.8629  max mem: 13835
[19:50:45.212620] [Train][Ep-50/100]  [ 100/1435]  eta: 0:27:33  lr: 0.0004 (0.0004)  loss: 1187.1279 (1180.0168)  grad_norm: 660.6844 (669.0704)  amp_scale: 1.0000 (1.0000)  time: 1.2564  data: 0.6844  max mem: 13835
[19:52:44.675043] [Train][Ep-50/100]  [ 200/1435]  eta: 0:25:02  lr: 0.0004 (0.0004)  loss: 1166.8054 (1178.4417)  grad_norm: 687.6393 (679.1617)  amp_scale: 1.0000 (1.0000)  time: 1.2536  data: 0.2730  max mem: 13835
[19:54:40.970652] [Train][Ep-50/100]  [ 300/1435]  eta: 0:22:40  lr: 0.0004 (0.0004)  loss: 1137.2451 (1173.7198)  grad_norm: 676.4185 (682.4870)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.4805  max mem: 13835
[19:56:40.178444] [Train][Ep-50/100]  [ 400/1435]  eta: 0:20:39  lr: 0.0004 (0.0004)  loss: 1174.6663 (1174.0650)  grad_norm: 685.2906 (682.6724)  amp_scale: 1.0000 (1.0000)  time: 1.2153  data: 0.1364  max mem: 13835
[19:58:43.929222] [Train][Ep-50/100]  [ 500/1435]  eta: 0:18:46  lr: 0.0004 (0.0004)  loss: 1159.1042 (1175.2310)  grad_norm: 665.9003 (680.3498)  amp_scale: 1.0000 (1.0000)  time: 1.1453  data: 0.1970  max mem: 13835
[20:00:42.757959] [Train][Ep-50/100]  [ 600/1435]  eta: 0:16:43  lr: 0.0004 (0.0004)  loss: 1193.3032 (1179.1971)  grad_norm: 675.0751 (680.6011)  amp_scale: 1.0000 (1.0000)  time: 1.1570  data: 0.0228  max mem: 13835
[20:02:38.131932] [Train][Ep-50/100]  [ 700/1435]  eta: 0:14:38  lr: 0.0004 (0.0004)  loss: 1152.5186 (1176.6647)  grad_norm: 654.9960 (679.1157)  amp_scale: 1.0000 (1.0000)  time: 1.1663  data: 0.5878  max mem: 13835
[20:04:34.272479] [Train][Ep-50/100]  [ 800/1435]  eta: 0:12:36  lr: 0.0004 (0.0004)  loss: 1155.0063 (1177.1933)  grad_norm: 687.1877 (679.4013)  amp_scale: 1.0000 (1.0000)  time: 1.1985  data: 0.3752  max mem: 13835
[20:06:35.681340] [Train][Ep-50/100]  [ 900/1435]  eta: 0:10:38  lr: 0.0004 (0.0004)  loss: 1169.6212 (1176.9441)  grad_norm: 682.7834 (681.0992)  amp_scale: 1.0000 (1.0000)  time: 1.1967  data: 0.1150  max mem: 13835
[20:08:37.360575] [Train][Ep-50/100]  [1000/1435]  eta: 0:08:40  lr: 0.0004 (0.0004)  loss: 1172.9928 (1179.0716)  grad_norm: 662.7921 (680.7605)  amp_scale: 1.0000 (1.0000)  time: 1.2023  data: 0.0364  max mem: 13835
[20:10:35.993340] [Train][Ep-50/100]  [1100/1435]  eta: 0:06:40  lr: 0.0004 (0.0004)  loss: 1161.0522 (1179.6336)  grad_norm: 673.3253 (680.0117)  amp_scale: 1.0000 (1.0000)  time: 1.1433  data: 0.2826  max mem: 13835
[20:12:34.262550] [Train][Ep-50/100]  [1200/1435]  eta: 0:04:40  lr: 0.0004 (0.0004)  loss: 1170.9114 (1179.6588)  grad_norm: 676.0299 (680.6478)  amp_scale: 1.0000 (1.0000)  time: 1.2306  data: 0.2899  max mem: 13835
[20:14:33.842475] [Train][Ep-50/100]  [1300/1435]  eta: 0:02:41  lr: 0.0004 (0.0004)  loss: 1205.1625 (1181.5940)  grad_norm: 706.6396 (682.2604)  amp_scale: 1.0000 (1.0000)  time: 1.1937  data: 0.0091  max mem: 13835
[20:16:40.571984] [Train][Ep-50/100]  [1400/1435]  eta: 0:00:41  lr: 0.0004 (0.0004)  loss: 1200.5743 (1182.2697)  grad_norm: 686.4050 (682.5701)  amp_scale: 1.0000 (1.0000)  time: 1.2587  data: 0.0004  max mem: 13835
[20:17:20.768057] [Train][Ep-50/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1200.5743 (1182.1094)  grad_norm: 693.4883 (682.8494)  amp_scale: 1.0000 (1.0000)  time: 1.2006  data: 0.0350  max mem: 13835
[20:17:20.768931] [Train][Ep-50/100] Total time: 0:28:40 (1.1991 s / it)
[20:17:20.769367] Syncing meters...
[20:17:21.833489] Averaged stats: lr: 0.0004 (0.0004)  loss: 1200.5743 (1177.5390)  grad_norm: 693.4883 (682.8494)  amp_scale: 1.0000 (1.0000)
[20:17:23.941239] [Eval][Ep-50/100]  [  0/121]  eta: 0:04:13  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0991  data: 1.9386  max mem: 13835
[20:19:11.962962] [Eval][Ep-50/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9073  data: 0.7469  max mem: 13835
[20:19:29.351808] [Eval][Ep-50/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8694  data: 0.7134  max mem: 13835
[20:19:29.352653] [Eval][Ep-50/100] Total time: 0:02:07 (1.0538 s / it)
[20:19:30.023953] [Eval][Ep-50/100] val_acc1_image=27.79 | val_acc1_audio=41.14 | val_acc1_fusion=38.00 | val_acc1_all=52.44
[20:19:38.447692] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 51)
[20:19:40.209004] [Train][Ep-51/100]  [   0/1435]  eta: 0:41:55  lr: 0.0004 (0.0004)  time: 1.7530  data: 1.2720  max mem: 13835
[20:21:45.657512] [Train][Ep-51/100]  [ 100/1435]  eta: 0:28:01  lr: 0.0004 (0.0004)  loss: 1160.1096 (1154.6354)  grad_norm: 651.8353 (667.1221)  amp_scale: 1.0000 (1.0000)  time: 1.2936  data: 0.7242  max mem: 13835
[20:23:55.231822] [Train][Ep-51/100]  [ 200/1435]  eta: 0:26:17  lr: 0.0004 (0.0004)  loss: 1126.8334 (1163.1568)  grad_norm: 699.5887 (677.3779)  amp_scale: 1.0000 (1.0000)  time: 1.3402  data: 0.7701  max mem: 13835
[20:26:07.802967] [Train][Ep-51/100]  [ 300/1435]  eta: 0:24:28  lr: 0.0004 (0.0004)  loss: 1173.2518 (1167.4176)  grad_norm: 672.9262 (676.6734)  amp_scale: 1.0000 (1.0000)  time: 1.3567  data: 0.7841  max mem: 13835
[20:28:17.476226] [Train][Ep-51/100]  [ 400/1435]  eta: 0:22:19  lr: 0.0004 (0.0004)  loss: 1186.2111 (1170.0748)  grad_norm: 678.1544 (678.8718)  amp_scale: 1.0000 (1.0000)  time: 1.2376  data: 0.6675  max mem: 13835
[20:30:23.266033] [Train][Ep-51/100]  [ 500/1435]  eta: 0:20:03  lr: 0.0004 (0.0004)  loss: 1167.9438 (1166.3070)  grad_norm: 662.0781 (678.5566)  amp_scale: 1.0000 (1.0000)  time: 1.3093  data: 0.7094  max mem: 13835
[20:32:32.580997] [Train][Ep-51/100]  [ 600/1435]  eta: 0:17:55  lr: 0.0004 (0.0004)  loss: 1158.8729 (1169.9409)  grad_norm: 687.5097 (679.9652)  amp_scale: 1.0000 (1.0000)  time: 1.3368  data: 0.0010  max mem: 13835
[20:34:43.267365] [Train][Ep-51/100]  [ 700/1435]  eta: 0:15:48  lr: 0.0004 (0.0004)  loss: 1146.6760 (1168.3751)  grad_norm: 662.2072 (678.0262)  amp_scale: 1.0000 (1.0000)  time: 1.3108  data: 0.0010  max mem: 13835
[20:36:43.788746] [Train][Ep-51/100]  [ 800/1435]  eta: 0:13:32  lr: 0.0004 (0.0004)  loss: 1118.2788 (1165.1214)  grad_norm: 679.5869 (678.8936)  amp_scale: 1.0000 (1.0000)  time: 1.1773  data: 0.5544  max mem: 13835
[20:38:52.479983] [Train][Ep-51/100]  [ 900/1435]  eta: 0:11:25  lr: 0.0004 (0.0004)  loss: 1139.4882 (1166.7260)  grad_norm: 696.1478 (680.9787)  amp_scale: 1.0000 (1.0000)  time: 1.2808  data: 0.6853  max mem: 13835
[20:40:54.378183] [Train][Ep-51/100]  [1000/1435]  eta: 0:09:14  lr: 0.0004 (0.0004)  loss: 1166.8057 (1167.5265)  grad_norm: 670.9370 (680.1961)  amp_scale: 1.0000 (1.0000)  time: 1.1438  data: 0.5137  max mem: 13835
[20:42:58.663103] [Train][Ep-51/100]  [1100/1435]  eta: 0:07:06  lr: 0.0004 (0.0004)  loss: 1157.1603 (1167.0832)  grad_norm: 688.8856 (680.4089)  amp_scale: 1.0000 (1.0000)  time: 1.1789  data: 0.3270  max mem: 13835
[20:45:01.386744] [Train][Ep-51/100]  [1200/1435]  eta: 0:04:57  lr: 0.0004 (0.0004)  loss: 1187.1031 (1169.6906)  grad_norm: 675.2863 (680.6207)  amp_scale: 1.0000 (1.0000)  time: 1.2782  data: 0.1275  max mem: 13835
[20:47:04.956293] [Train][Ep-51/100]  [1300/1435]  eta: 0:02:50  lr: 0.0004 (0.0004)  loss: 1184.6532 (1171.5487)  grad_norm: 681.9580 (681.4439)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.2360  max mem: 13835
[20:49:03.757333] [Train][Ep-51/100]  [1400/1435]  eta: 0:00:44  lr: 0.0004 (0.0004)  loss: 1165.2773 (1171.6494)  grad_norm: 688.0062 (682.2077)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.1777  max mem: 13835
[20:49:44.334943] [Train][Ep-51/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1169.8188 (1172.1366)  grad_norm: 691.5917 (682.5419)  amp_scale: 1.0000 (1.0000)  time: 1.2216  data: 0.0031  max mem: 13835
[20:49:44.336541] [Train][Ep-51/100] Total time: 0:30:05 (1.2585 s / it)
[20:49:44.337882] Syncing meters...
[20:49:45.702764] Averaged stats: lr: 0.0004 (0.0004)  loss: 1169.8188 (1169.7191)  grad_norm: 691.5917 (682.5419)  amp_scale: 1.0000 (1.0000)
[20:49:55.390359] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 52)
[20:49:57.377052] [Train][Ep-52/100]  [   0/1435]  eta: 0:47:18  lr: 0.0004 (0.0004)  time: 1.9781  data: 1.4941  max mem: 13835
[20:51:54.701327] [Train][Ep-52/100]  [ 100/1435]  eta: 0:26:16  lr: 0.0004 (0.0004)  loss: 1107.7419 (1118.5965)  grad_norm: 659.2654 (666.1088)  amp_scale: 1.0000 (1.0000)  time: 1.2624  data: 0.6923  max mem: 13835
[20:53:59.868381] [Train][Ep-52/100]  [ 200/1435]  eta: 0:25:02  lr: 0.0004 (0.0004)  loss: 1126.7299 (1123.9784)  grad_norm: 661.0802 (666.4032)  amp_scale: 1.0000 (1.0000)  time: 1.2242  data: 0.6561  max mem: 13835
[20:56:07.649000] [Train][Ep-52/100]  [ 300/1435]  eta: 0:23:23  lr: 0.0004 (0.0004)  loss: 1139.4469 (1135.2258)  grad_norm: 657.8967 (672.0599)  amp_scale: 1.0000 (1.0000)  time: 1.2683  data: 0.6989  max mem: 13835
[20:58:12.543299] [Train][Ep-52/100]  [ 400/1435]  eta: 0:21:23  lr: 0.0004 (0.0004)  loss: 1172.5138 (1142.7165)  grad_norm: 679.2292 (674.0124)  amp_scale: 1.0000 (1.0000)  time: 1.2007  data: 0.6306  max mem: 13835
[21:00:08.472171] [Train][Ep-52/100]  [ 500/1435]  eta: 0:19:04  lr: 0.0004 (0.0004)  loss: 1136.3303 (1145.6976)  grad_norm: 663.1819 (676.6764)  amp_scale: 1.0000 (1.0000)  time: 1.1400  data: 0.1164  max mem: 13835
[21:02:12.183577] [Train][Ep-52/100]  [ 600/1435]  eta: 0:17:03  lr: 0.0004 (0.0004)  loss: 1136.0046 (1143.4286)  grad_norm: 692.4525 (677.8608)  amp_scale: 1.0000 (1.0000)  time: 1.2181  data: 0.2826  max mem: 13835
[21:04:19.652853] [Train][Ep-52/100]  [ 700/1435]  eta: 0:15:06  lr: 0.0004 (0.0004)  loss: 1127.6693 (1146.3078)  grad_norm: 683.1713 (678.7798)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.2638  max mem: 13835
[21:06:28.042728] [Train][Ep-52/100]  [ 800/1435]  eta: 0:13:06  lr: 0.0004 (0.0004)  loss: 1149.3053 (1147.4358)  grad_norm: 674.7038 (678.5810)  amp_scale: 1.0000 (1.0000)  time: 1.2378  data: 0.0205  max mem: 13835
[21:08:36.135729] [Train][Ep-52/100]  [ 900/1435]  eta: 0:11:05  lr: 0.0004 (0.0004)  loss: 1143.5776 (1151.0597)  grad_norm: 689.1245 (679.3071)  amp_scale: 1.0000 (1.0000)  time: 1.2148  data: 0.5124  max mem: 13835
[21:10:39.934812] [Train][Ep-52/100]  [1000/1435]  eta: 0:09:00  lr: 0.0004 (0.0004)  loss: 1208.1492 (1155.1862)  grad_norm: 678.2142 (679.8127)  amp_scale: 1.0000 (1.0000)  time: 1.2714  data: 0.7037  max mem: 13835
[21:12:43.442861] [Train][Ep-52/100]  [1100/1435]  eta: 0:06:56  lr: 0.0004 (0.0004)  loss: 1130.3726 (1154.6158)  grad_norm: 683.4285 (679.7857)  amp_scale: 1.0000 (1.0000)  time: 1.2243  data: 0.6557  max mem: 13835
[21:14:46.267891] [Train][Ep-52/100]  [1200/1435]  eta: 0:04:51  lr: 0.0004 (0.0004)  loss: 1166.5533 (1155.8309)  grad_norm: 679.1396 (680.2709)  amp_scale: 1.0000 (1.0000)  time: 1.2420  data: 0.6463  max mem: 13835
[21:16:52.213092] [Train][Ep-52/100]  [1300/1435]  eta: 0:02:47  lr: 0.0004 (0.0004)  loss: 1148.2936 (1154.9863)  grad_norm: 692.4160 (680.2100)  amp_scale: 1.0000 (1.0000)  time: 1.2863  data: 0.6899  max mem: 13835
[21:19:02.355438] [Train][Ep-52/100]  [1400/1435]  eta: 0:00:43  lr: 0.0004 (0.0004)  loss: 1113.2988 (1153.6660)  grad_norm: 672.3287 (679.8911)  amp_scale: 1.0000 (1.0000)  time: 1.3118  data: 0.0075  max mem: 13835
[21:19:43.274026] [Train][Ep-52/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1136.1289 (1152.7455)  grad_norm: 678.2998 (679.9703)  amp_scale: 1.0000 (1.0000)  time: 1.2193  data: 0.2791  max mem: 13835
[21:19:43.274945] [Train][Ep-52/100] Total time: 0:29:47 (1.2459 s / it)
[21:19:43.275489] Syncing meters...
[21:19:44.537842] Averaged stats: lr: 0.0004 (0.0004)  loss: 1136.1289 (1161.9342)  grad_norm: 678.2998 (679.9703)  amp_scale: 1.0000 (1.0000)
[21:19:52.707053] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 53)
[21:19:54.825550] [Train][Ep-53/100]  [   0/1435]  eta: 0:50:28  lr: 0.0004 (0.0004)  time: 2.1102  data: 1.6277  max mem: 13835
[21:21:57.442444] [Train][Ep-53/100]  [ 100/1435]  eta: 0:27:28  lr: 0.0004 (0.0004)  loss: 1134.8201 (1154.1893)  grad_norm: 690.2366 (685.1521)  amp_scale: 1.0000 (1.0000)  time: 1.2350  data: 0.6672  max mem: 13835
[21:23:59.296523] [Train][Ep-53/100]  [ 200/1435]  eta: 0:25:14  lr: 0.0004 (0.0004)  loss: 1178.8030 (1172.2062)  grad_norm: 679.5380 (685.6291)  amp_scale: 1.0000 (1.0000)  time: 1.2425  data: 0.6733  max mem: 13835
[21:26:01.082131] [Train][Ep-53/100]  [ 300/1435]  eta: 0:23:08  lr: 0.0004 (0.0004)  loss: 1135.3782 (1162.5010)  grad_norm: 678.0667 (684.5019)  amp_scale: 1.0000 (1.0000)  time: 1.1990  data: 0.6186  max mem: 13835
[21:28:02.592518] [Train][Ep-53/100]  [ 400/1435]  eta: 0:21:04  lr: 0.0004 (0.0004)  loss: 1112.9592 (1157.4765)  grad_norm: 702.2956 (685.4055)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.5325  max mem: 13835
[21:30:04.070092] [Train][Ep-53/100]  [ 500/1435]  eta: 0:19:00  lr: 0.0004 (0.0004)  loss: 1151.8879 (1160.8706)  grad_norm: 686.0920 (686.5419)  amp_scale: 1.0000 (1.0000)  time: 1.1604  data: 0.5878  max mem: 13835
[21:32:02.751995] [Train][Ep-53/100]  [ 600/1435]  eta: 0:16:54  lr: 0.0004 (0.0004)  loss: 1150.7517 (1162.2341)  grad_norm: 674.8680 (683.5119)  amp_scale: 1.0000 (1.0000)  time: 1.2031  data: 0.2225  max mem: 13835
[21:34:08.585935] [Train][Ep-53/100]  [ 700/1435]  eta: 0:14:57  lr: 0.0004 (0.0004)  loss: 1149.7500 (1159.0917)  grad_norm: 685.1614 (684.9637)  amp_scale: 1.0000 (1.0000)  time: 1.1999  data: 0.0003  max mem: 13835
[21:36:09.620478] [Train][Ep-53/100]  [ 800/1435]  eta: 0:12:54  lr: 0.0004 (0.0004)  loss: 1119.4658 (1158.5856)  grad_norm: 699.2654 (686.4266)  amp_scale: 1.0000 (1.0000)  time: 1.1307  data: 0.5467  max mem: 13835
[21:38:10.549860] [Train][Ep-53/100]  [ 900/1435]  eta: 0:10:51  lr: 0.0004 (0.0004)  loss: 1180.4330 (1162.2053)  grad_norm: 665.1484 (685.3169)  amp_scale: 1.0000 (1.0000)  time: 1.2355  data: 0.3338  max mem: 13835
[21:40:11.173953] [Train][Ep-53/100]  [1000/1435]  eta: 0:08:49  lr: 0.0004 (0.0004)  loss: 1169.5328 (1161.4442)  grad_norm: 669.7692 (684.1266)  amp_scale: 1.0000 (1.0000)  time: 1.2345  data: 0.3139  max mem: 13835
[21:42:19.801947] [Train][Ep-53/100]  [1100/1435]  eta: 0:06:49  lr: 0.0004 (0.0004)  loss: 1155.9897 (1159.8850)  grad_norm: 692.4535 (684.0544)  amp_scale: 1.0000 (1.0000)  time: 1.2163  data: 0.0171  max mem: 13835
[21:44:25.839809] [Train][Ep-53/100]  [1200/1435]  eta: 0:04:48  lr: 0.0004 (0.0004)  loss: 1167.5990 (1159.4288)  grad_norm: 682.4481 (684.4117)  amp_scale: 1.0000 (1.0000)  time: 1.2014  data: 0.3956  max mem: 13835
[21:46:22.728352] [Train][Ep-53/100]  [1300/1435]  eta: 0:02:44  lr: 0.0004 (0.0004)  loss: 1171.1309 (1161.5489)  grad_norm: 679.0032 (684.9128)  amp_scale: 1.0000 (1.0000)  time: 1.1571  data: 0.4930  max mem: 13835
[21:48:23.967660] [Train][Ep-53/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.9398)  grad_norm: 667.9927 (684.8692)  amp_scale: 1.0000 (1.0000)  time: 1.2706  data: 0.0190  max mem: 13835
[21:49:06.170333] [Train][Ep-53/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.0707)  grad_norm: 684.9609 (685.1899)  amp_scale: 1.0000 (1.0000)  time: 1.2777  data: 0.0017  max mem: 13835
[21:49:06.171427] [Train][Ep-53/100] Total time: 0:29:13 (1.2219 s / it)
[21:49:06.171855] Syncing meters...
[21:49:06.971946] Averaged stats: lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.9023)  grad_norm: 684.9609 (685.1899)  amp_scale: 1.0000 (1.0000)
[21:49:17.414904] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 54)
[21:49:19.214036] [Train][Ep-54/100]  [   0/1435]  eta: 0:42:49  lr: 0.0004 (0.0004)  time: 1.7909  data: 1.3080  max mem: 13835
[21:51:14.675173] [Train][Ep-54/100]  [ 100/1435]  eta: 0:25:49  lr: 0.0004 (0.0004)  loss: 1154.0098 (1141.5965)  grad_norm: 662.6190 (670.7087)  amp_scale: 1.0000 (1.0000)  time: 1.1383  data: 0.5375  max mem: 13835
[21:53:15.475592] [Train][Ep-54/100]  [ 200/1435]  eta: 0:24:22  lr: 0.0004 (0.0004)  loss: 1211.2192 (1167.7552)  grad_norm: 692.4968 (680.8701)  amp_scale: 1.0000 (1.0000)  time: 1.1664  data: 0.4729  max mem: 13835
[21:55:17.719596] [Train][Ep-54/100]  [ 300/1435]  eta: 0:22:38  lr: 0.0004 (0.0004)  loss: 1149.4923 (1163.0624)  grad_norm: 674.8604 (680.5169)  amp_scale: 1.0000 (1.0000)  time: 1.2144  data: 0.2253  max mem: 13835
[21:57:14.323421] [Train][Ep-54/100]  [ 400/1435]  eta: 0:20:30  lr: 0.0004 (0.0004)  loss: 1130.0554 (1154.6911)  grad_norm: 687.5696 (681.8224)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.3423  max mem: 13835
[21:59:16.797740] [Train][Ep-54/100]  [ 500/1435]  eta: 0:18:38  lr: 0.0004 (0.0004)  loss: 1135.2944 (1151.4310)  grad_norm: 688.3165 (679.7705)  amp_scale: 1.0000 (1.0000)  time: 1.1940  data: 0.0203  max mem: 13835
[22:01:19.472491] [Train][Ep-54/100]  [ 600/1435]  eta: 0:16:43  lr: 0.0004 (0.0004)  loss: 1133.1404 (1146.4122)  grad_norm: 683.3227 (681.4751)  amp_scale: 1.0000 (1.0000)  time: 1.2555  data: 0.0158  max mem: 13835
[22:03:19.767961] [Train][Ep-54/100]  [ 700/1435]  eta: 0:14:43  lr: 0.0004 (0.0004)  loss: 1151.0945 (1150.4387)  grad_norm: 669.1240 (682.1746)  amp_scale: 1.0000 (1.0000)  time: 1.2891  data: 0.0011  max mem: 13835
[22:05:18.692710] [Train][Ep-54/100]  [ 800/1435]  eta: 0:12:42  lr: 0.0004 (0.0004)  loss: 1110.8517 (1147.5824)  grad_norm: 671.4208 (681.2047)  amp_scale: 1.0000 (1.0000)  time: 1.1182  data: 0.4649  max mem: 13835
[22:07:20.539910] [Train][Ep-54/100]  [ 900/1435]  eta: 0:10:43  lr: 0.0004 (0.0004)  loss: 1150.1005 (1149.5950)  grad_norm: 684.2000 (682.6451)  amp_scale: 1.0000 (1.0000)  time: 1.3289  data: 0.1444  max mem: 13835
[22:09:24.097541] [Train][Ep-54/100]  [1000/1435]  eta: 0:08:44  lr: 0.0004 (0.0004)  loss: 1127.6464 (1150.1500)  grad_norm: 679.6629 (683.3499)  amp_scale: 1.0000 (1.0000)  time: 1.1577  data: 0.1691  max mem: 13835
[22:11:24.712485] [Train][Ep-54/100]  [1100/1435]  eta: 0:06:43  lr: 0.0004 (0.0004)  loss: 1158.1869 (1153.6645)  grad_norm: 691.6543 (684.9957)  amp_scale: 1.0000 (1.0000)  time: 1.2121  data: 0.0170  max mem: 13835
[22:13:28.000086] [Train][Ep-54/100]  [1200/1435]  eta: 0:04:43  lr: 0.0004 (0.0004)  loss: 1118.6244 (1151.6636)  grad_norm: 678.9918 (685.5771)  amp_scale: 1.0000 (1.0000)  time: 1.2895  data: 0.0333  max mem: 13835
[22:15:30.272207] [Train][Ep-54/100]  [1300/1435]  eta: 0:02:43  lr: 0.0004 (0.0004)  loss: 1160.2275 (1151.1234)  grad_norm: 673.7654 (685.3501)  amp_scale: 1.0000 (1.0000)  time: 1.2428  data: 0.1345  max mem: 13835
[22:17:31.284389] [Train][Ep-54/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1156.1467 (1151.8224)  grad_norm: 678.8384 (685.3433)  amp_scale: 1.0000 (1.0000)  time: 1.2291  data: 0.6150  max mem: 13835
[22:18:11.145633] [Train][Ep-54/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1153.1736 (1150.6489)  grad_norm: 689.2122 (685.8974)  amp_scale: 1.0000 (1.0000)  time: 1.1505  data: 0.4293  max mem: 13835
[22:18:11.146526] [Train][Ep-54/100] Total time: 0:28:53 (1.2082 s / it)
[22:18:11.146992] Syncing meters...
[22:18:11.148333] Averaged stats: lr: 0.0004 (0.0004)  loss: 1153.1736 (1154.3003)  grad_norm: 689.2122 (685.8974)  amp_scale: 1.0000 (1.0000)
[22:18:19.361978] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 55)
[22:18:21.147316] [Train][Ep-55/100]  [   0/1435]  eta: 0:42:29  lr: 0.0004 (0.0004)  time: 1.7768  data: 1.2926  max mem: 13835
[22:20:20.240002] [Train][Ep-55/100]  [ 100/1435]  eta: 0:26:37  lr: 0.0004 (0.0004)  loss: 1141.2808 (1136.2970)  grad_norm: 661.8936 (666.5656)  amp_scale: 1.0000 (1.0000)  time: 1.1934  data: 0.6237  max mem: 13835
[22:22:23.275811] [Train][Ep-55/100]  [ 200/1435]  eta: 0:24:58  lr: 0.0004 (0.0004)  loss: 1151.6367 (1143.5581)  grad_norm: 659.7584 (665.8780)  amp_scale: 1.0000 (1.0000)  time: 1.2496  data: 0.6773  max mem: 13835
[22:24:22.328316] [Train][Ep-55/100]  [ 300/1435]  eta: 0:22:48  lr: 0.0004 (0.0004)  loss: 1104.6223 (1143.8972)  grad_norm: 665.2432 (668.9612)  amp_scale: 1.0000 (1.0000)  time: 1.2374  data: 0.4207  max mem: 13835
[22:26:23.939753] [Train][Ep-55/100]  [ 400/1435]  eta: 0:20:50  lr: 0.0004 (0.0004)  loss: 1123.3938 (1145.6056)  grad_norm: 685.7966 (674.5473)  amp_scale: 1.0000 (1.0000)  time: 1.2995  data: 0.1377  max mem: 13835
[22:28:25.378082] [Train][Ep-55/100]  [ 500/1435]  eta: 0:18:50  lr: 0.0004 (0.0004)  loss: 1111.0420 (1146.5607)  grad_norm: 660.1848 (675.2542)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.2359  max mem: 13835
[22:30:25.769496] [Train][Ep-55/100]  [ 600/1435]  eta: 0:16:49  lr: 0.0004 (0.0004)  loss: 1141.8640 (1145.9449)  grad_norm: 670.3505 (676.2587)  amp_scale: 1.0000 (1.0000)  time: 1.2005  data: 0.6284  max mem: 13835
[22:32:23.507280] [Train][Ep-55/100]  [ 700/1435]  eta: 0:14:45  lr: 0.0004 (0.0004)  loss: 1090.7911 (1139.5533)  grad_norm: 702.6535 (679.8163)  amp_scale: 1.0000 (1.0000)  time: 1.1791  data: 0.6016  max mem: 13835
[22:34:20.185988] [Train][Ep-55/100]  [ 800/1435]  eta: 0:12:41  lr: 0.0004 (0.0004)  loss: 1157.9999 (1141.9294)  grad_norm: 684.4780 (680.6423)  amp_scale: 1.0000 (1.0000)  time: 1.1984  data: 0.6245  max mem: 13835
[22:36:16.633958] [Train][Ep-55/100]  [ 900/1435]  eta: 0:10:39  lr: 0.0004 (0.0004)  loss: 1114.5884 (1142.7498)  grad_norm: 673.4995 (681.4554)  amp_scale: 1.0000 (1.0000)  time: 1.1411  data: 0.5456  max mem: 13835
[22:38:17.132003] [Train][Ep-55/100]  [1000/1435]  eta: 0:08:40  lr: 0.0004 (0.0004)  loss: 1140.5420 (1144.7659)  grad_norm: 691.2094 (682.4989)  amp_scale: 1.0000 (1.0000)  time: 1.1405  data: 0.2221  max mem: 13835
[22:40:16.889119] [Train][Ep-55/100]  [1100/1435]  eta: 0:06:40  lr: 0.0003 (0.0004)  loss: 1148.3146 (1144.8140)  grad_norm: 681.6050 (682.7563)  amp_scale: 1.0000 (1.0000)  time: 1.2376  data: 0.0247  max mem: 13835
[22:42:19.584596] [Train][Ep-55/100]  [1200/1435]  eta: 0:04:41  lr: 0.0003 (0.0004)  loss: 1203.3051 (1146.4615)  grad_norm: 671.5699 (681.6957)  amp_scale: 1.0000 (1.0000)  time: 1.2135  data: 0.0013  max mem: 13835
[22:44:20.690580] [Train][Ep-55/100]  [1300/1435]  eta: 0:02:42  lr: 0.0003 (0.0004)  loss: 1140.7765 (1146.7267)  grad_norm: 695.2836 (682.0185)  amp_scale: 1.0000 (1.0000)  time: 1.1467  data: 0.4988  max mem: 13835
[22:46:18.684260] [Train][Ep-55/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0004)  loss: 1168.8640 (1148.5437)  grad_norm: 672.3698 (682.3125)  amp_scale: 1.0000 (1.0000)  time: 1.2398  data: 0.2570  max mem: 13835
[22:46:58.536568] [Train][Ep-55/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0004)  loss: 1155.6774 (1148.8123)  grad_norm: 696.6046 (682.9444)  amp_scale: 1.0000 (1.0000)  time: 1.1125  data: 0.3776  max mem: 13835
[22:46:58.537603] [Train][Ep-55/100] Total time: 0:28:39 (1.1980 s / it)
[22:46:58.538098] Syncing meters...
[22:46:58.824188] Averaged stats: lr: 0.0003 (0.0004)  loss: 1155.6774 (1147.5761)  grad_norm: 696.6046 (682.9444)  amp_scale: 1.0000 (1.0000)
[22:47:07.290260] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 56)
[22:47:09.639747] [Train][Ep-56/100]  [   0/1435]  eta: 0:55:59  lr: 0.0003 (0.0003)  time: 2.3410  data: 1.8561  max mem: 13835
[22:49:04.929467] [Train][Ep-56/100]  [ 100/1435]  eta: 0:25:54  lr: 0.0003 (0.0003)  loss: 1171.2557 (1149.3444)  grad_norm: 660.5222 (681.5519)  amp_scale: 1.0000 (1.0000)  time: 1.2122  data: 0.5795  max mem: 13835
[22:51:02.640486] [Train][Ep-56/100]  [ 200/1435]  eta: 0:24:05  lr: 0.0003 (0.0003)  loss: 1157.1643 (1149.4671)  grad_norm: 689.5543 (686.4041)  amp_scale: 1.0000 (1.0000)  time: 1.1572  data: 0.2369  max mem: 13835
[22:52:58.115495] [Train][Ep-56/100]  [ 300/1435]  eta: 0:22:02  lr: 0.0003 (0.0003)  loss: 1144.9154 (1148.0577)  grad_norm: 677.5391 (681.3846)  amp_scale: 1.0000 (1.0000)  time: 1.1468  data: 0.5664  max mem: 13835
[22:54:56.237079] [Train][Ep-56/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1125.3744 (1140.7114)  grad_norm: 681.6371 (683.9067)  amp_scale: 1.0000 (1.0000)  time: 1.1891  data: 0.6165  max mem: 13835
[22:56:52.486703] [Train][Ep-56/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0003 (0.0003)  loss: 1134.6815 (1141.7886)  grad_norm: 693.0525 (686.1660)  amp_scale: 1.0000 (1.0000)  time: 1.2068  data: 0.0280  max mem: 13835
[22:58:56.389669] [Train][Ep-56/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0003 (0.0003)  loss: 1157.5814 (1141.7275)  grad_norm: 675.6992 (685.4572)  amp_scale: 1.0000 (1.0000)  time: 1.2515  data: 0.0006  max mem: 13835
[23:00:55.195567] [Train][Ep-56/100]  [ 700/1435]  eta: 0:14:27  lr: 0.0003 (0.0003)  loss: 1124.1108 (1140.3758)  grad_norm: 683.4939 (685.5036)  amp_scale: 1.0000 (1.0000)  time: 1.0967  data: 0.3211  max mem: 13835
[23:02:57.673276] [Train][Ep-56/100]  [ 800/1435]  eta: 0:12:33  lr: 0.0003 (0.0003)  loss: 1132.4875 (1138.4059)  grad_norm: 668.8890 (684.1729)  amp_scale: 1.0000 (1.0000)  time: 1.2179  data: 0.1942  max mem: 13835
[23:04:52.444287] [Train][Ep-56/100]  [ 900/1435]  eta: 0:10:32  lr: 0.0003 (0.0003)  loss: 1180.1276 (1142.8817)  grad_norm: 671.0045 (683.9162)  amp_scale: 1.0000 (1.0000)  time: 1.1070  data: 0.0915  max mem: 13835
[23:06:45.159687] [Train][Ep-56/100]  [1000/1435]  eta: 0:08:31  lr: 0.0003 (0.0003)  loss: 1136.1702 (1141.5383)  grad_norm: 685.4603 (683.6448)  amp_scale: 1.0000 (1.0000)  time: 1.0949  data: 0.3471  max mem: 13835
[23:08:41.828217] [Train][Ep-56/100]  [1100/1435]  eta: 0:06:33  lr: 0.0003 (0.0003)  loss: 1152.7764 (1140.7475)  grad_norm: 697.9684 (685.6197)  amp_scale: 1.0000 (1.0000)  time: 1.1906  data: 0.0002  max mem: 13835
[23:10:38.031418] [Train][Ep-56/100]  [1200/1435]  eta: 0:04:36  lr: 0.0003 (0.0003)  loss: 1136.8176 (1140.4033)  grad_norm: 672.1470 (685.3969)  amp_scale: 1.0000 (1.0000)  time: 1.1804  data: 0.0009  max mem: 13835
[23:12:32.816248] [Train][Ep-56/100]  [1300/1435]  eta: 0:02:38  lr: 0.0003 (0.0003)  loss: 1124.1840 (1140.5575)  grad_norm: 666.4873 (684.3965)  amp_scale: 1.0000 (1.0000)  time: 1.1916  data: 0.4459  max mem: 13835
[23:14:34.604688] [Train][Ep-56/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1142.5516 (1142.1739)  grad_norm: 685.4128 (684.6443)  amp_scale: 1.0000 (1.0000)  time: 1.2900  data: 0.0201  max mem: 13835
[23:15:15.637894] [Train][Ep-56/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1118.6050 (1142.6452)  grad_norm: 685.4128 (684.6523)  amp_scale: 1.0000 (1.0000)  time: 1.2060  data: 0.0903  max mem: 13835
[23:15:15.638854] [Train][Ep-56/100] Total time: 0:28:08 (1.1765 s / it)
[23:15:15.639333] Syncing meters...
[23:15:16.126118] Averaged stats: lr: 0.0003 (0.0003)  loss: 1118.6050 (1142.6959)  grad_norm: 685.4128 (684.6523)  amp_scale: 1.0000 (1.0000)
[23:15:25.796749] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 57)
[23:15:27.976318] [Train][Ep-57/100]  [   0/1435]  eta: 0:51:55  lr: 0.0003 (0.0003)  time: 2.1712  data: 1.6876  max mem: 13835
[23:17:22.446020] [Train][Ep-57/100]  [ 100/1435]  eta: 0:25:41  lr: 0.0003 (0.0003)  loss: 1166.3811 (1164.5284)  grad_norm: 660.1027 (674.4946)  amp_scale: 1.0000 (1.0000)  time: 1.1325  data: 0.2718  max mem: 13835
[23:19:14.657367] [Train][Ep-57/100]  [ 200/1435]  eta: 0:23:26  lr: 0.0003 (0.0003)  loss: 1148.8130 (1148.1609)  grad_norm: 663.6804 (673.4579)  amp_scale: 1.0000 (1.0000)  time: 1.1313  data: 0.0910  max mem: 13835
[23:21:05.469947] [Train][Ep-57/100]  [ 300/1435]  eta: 0:21:20  lr: 0.0003 (0.0003)  loss: 1163.1217 (1151.8936)  grad_norm: 688.0098 (681.1293)  amp_scale: 1.0000 (1.0000)  time: 1.1270  data: 0.3344  max mem: 13835
[23:23:02.842688] [Train][Ep-57/100]  [ 400/1435]  eta: 0:19:39  lr: 0.0003 (0.0003)  loss: 1153.6860 (1149.7753)  grad_norm: 691.8072 (685.7582)  amp_scale: 1.0000 (1.0000)  time: 1.1846  data: 0.0007  max mem: 13835
[23:25:01.686498] [Train][Ep-57/100]  [ 500/1435]  eta: 0:17:54  lr: 0.0003 (0.0003)  loss: 1134.9458 (1147.4420)  grad_norm: 674.8597 (684.7729)  amp_scale: 1.0000 (1.0000)  time: 1.1780  data: 0.0439  max mem: 13835
[23:26:56.526743] [Train][Ep-57/100]  [ 600/1435]  eta: 0:15:59  lr: 0.0003 (0.0003)  loss: 1112.9624 (1142.1629)  grad_norm: 685.6636 (684.8090)  amp_scale: 1.0000 (1.0000)  time: 1.1137  data: 0.0827  max mem: 13835
[23:28:51.892590] [Train][Ep-57/100]  [ 700/1435]  eta: 0:14:05  lr: 0.0003 (0.0003)  loss: 1089.9392 (1136.0436)  grad_norm: 670.7135 (683.6975)  amp_scale: 1.0000 (1.0000)  time: 1.1527  data: 0.2234  max mem: 13835
[23:30:47.847802] [Train][Ep-57/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0003 (0.0003)  loss: 1158.0289 (1138.8569)  grad_norm: 682.6401 (684.0961)  amp_scale: 1.0000 (1.0000)  time: 1.1856  data: 0.1746  max mem: 13835
[23:32:41.270869] [Train][Ep-57/100]  [ 900/1435]  eta: 0:10:14  lr: 0.0003 (0.0003)  loss: 1153.8628 (1139.0711)  grad_norm: 680.1334 (683.2176)  amp_scale: 1.0000 (1.0000)  time: 1.1139  data: 0.1262  max mem: 13835
[23:34:32.871826] [Train][Ep-57/100]  [1000/1435]  eta: 0:08:18  lr: 0.0003 (0.0003)  loss: 1137.6764 (1140.5236)  grad_norm: 704.2466 (685.1673)  amp_scale: 1.0000 (1.0000)  time: 1.1038  data: 0.3665  max mem: 13835
[23:36:34.977751] [Train][Ep-57/100]  [1100/1435]  eta: 0:06:26  lr: 0.0003 (0.0003)  loss: 1111.9238 (1139.9551)  grad_norm: 679.0955 (683.9455)  amp_scale: 1.0000 (1.0000)  time: 1.1804  data: 0.4552  max mem: 13835
[23:38:33.124439] [Train][Ep-57/100]  [1200/1435]  eta: 0:04:31  lr: 0.0003 (0.0003)  loss: 1120.8940 (1139.7984)  grad_norm: 653.8906 (682.8337)  amp_scale: 1.0000 (1.0000)  time: 1.1785  data: 0.1015  max mem: 13835
[23:40:28.943984] [Train][Ep-57/100]  [1300/1435]  eta: 0:02:35  lr: 0.0003 (0.0003)  loss: 1103.7347 (1138.4977)  grad_norm: 695.6777 (683.7147)  amp_scale: 1.0000 (1.0000)  time: 1.1890  data: 0.1213  max mem: 13835
[23:42:26.589742] [Train][Ep-57/100]  [1400/1435]  eta: 0:00:40  lr: 0.0003 (0.0003)  loss: 1130.4768 (1139.2786)  grad_norm: 691.8406 (684.2135)  amp_scale: 1.0000 (1.0000)  time: 1.2423  data: 0.2411  max mem: 13835
[23:43:06.086102] [Train][Ep-57/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1130.4768 (1140.1219)  grad_norm: 682.9067 (684.2416)  amp_scale: 1.0000 (1.0000)  time: 1.1986  data: 0.2395  max mem: 13835
[23:43:06.086952] [Train][Ep-57/100] Total time: 0:27:40 (1.1570 s / it)
[23:43:06.087467] Syncing meters...
[23:43:06.540055] Averaged stats: lr: 0.0003 (0.0003)  loss: 1130.4768 (1137.6848)  grad_norm: 682.9067 (684.2416)  amp_scale: 1.0000 (1.0000)
[23:43:14.996928] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 58)
[23:43:17.012279] [Train][Ep-58/100]  [   0/1435]  eta: 0:48:00  lr: 0.0003 (0.0003)  time: 2.0070  data: 1.5237  max mem: 13835
[23:45:12.486476] [Train][Ep-58/100]  [ 100/1435]  eta: 0:25:52  lr: 0.0003 (0.0003)  loss: 1123.9567 (1136.3262)  grad_norm: 679.7964 (682.1668)  amp_scale: 1.0000 (1.0000)  time: 1.1698  data: 0.5968  max mem: 13835
[23:47:13.697270] [Train][Ep-58/100]  [ 200/1435]  eta: 0:24:26  lr: 0.0003 (0.0003)  loss: 1121.3147 (1130.5098)  grad_norm: 658.5123 (679.9005)  amp_scale: 1.0000 (1.0000)  time: 1.2438  data: 0.6754  max mem: 13835
[23:49:13.017759] [Train][Ep-58/100]  [ 300/1435]  eta: 0:22:29  lr: 0.0003 (0.0003)  loss: 1119.1896 (1132.8614)  grad_norm: 671.0079 (680.3062)  amp_scale: 1.0000 (1.0000)  time: 1.1570  data: 0.5844  max mem: 13835
[23:51:08.672401] [Train][Ep-58/100]  [ 400/1435]  eta: 0:20:22  lr: 0.0003 (0.0003)  loss: 1118.3818 (1129.6602)  grad_norm: 680.8212 (681.9142)  amp_scale: 1.0000 (1.0000)  time: 1.1978  data: 0.6275  max mem: 13835
[23:53:07.747398] [Train][Ep-58/100]  [ 500/1435]  eta: 0:18:26  lr: 0.0003 (0.0003)  loss: 1129.3602 (1134.2819)  grad_norm: 666.3541 (679.0897)  amp_scale: 1.0000 (1.0000)  time: 1.2545  data: 0.6834  max mem: 13835
[23:55:00.346386] [Train][Ep-58/100]  [ 600/1435]  eta: 0:16:19  lr: 0.0003 (0.0003)  loss: 1116.7169 (1134.6632)  grad_norm: 682.3671 (679.5563)  amp_scale: 1.0000 (1.0000)  time: 1.1484  data: 0.5798  max mem: 13835
[23:56:55.074598] [Train][Ep-58/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0003 (0.0003)  loss: 1105.2561 (1135.1032)  grad_norm: 677.1669 (678.9622)  amp_scale: 1.0000 (1.0000)  time: 1.1257  data: 0.1328  max mem: 13835
[23:58:48.982913] [Train][Ep-58/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0003 (0.0003)  loss: 1122.3890 (1132.4289)  grad_norm: 666.1663 (678.4088)  amp_scale: 1.0000 (1.0000)  time: 1.2112  data: 0.0773  max mem: 13835
[00:00:47.145657] [Train][Ep-58/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0003 (0.0003)  loss: 1147.1532 (1134.2992)  grad_norm: 661.0896 (677.4008)  amp_scale: 1.0000 (1.0000)  time: 1.1493  data: 0.1662  max mem: 13835
[00:02:50.006134] [Train][Ep-58/100]  [1000/1435]  eta: 0:08:30  lr: 0.0003 (0.0003)  loss: 1089.9968 (1131.6343)  grad_norm: 690.9485 (678.8664)  amp_scale: 1.0000 (1.0000)  time: 1.2388  data: 0.0028  max mem: 13835
[00:04:50.015858] [Train][Ep-58/100]  [1100/1435]  eta: 0:06:34  lr: 0.0003 (0.0003)  loss: 1144.5507 (1132.6220)  grad_norm: 684.4959 (679.5155)  amp_scale: 1.0000 (1.0000)  time: 1.2591  data: 0.0136  max mem: 13835
[00:06:52.165554] [Train][Ep-58/100]  [1200/1435]  eta: 0:04:37  lr: 0.0003 (0.0003)  loss: 1134.5656 (1132.8019)  grad_norm: 683.3121 (678.9261)  amp_scale: 1.0000 (1.0000)  time: 1.2159  data: 0.0020  max mem: 13835
[00:08:51.071763] [Train][Ep-58/100]  [1300/1435]  eta: 0:02:39  lr: 0.0003 (0.0003)  loss: 1150.5400 (1134.0634)  grad_norm: 681.1357 (679.6273)  amp_scale: 1.0000 (1.0000)  time: 1.2163  data: 0.6449  max mem: 13835
[00:10:49.737179] [Train][Ep-58/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1135.3921 (1134.7242)  grad_norm: 670.3719 (679.7641)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.1663  max mem: 13835
[00:11:30.581303] [Train][Ep-58/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1099.1763 (1133.5599)  grad_norm: 645.7031 (679.4032)  amp_scale: 1.0000 (1.0000)  time: 1.2172  data: 0.0010  max mem: 13835
[00:11:30.582170] [Train][Ep-58/100] Total time: 0:28:15 (1.1816 s / it)
[00:11:30.582663] Syncing meters...
[00:11:30.926941] Averaged stats: lr: 0.0003 (0.0003)  loss: 1099.1763 (1132.5552)  grad_norm: 645.7031 (679.4032)  amp_scale: 1.0000 (1.0000)
[00:11:40.739031] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 59)
[00:11:42.578797] [Train][Ep-59/100]  [   0/1435]  eta: 0:43:48  lr: 0.0003 (0.0003)  time: 1.8316  data: 1.3472  max mem: 13835
[00:13:37.871907] [Train][Ep-59/100]  [ 100/1435]  eta: 0:25:48  lr: 0.0003 (0.0003)  loss: 1121.9197 (1127.0439)  grad_norm: 669.4777 (671.0205)  amp_scale: 1.0000 (1.0000)  time: 1.1275  data: 0.5560  max mem: 13835
[00:15:30.704097] [Train][Ep-59/100]  [ 200/1435]  eta: 0:23:32  lr: 0.0003 (0.0003)  loss: 1122.8838 (1129.6525)  grad_norm: 667.5913 (674.4738)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.5471  max mem: 13835
[00:17:30.637969] [Train][Ep-59/100]  [ 300/1435]  eta: 0:21:59  lr: 0.0003 (0.0003)  loss: 1131.4346 (1130.4302)  grad_norm: 667.4335 (675.7158)  amp_scale: 1.0000 (1.0000)  time: 1.2679  data: 0.0696  max mem: 13835
[00:19:28.004494] [Train][Ep-59/100]  [ 400/1435]  eta: 0:20:05  lr: 0.0003 (0.0003)  loss: 1122.6986 (1131.1573)  grad_norm: 690.5157 (679.4574)  amp_scale: 1.0000 (1.0000)  time: 1.1541  data: 0.4166  max mem: 13835
[00:21:31.920934] [Train][Ep-59/100]  [ 500/1435]  eta: 0:18:23  lr: 0.0003 (0.0003)  loss: 1121.2173 (1134.8589)  grad_norm: 666.8882 (679.1541)  amp_scale: 1.0000 (1.0000)  time: 1.1930  data: 0.6240  max mem: 13835
[00:23:28.193161] [Train][Ep-59/100]  [ 600/1435]  eta: 0:16:22  lr: 0.0003 (0.0003)  loss: 1148.8973 (1137.8164)  grad_norm: 676.7567 (679.0790)  amp_scale: 1.0000 (1.0000)  time: 1.1692  data: 0.2424  max mem: 13835
[00:25:32.015972] [Train][Ep-59/100]  [ 700/1435]  eta: 0:14:31  lr: 0.0003 (0.0003)  loss: 1112.8109 (1135.4391)  grad_norm: 667.3809 (678.5201)  amp_scale: 1.0000 (1.0000)  time: 1.2194  data: 0.2517  max mem: 13835
[00:27:37.523877] [Train][Ep-59/100]  [ 800/1435]  eta: 0:12:38  lr: 0.0003 (0.0003)  loss: 1104.4401 (1132.4668)  grad_norm: 658.2122 (677.6956)  amp_scale: 1.0000 (1.0000)  time: 1.2705  data: 0.1050  max mem: 13835
[00:29:35.858597] [Train][Ep-59/100]  [ 900/1435]  eta: 0:10:38  lr: 0.0003 (0.0003)  loss: 1092.6887 (1130.4556)  grad_norm: 692.7753 (679.1812)  amp_scale: 1.0000 (1.0000)  time: 1.1475  data: 0.1170  max mem: 13835
[00:31:30.263964] [Train][Ep-59/100]  [1000/1435]  eta: 0:08:36  lr: 0.0003 (0.0003)  loss: 1126.9747 (1130.6399)  grad_norm: 690.8911 (680.1996)  amp_scale: 1.0000 (1.0000)  time: 1.2026  data: 0.0468  max mem: 13835
[00:33:32.194873] [Train][Ep-59/100]  [1100/1435]  eta: 0:06:39  lr: 0.0003 (0.0003)  loss: 1139.3704 (1130.7458)  grad_norm: 669.4537 (679.8428)  amp_scale: 1.0000 (1.0000)  time: 1.1849  data: 0.0088  max mem: 13835
[00:35:26.403834] [Train][Ep-59/100]  [1200/1435]  eta: 0:04:38  lr: 0.0003 (0.0003)  loss: 1094.7814 (1130.0162)  grad_norm: 662.0041 (679.8365)  amp_scale: 1.0000 (1.0000)  time: 1.1676  data: 0.2486  max mem: 13835
[00:37:31.174856] [Train][Ep-59/100]  [1300/1435]  eta: 0:02:40  lr: 0.0003 (0.0003)  loss: 1114.6605 (1130.3766)  grad_norm: 654.4799 (679.0791)  amp_scale: 1.0000 (1.0000)  time: 1.2487  data: 0.0679  max mem: 13835
[00:39:35.479840] [Train][Ep-59/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.1259)  grad_norm: 675.0012 (678.7607)  amp_scale: 1.0000 (1.0000)  time: 1.2115  data: 0.1826  max mem: 13835
[00:40:13.544532] [Train][Ep-59/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.0173)  grad_norm: 676.5873 (678.4562)  amp_scale: 1.0000 (1.0000)  time: 1.1195  data: 0.1690  max mem: 13835
[00:40:13.545321] [Train][Ep-59/100] Total time: 0:28:32 (1.1936 s / it)
[00:40:13.545703] Syncing meters...
[00:40:14.534879] Averaged stats: lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.7311)  grad_norm: 676.5873 (678.4562)  amp_scale: 1.0000 (1.0000)
[00:40:23.072528] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 60)
[00:40:25.035474] [Train][Ep-60/100]  [   0/1435]  eta: 0:46:44  lr: 0.0003 (0.0003)  time: 1.9546  data: 1.4695  max mem: 13835
[00:42:21.111501] [Train][Ep-60/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0003 (0.0003)  loss: 1094.6414 (1116.9438)  grad_norm: 694.8044 (697.4207)  amp_scale: 1.0000 (1.0000)  time: 1.1487  data: 0.5433  max mem: 13835
[00:44:18.665005] [Train][Ep-60/100]  [ 200/1435]  eta: 0:24:07  lr: 0.0003 (0.0003)  loss: 1117.0344 (1113.3056)  grad_norm: 668.2070 (687.3980)  amp_scale: 1.0000 (1.0000)  time: 1.1791  data: 0.3261  max mem: 13835
[00:46:12.281303] [Train][Ep-60/100]  [ 300/1435]  eta: 0:21:56  lr: 0.0003 (0.0003)  loss: 1126.1998 (1120.8908)  grad_norm: 673.8222 (686.5567)  amp_scale: 1.0000 (1.0000)  time: 1.1295  data: 0.3297  max mem: 13835
[00:48:12.122154] [Train][Ep-60/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1130.1552 (1125.0227)  grad_norm: 672.2643 (683.5852)  amp_scale: 1.0000 (1.0000)  time: 1.2429  data: 0.0033  max mem: 13835
[00:50:11.196588] [Train][Ep-60/100]  [ 500/1435]  eta: 0:18:17  lr: 0.0003 (0.0003)  loss: 1133.7047 (1129.8838)  grad_norm: 660.3648 (681.4690)  amp_scale: 1.0000 (1.0000)  time: 1.1497  data: 0.4981  max mem: 13835
[00:52:14.155874] [Train][Ep-60/100]  [ 600/1435]  eta: 0:16:27  lr: 0.0003 (0.0003)  loss: 1167.6117 (1133.9240)  grad_norm: 683.9601 (683.5630)  amp_scale: 1.0000 (1.0000)  time: 1.2460  data: 0.6714  max mem: 13835
[00:54:14.641025] [Train][Ep-60/100]  [ 700/1435]  eta: 0:14:31  lr: 0.0003 (0.0003)  loss: 1099.5969 (1131.0482)  grad_norm: 676.0014 (682.6267)  amp_scale: 1.0000 (1.0000)  time: 1.1268  data: 0.5566  max mem: 13835
[00:56:08.470202] [Train][Ep-60/100]  [ 800/1435]  eta: 0:12:29  lr: 0.0003 (0.0003)  loss: 1075.4445 (1128.7306)  grad_norm: 674.8703 (681.7621)  amp_scale: 1.0000 (1.0000)  time: 1.1797  data: 0.2166  max mem: 13835
[00:58:05.535190] [Train][Ep-60/100]  [ 900/1435]  eta: 0:10:30  lr: 0.0003 (0.0003)  loss: 1121.8022 (1129.3915)  grad_norm: 660.2281 (681.4753)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.0156  max mem: 13835
[01:00:08.764445] [Train][Ep-60/100]  [1000/1435]  eta: 0:08:35  lr: 0.0003 (0.0003)  loss: 1093.3651 (1127.1926)  grad_norm: 686.1987 (682.5075)  amp_scale: 1.0000 (1.0000)  time: 1.2857  data: 0.0270  max mem: 13835
[01:02:04.984669] [Train][Ep-60/100]  [1100/1435]  eta: 0:06:36  lr: 0.0003 (0.0003)  loss: 1124.1228 (1126.2518)  grad_norm: 701.0429 (685.0812)  amp_scale: 1.0000 (1.0000)  time: 1.1309  data: 0.2347  max mem: 13835
[01:04:00.766599] [Train][Ep-60/100]  [1200/1435]  eta: 0:04:37  lr: 0.0003 (0.0003)  loss: 1105.4857 (1126.4349)  grad_norm: 685.3036 (685.6680)  amp_scale: 1.0000 (1.0000)  time: 1.1076  data: 0.4596  max mem: 13835
[01:05:58.012380] [Train][Ep-60/100]  [1300/1435]  eta: 0:02:39  lr: 0.0003 (0.0003)  loss: 1130.5465 (1127.4025)  grad_norm: 680.1862 (685.3178)  amp_scale: 1.0000 (1.0000)  time: 1.1188  data: 0.4483  max mem: 13835
[01:07:58.898693] [Train][Ep-60/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1112.8419 (1127.8726)  grad_norm: 668.3009 (684.4726)  amp_scale: 1.0000 (1.0000)  time: 1.2231  data: 0.0761  max mem: 13835
[01:08:39.671287] [Train][Ep-60/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1112.8419 (1127.4339)  grad_norm: 684.4253 (684.4345)  amp_scale: 1.0000 (1.0000)  time: 1.2247  data: 0.0513  max mem: 13835
[01:08:39.672158] [Train][Ep-60/100] Total time: 0:28:16 (1.1823 s / it)
[01:08:39.672549] Syncing meters...
[01:08:40.119617] Averaged stats: lr: 0.0003 (0.0003)  loss: 1112.8419 (1125.8958)  grad_norm: 684.4253 (684.4345)  amp_scale: 1.0000 (1.0000)
[01:08:42.257231] [Eval][Ep-60/100]  [  0/121]  eta: 0:04:17  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.1297  data: 1.9686  max mem: 13835
[01:10:26.333891] [Eval][Ep-60/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9328  data: 0.7732  max mem: 13835
[01:10:44.274018] [Eval][Ep-60/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8968  data: 0.7399  max mem: 13835
[01:10:44.274885] [Eval][Ep-60/100] Total time: 0:02:04 (1.0260 s / it)
[01:10:45.674491] [Eval][Ep-60/100] val_acc1_image=27.69 | val_acc1_audio=41.80 | val_acc1_fusion=38.59 | val_acc1_all=52.43
[01:10:55.227662] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 61)
[01:10:57.135092] [Train][Ep-61/100]  [   0/1435]  eta: 0:45:25  lr: 0.0003 (0.0003)  time: 1.8993  data: 1.4193  max mem: 13835
[01:12:56.818908] [Train][Ep-61/100]  [ 100/1435]  eta: 0:26:47  lr: 0.0003 (0.0003)  loss: 1133.1208 (1150.3961)  grad_norm: 680.7669 (688.3679)  amp_scale: 1.0000 (1.0000)  time: 1.1259  data: 0.5549  max mem: 13835
[01:14:51.322155] [Train][Ep-61/100]  [ 200/1435]  eta: 0:24:10  lr: 0.0003 (0.0003)  loss: 1103.6975 (1132.3689)  grad_norm: 676.0978 (682.5214)  amp_scale: 1.0000 (1.0000)  time: 1.1893  data: 0.0011  max mem: 13835
[01:16:49.796544] [Train][Ep-61/100]  [ 300/1435]  eta: 0:22:16  lr: 0.0003 (0.0003)  loss: 1105.2249 (1132.4882)  grad_norm: 674.3769 (682.4874)  amp_scale: 1.0000 (1.0000)  time: 1.1665  data: 0.1671  max mem: 13835
[01:18:44.162750] [Train][Ep-61/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1103.2438 (1126.4060)  grad_norm: 679.1340 (683.4138)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.3185  max mem: 13835
[01:20:40.999440] [Train][Ep-61/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0003 (0.0003)  loss: 1128.9828 (1128.8012)  grad_norm: 670.5722 (679.8823)  amp_scale: 1.0000 (1.0000)  time: 1.1249  data: 0.2621  max mem: 13835
[01:22:38.758955] [Train][Ep-61/100]  [ 600/1435]  eta: 0:16:17  lr: 0.0003 (0.0003)  loss: 1157.7257 (1131.5505)  grad_norm: 676.5739 (680.6669)  amp_scale: 1.0000 (1.0000)  time: 1.2091  data: 0.1047  max mem: 13835
[01:24:40.130726] [Train][Ep-61/100]  [ 700/1435]  eta: 0:14:24  lr: 0.0003 (0.0003)  loss: 1101.3042 (1127.8022)  grad_norm: 668.1329 (680.1843)  amp_scale: 1.0000 (1.0000)  time: 1.2184  data: 0.5903  max mem: 13835
[01:26:39.264249] [Train][Ep-61/100]  [ 800/1435]  eta: 0:12:28  lr: 0.0003 (0.0003)  loss: 1109.9492 (1128.2480)  grad_norm: 691.9919 (682.5843)  amp_scale: 1.0000 (1.0000)  time: 1.1760  data: 0.4900  max mem: 13835
[01:28:42.668740] [Train][Ep-61/100]  [ 900/1435]  eta: 0:10:33  lr: 0.0003 (0.0003)  loss: 1113.0869 (1127.8763)  grad_norm: 658.7227 (680.5302)  amp_scale: 1.0000 (1.0000)  time: 1.2412  data: 0.2014  max mem: 13835
[01:30:40.629807] [Train][Ep-61/100]  [1000/1435]  eta: 0:08:35  lr: 0.0003 (0.0003)  loss: 1111.9211 (1128.5359)  grad_norm: 679.6602 (681.1189)  amp_scale: 1.0000 (1.0000)  time: 1.2043  data: 0.6325  max mem: 13835
[01:32:41.612414] [Train][Ep-61/100]  [1100/1435]  eta: 0:06:37  lr: 0.0003 (0.0003)  loss: 1086.7367 (1127.5022)  grad_norm: 670.7905 (680.2675)  amp_scale: 1.0000 (1.0000)  time: 1.1821  data: 0.0370  max mem: 13835
[01:34:44.325658] [Train][Ep-61/100]  [1200/1435]  eta: 0:04:39  lr: 0.0003 (0.0003)  loss: 1107.8143 (1126.4177)  grad_norm: 692.2354 (681.6744)  amp_scale: 1.0000 (1.0000)  time: 1.2067  data: 0.0298  max mem: 13835
[01:36:39.682096] [Train][Ep-61/100]  [1300/1435]  eta: 0:02:40  lr: 0.0003 (0.0003)  loss: 1132.4368 (1126.2208)  grad_norm: 669.4879 (681.5297)  amp_scale: 1.0000 (1.0000)  time: 1.1118  data: 0.5053  max mem: 13835
[01:38:38.240783] [Train][Ep-61/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1103.1010 (1125.1523)  grad_norm: 683.2739 (682.2953)  amp_scale: 1.0000 (1.0000)  time: 1.2185  data: 0.0550  max mem: 13835
[01:39:16.782545] [Train][Ep-61/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1116.8125 (1125.9831)  grad_norm: 683.2739 (682.5439)  amp_scale: 1.0000 (1.0000)  time: 1.1380  data: 0.1861  max mem: 13835
[01:39:16.783495] [Train][Ep-61/100] Total time: 0:28:21 (1.1857 s / it)
[01:39:16.783909] Syncing meters...
[01:39:16.846804] Averaged stats: lr: 0.0003 (0.0003)  loss: 1116.8125 (1122.5772)  grad_norm: 683.2739 (682.5439)  amp_scale: 1.0000 (1.0000)
[01:39:27.725498] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 62)
[01:39:29.965107] [Train][Ep-62/100]  [   0/1435]  eta: 0:53:21  lr: 0.0003 (0.0003)  time: 2.2310  data: 1.7483  max mem: 13835
[01:41:26.449512] [Train][Ep-62/100]  [ 100/1435]  eta: 0:26:09  lr: 0.0003 (0.0003)  loss: 1084.2795 (1094.1282)  grad_norm: 665.6312 (669.8192)  amp_scale: 1.0000 (1.0000)  time: 1.1290  data: 0.5486  max mem: 13835
[01:43:22.948591] [Train][Ep-62/100]  [ 200/1435]  eta: 0:24:05  lr: 0.0003 (0.0003)  loss: 1113.1803 (1101.5155)  grad_norm: 673.4159 (674.8770)  amp_scale: 1.0000 (1.0000)  time: 1.0464  data: 0.4097  max mem: 13835
[01:45:17.282870] [Train][Ep-62/100]  [ 300/1435]  eta: 0:21:58  lr: 0.0003 (0.0003)  loss: 1087.4833 (1098.4277)  grad_norm: 683.3630 (678.7905)  amp_scale: 1.0000 (1.0000)  time: 1.1656  data: 0.3198  max mem: 13835
[01:47:12.218964] [Train][Ep-62/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0003 (0.0003)  loss: 1130.2765 (1103.6864)  grad_norm: 672.6752 (680.2137)  amp_scale: 1.0000 (1.0000)  time: 1.1103  data: 0.3664  max mem: 13835
[01:49:13.542805] [Train][Ep-62/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0003 (0.0003)  loss: 1100.8850 (1102.6706)  grad_norm: 697.6655 (683.3400)  amp_scale: 1.0000 (1.0000)  time: 1.1349  data: 0.3093  max mem: 13835
[01:51:06.610284] [Train][Ep-62/100]  [ 600/1435]  eta: 0:16:10  lr: 0.0003 (0.0003)  loss: 1107.1379 (1106.9146)  grad_norm: 696.6996 (686.1558)  amp_scale: 1.0000 (1.0000)  time: 1.1346  data: 0.5605  max mem: 13835
[01:53:07.262616] [Train][Ep-62/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0003 (0.0003)  loss: 1104.1501 (1106.5059)  grad_norm: 672.9807 (684.1393)  amp_scale: 1.0000 (1.0000)  time: 1.2382  data: 0.2465  max mem: 13835
[01:55:07.558483] [Train][Ep-62/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0003 (0.0003)  loss: 1094.3663 (1106.6942)  grad_norm: 663.7806 (682.8447)  amp_scale: 1.0000 (1.0000)  time: 1.1732  data: 0.0609  max mem: 13835
[01:57:06.780322] [Train][Ep-62/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0003 (0.0003)  loss: 1104.5276 (1107.3638)  grad_norm: 689.1499 (684.5376)  amp_scale: 1.0000 (1.0000)  time: 1.2481  data: 0.0224  max mem: 13835
[01:59:08.124566] [Train][Ep-62/100]  [1000/1435]  eta: 0:08:32  lr: 0.0003 (0.0003)  loss: 1095.9692 (1108.2892)  grad_norm: 665.7405 (683.7555)  amp_scale: 1.0000 (1.0000)  time: 1.2215  data: 0.2051  max mem: 13835
[02:01:00.084167] [Train][Ep-62/100]  [1100/1435]  eta: 0:06:33  lr: 0.0003 (0.0003)  loss: 1088.6865 (1108.0730)  grad_norm: 698.0090 (684.7496)  amp_scale: 1.0000 (1.0000)  time: 1.0784  data: 0.4437  max mem: 13835
[02:02:56.246050] [Train][Ep-62/100]  [1200/1435]  eta: 0:04:35  lr: 0.0003 (0.0003)  loss: 1079.3174 (1107.5765)  grad_norm: 675.9305 (684.1154)  amp_scale: 1.0000 (1.0000)  time: 1.1326  data: 0.1971  max mem: 13835
[02:04:55.685861] [Train][Ep-62/100]  [1300/1435]  eta: 0:02:38  lr: 0.0003 (0.0003)  loss: 1111.2739 (1107.8020)  grad_norm: 659.0616 (683.3970)  amp_scale: 1.0000 (1.0000)  time: 1.2600  data: 0.2093  max mem: 13835
[02:06:50.586903] [Train][Ep-62/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1122.8087 (1109.5764)  grad_norm: 697.5992 (684.3106)  amp_scale: 1.0000 (1.0000)  time: 1.1383  data: 0.4506  max mem: 13835
[02:07:29.785279] [Train][Ep-62/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1097.4268 (1109.2787)  grad_norm: 701.2114 (684.6932)  amp_scale: 1.0000 (1.0000)  time: 1.2113  data: 0.6254  max mem: 13835
[02:07:29.786236] [Train][Ep-62/100] Total time: 0:28:02 (1.1722 s / it)
[02:07:29.786791] Syncing meters...
[02:07:29.804258] Averaged stats: lr: 0.0003 (0.0003)  loss: 1097.4268 (1115.7999)  grad_norm: 701.2114 (684.6932)  amp_scale: 1.0000 (1.0000)
[02:07:38.290543] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 63)
[02:07:40.168811] [Train][Ep-63/100]  [   0/1435]  eta: 0:44:43  lr: 0.0003 (0.0003)  time: 1.8702  data: 1.3881  max mem: 13835
[02:09:35.406777] [Train][Ep-63/100]  [ 100/1435]  eta: 0:25:47  lr: 0.0003 (0.0003)  loss: 1127.2312 (1122.3156)  grad_norm: 683.0700 (691.6490)  amp_scale: 1.0000 (1.0000)  time: 1.1482  data: 0.5233  max mem: 13835
[02:11:33.862684] [Train][Ep-63/100]  [ 200/1435]  eta: 0:24:07  lr: 0.0003 (0.0003)  loss: 1077.9928 (1100.7412)  grad_norm: 655.8652 (676.3162)  amp_scale: 1.0000 (1.0000)  time: 1.1245  data: 0.1261  max mem: 13835
[02:13:28.451891] [Train][Ep-63/100]  [ 300/1435]  eta: 0:22:00  lr: 0.0003 (0.0003)  loss: 1058.6890 (1095.1634)  grad_norm: 696.1917 (681.2079)  amp_scale: 1.0000 (1.0000)  time: 1.1178  data: 0.2428  max mem: 13835
[02:15:22.115512] [Train][Ep-63/100]  [ 400/1435]  eta: 0:19:57  lr: 0.0003 (0.0003)  loss: 1099.1813 (1100.1712)  grad_norm: 655.0016 (680.3980)  amp_scale: 1.0000 (1.0000)  time: 1.1505  data: 0.2822  max mem: 13835
[02:17:19.936517] [Train][Ep-63/100]  [ 500/1435]  eta: 0:18:05  lr: 0.0003 (0.0003)  loss: 1094.4032 (1099.3642)  grad_norm: 684.9783 (682.6840)  amp_scale: 1.0000 (1.0000)  time: 1.2100  data: 0.0603  max mem: 13835
[02:19:17.572505] [Train][Ep-63/100]  [ 600/1435]  eta: 0:16:11  lr: 0.0003 (0.0003)  loss: 1113.1714 (1102.6037)  grad_norm: 678.0714 (682.9693)  amp_scale: 1.0000 (1.0000)  time: 1.1779  data: 0.2084  max mem: 13835
[02:21:14.081109] [Train][Ep-63/100]  [ 700/1435]  eta: 0:14:15  lr: 0.0003 (0.0003)  loss: 1082.6757 (1102.8616)  grad_norm: 683.0816 (683.1301)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.0285  max mem: 13835
[02:23:13.068540] [Train][Ep-63/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0003 (0.0003)  loss: 1097.2981 (1105.2892)  grad_norm: 661.6086 (682.3084)  amp_scale: 1.0000 (1.0000)  time: 1.1497  data: 0.2267  max mem: 13835
[02:25:07.848270] [Train][Ep-63/100]  [ 900/1435]  eta: 0:10:23  lr: 0.0003 (0.0003)  loss: 1078.7463 (1103.5108)  grad_norm: 699.9334 (684.3723)  amp_scale: 1.0000 (1.0000)  time: 1.0813  data: 0.2796  max mem: 13835
[02:27:04.804756] [Train][Ep-63/100]  [1000/1435]  eta: 0:08:26  lr: 0.0003 (0.0003)  loss: 1106.3481 (1105.3922)  grad_norm: 677.9419 (685.0056)  amp_scale: 1.0000 (1.0000)  time: 1.2175  data: 0.0114  max mem: 13835
[02:29:01.567675] [Train][Ep-63/100]  [1100/1435]  eta: 0:06:30  lr: 0.0003 (0.0003)  loss: 1071.8944 (1104.5392)  grad_norm: 668.4068 (684.4278)  amp_scale: 1.0000 (1.0000)  time: 1.2102  data: 0.2216  max mem: 13835
[02:30:57.050158] [Train][Ep-63/100]  [1200/1435]  eta: 0:04:33  lr: 0.0003 (0.0003)  loss: 1078.4426 (1104.4145)  grad_norm: 666.8069 (684.0612)  amp_scale: 1.0000 (1.0000)  time: 1.1480  data: 0.0136  max mem: 13835
[02:32:58.733987] [Train][Ep-63/100]  [1300/1435]  eta: 0:02:37  lr: 0.0003 (0.0003)  loss: 1102.5288 (1105.9231)  grad_norm: 678.6703 (684.1075)  amp_scale: 1.0000 (1.0000)  time: 1.2580  data: 0.6899  max mem: 13835
[02:34:56.630633] [Train][Ep-63/100]  [1400/1435]  eta: 0:00:40  lr: 0.0003 (0.0003)  loss: 1136.2181 (1107.2422)  grad_norm: 682.5721 (684.0297)  amp_scale: 1.0000 (1.0000)  time: 1.1664  data: 0.1718  max mem: 13835
[02:35:33.060757] [Train][Ep-63/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1122.5685 (1107.6587)  grad_norm: 678.9230 (683.6287)  amp_scale: 1.0000 (1.0000)  time: 1.0457  data: 0.1294  max mem: 13835
[02:35:33.061595] [Train][Ep-63/100] Total time: 0:27:54 (1.1671 s / it)
[02:35:33.062102] Syncing meters...
[02:35:33.928611] Averaged stats: lr: 0.0003 (0.0003)  loss: 1122.5685 (1109.8081)  grad_norm: 678.9230 (683.6287)  amp_scale: 1.0000 (1.0000)
[02:35:43.916426] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 64)
[02:35:45.821955] [Train][Ep-64/100]  [   0/1435]  eta: 0:45:22  lr: 0.0003 (0.0003)  time: 1.8973  data: 1.4142  max mem: 13835
[02:37:40.921010] [Train][Ep-64/100]  [ 100/1435]  eta: 0:25:46  lr: 0.0003 (0.0003)  loss: 1101.4612 (1101.4630)  grad_norm: 677.9417 (677.5683)  amp_scale: 1.0000 (1.0000)  time: 1.1490  data: 0.5851  max mem: 13835
[02:39:32.509352] [Train][Ep-64/100]  [ 200/1435]  eta: 0:23:24  lr: 0.0003 (0.0003)  loss: 1092.9055 (1104.5740)  grad_norm: 675.7102 (679.5866)  amp_scale: 1.0000 (1.0000)  time: 1.0717  data: 0.3280  max mem: 13835
[02:41:28.050845] [Train][Ep-64/100]  [ 300/1435]  eta: 0:21:37  lr: 0.0003 (0.0003)  loss: 1091.0892 (1098.8463)  grad_norm: 660.9969 (676.7011)  amp_scale: 1.0000 (1.0000)  time: 1.1089  data: 0.5388  max mem: 13835
[02:43:21.756491] [Train][Ep-64/100]  [ 400/1435]  eta: 0:19:41  lr: 0.0003 (0.0003)  loss: 1101.0468 (1103.2406)  grad_norm: 672.6188 (676.6913)  amp_scale: 1.0000 (1.0000)  time: 1.1117  data: 0.4937  max mem: 13835
[02:45:20.904145] [Train][Ep-64/100]  [ 500/1435]  eta: 0:17:56  lr: 0.0002 (0.0003)  loss: 1139.0300 (1106.0712)  grad_norm: 673.8331 (678.5012)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.5993  max mem: 13835
[02:47:15.419188] [Train][Ep-64/100]  [ 600/1435]  eta: 0:16:00  lr: 0.0002 (0.0003)  loss: 1087.7965 (1105.2497)  grad_norm: 667.5818 (677.5706)  amp_scale: 1.0000 (1.0000)  time: 1.1021  data: 0.4960  max mem: 13835
[02:49:12.237838] [Train][Ep-64/100]  [ 700/1435]  eta: 0:14:07  lr: 0.0002 (0.0003)  loss: 1102.7629 (1106.1022)  grad_norm: 676.6734 (679.8174)  amp_scale: 1.0000 (1.0000)  time: 1.1630  data: 0.2784  max mem: 13835
[02:51:06.527359] [Train][Ep-64/100]  [ 800/1435]  eta: 0:12:11  lr: 0.0002 (0.0002)  loss: 1078.8160 (1104.3323)  grad_norm: 695.5401 (681.3473)  amp_scale: 1.0000 (1.0000)  time: 1.1421  data: 0.2497  max mem: 13835
[02:53:05.188611] [Train][Ep-64/100]  [ 900/1435]  eta: 0:10:18  lr: 0.0002 (0.0002)  loss: 1103.0908 (1104.7416)  grad_norm: 692.9426 (682.9203)  amp_scale: 1.0000 (1.0000)  time: 1.2658  data: 0.1346  max mem: 13835
[02:55:04.815033] [Train][Ep-64/100]  [1000/1435]  eta: 0:08:24  lr: 0.0002 (0.0002)  loss: 1100.7015 (1105.0181)  grad_norm: 685.0334 (683.4471)  amp_scale: 1.0000 (1.0000)  time: 1.1618  data: 0.0480  max mem: 13835
[02:56:58.805869] [Train][Ep-64/100]  [1100/1435]  eta: 0:06:27  lr: 0.0002 (0.0002)  loss: 1095.5891 (1105.3658)  grad_norm: 689.3782 (683.8208)  amp_scale: 1.0000 (1.0000)  time: 1.1091  data: 0.3072  max mem: 13835
[02:58:56.690827] [Train][Ep-64/100]  [1200/1435]  eta: 0:04:32  lr: 0.0002 (0.0002)  loss: 1142.5267 (1106.3974)  grad_norm: 674.4221 (683.7561)  amp_scale: 1.0000 (1.0000)  time: 1.1712  data: 0.3720  max mem: 13835
[03:00:50.917142] [Train][Ep-64/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1070.9614 (1106.2943)  grad_norm: 684.4152 (683.0946)  amp_scale: 1.0000 (1.0000)  time: 1.1037  data: 0.4954  max mem: 13835
[03:02:52.510221] [Train][Ep-64/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1112.0111 (1107.6796)  grad_norm: 678.4622 (683.8443)  amp_scale: 1.0000 (1.0000)  time: 1.2032  data: 0.0879  max mem: 13835
[03:03:29.887730] [Train][Ep-64/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1107.6964 (1107.8227)  grad_norm: 670.8315 (683.4764)  amp_scale: 1.0000 (1.0000)  time: 1.0785  data: 0.0932  max mem: 13835
[03:03:29.888718] [Train][Ep-64/100] Total time: 0:27:45 (1.1610 s / it)
[03:03:29.889221] Syncing meters...
[03:03:31.198244] Averaged stats: lr: 0.0002 (0.0002)  loss: 1107.6964 (1109.3859)  grad_norm: 670.8315 (683.4764)  amp_scale: 1.0000 (1.0000)
[03:03:39.450163] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 65)
[03:03:41.372432] [Train][Ep-65/100]  [   0/1435]  eta: 0:45:46  lr: 0.0002 (0.0002)  time: 1.9142  data: 1.4315  max mem: 13835
[03:05:36.489584] [Train][Ep-65/100]  [ 100/1435]  eta: 0:25:46  lr: 0.0002 (0.0002)  loss: 1109.2045 (1112.2017)  grad_norm: 686.3566 (681.6535)  amp_scale: 1.0000 (1.0000)  time: 1.1260  data: 0.5554  max mem: 13835
[03:07:33.845456] [Train][Ep-65/100]  [ 200/1435]  eta: 0:24:00  lr: 0.0002 (0.0002)  loss: 1094.3278 (1107.6285)  grad_norm: 683.2503 (677.6120)  amp_scale: 1.0000 (1.0000)  time: 1.0906  data: 0.5236  max mem: 13835
[03:09:32.332467] [Train][Ep-65/100]  [ 300/1435]  eta: 0:22:10  lr: 0.0002 (0.0002)  loss: 1098.2448 (1104.7645)  grad_norm: 681.9003 (681.7689)  amp_scale: 1.0000 (1.0000)  time: 1.2668  data: 0.6986  max mem: 13835
[03:11:34.463659] [Train][Ep-65/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0002 (0.0002)  loss: 1105.8829 (1108.6062)  grad_norm: 671.9269 (680.4283)  amp_scale: 1.0000 (1.0000)  time: 1.2537  data: 0.6879  max mem: 13835
[03:13:35.984690] [Train][Ep-65/100]  [ 500/1435]  eta: 0:18:33  lr: 0.0002 (0.0002)  loss: 1066.3965 (1103.4711)  grad_norm: 649.6008 (679.6896)  amp_scale: 1.0000 (1.0000)  time: 1.1932  data: 0.6276  max mem: 13835
[03:15:36.240343] [Train][Ep-65/100]  [ 600/1435]  eta: 0:16:35  lr: 0.0002 (0.0002)  loss: 1135.2014 (1103.5813)  grad_norm: 679.2510 (680.0791)  amp_scale: 1.0000 (1.0000)  time: 1.2746  data: 0.0774  max mem: 13835
[03:17:34.976355] [Train][Ep-65/100]  [ 700/1435]  eta: 0:14:35  lr: 0.0002 (0.0002)  loss: 1051.2650 (1100.1684)  grad_norm: 669.6006 (679.5123)  amp_scale: 1.0000 (1.0000)  time: 1.2424  data: 0.0510  max mem: 13835
[03:19:37.264958] [Train][Ep-65/100]  [ 800/1435]  eta: 0:12:39  lr: 0.0002 (0.0002)  loss: 1115.1053 (1101.6656)  grad_norm: 694.0386 (681.0267)  amp_scale: 1.0000 (1.0000)  time: 1.2218  data: 0.1347  max mem: 13835
[03:21:39.452038] [Train][Ep-65/100]  [ 900/1435]  eta: 0:10:41  lr: 0.0002 (0.0002)  loss: 1067.2234 (1100.7894)  grad_norm: 671.5763 (681.2359)  amp_scale: 1.0000 (1.0000)  time: 1.1116  data: 0.0668  max mem: 13835
[03:23:31.320920] [Train][Ep-65/100]  [1000/1435]  eta: 0:08:37  lr: 0.0002 (0.0002)  loss: 1073.9741 (1100.9938)  grad_norm: 682.4803 (681.8171)  amp_scale: 1.0000 (1.0000)  time: 1.0876  data: 0.2802  max mem: 13835
[03:25:33.854659] [Train][Ep-65/100]  [1100/1435]  eta: 0:06:39  lr: 0.0002 (0.0002)  loss: 1109.5656 (1101.3160)  grad_norm: 677.1648 (681.7587)  amp_scale: 1.0000 (1.0000)  time: 1.2203  data: 0.0590  max mem: 13835
[03:27:29.229120] [Train][Ep-65/100]  [1200/1435]  eta: 0:04:39  lr: 0.0002 (0.0002)  loss: 1118.5225 (1101.2514)  grad_norm: 695.6002 (682.6361)  amp_scale: 1.0000 (1.0000)  time: 1.2365  data: 0.1730  max mem: 13835
[03:29:26.321394] [Train][Ep-65/100]  [1300/1435]  eta: 0:02:40  lr: 0.0002 (0.0002)  loss: 1081.1182 (1101.8574)  grad_norm: 686.2149 (683.7898)  amp_scale: 1.0000 (1.0000)  time: 1.0920  data: 0.5161  max mem: 13835
[03:31:17.745864] [Train][Ep-65/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1095.1335 (1101.4731)  grad_norm: 693.5547 (684.3287)  amp_scale: 1.0000 (1.0000)  time: 1.0709  data: 0.2927  max mem: 13835
[03:31:53.744366] [Train][Ep-65/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1096.9014 (1101.6374)  grad_norm: 693.8436 (684.2998)  amp_scale: 1.0000 (1.0000)  time: 1.0971  data: 0.5202  max mem: 13835
[03:31:53.745287] [Train][Ep-65/100] Total time: 0:28:14 (1.1807 s / it)
[03:31:53.745773] Syncing meters...
[03:31:53.839459] Averaged stats: lr: 0.0002 (0.0002)  loss: 1096.9014 (1102.7099)  grad_norm: 693.8436 (684.2998)  amp_scale: 1.0000 (1.0000)
[03:32:03.235698] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 66)
[03:32:05.002621] [Train][Ep-66/100]  [   0/1435]  eta: 0:42:04  lr: 0.0002 (0.0002)  time: 1.7593  data: 1.2767  max mem: 13835
[03:34:02.636473] [Train][Ep-66/100]  [ 100/1435]  eta: 0:26:18  lr: 0.0002 (0.0002)  loss: 1059.9023 (1088.5504)  grad_norm: 677.4982 (680.8672)  amp_scale: 1.0000 (1.0000)  time: 1.1618  data: 0.5881  max mem: 13835
[03:36:02.437148] [Train][Ep-66/100]  [ 200/1435]  eta: 0:24:29  lr: 0.0002 (0.0002)  loss: 1106.4691 (1101.9113)  grad_norm: 679.6228 (680.0791)  amp_scale: 1.0000 (1.0000)  time: 1.1434  data: 0.4087  max mem: 13835
[03:37:56.274510] [Train][Ep-66/100]  [ 300/1435]  eta: 0:22:11  lr: 0.0002 (0.0002)  loss: 1055.1689 (1096.3337)  grad_norm: 674.2780 (682.3312)  amp_scale: 1.0000 (1.0000)  time: 1.0870  data: 0.4341  max mem: 13835
[03:39:56.955686] [Train][Ep-66/100]  [ 400/1435]  eta: 0:20:22  lr: 0.0002 (0.0002)  loss: 1128.3257 (1102.4045)  grad_norm: 686.5491 (683.9784)  amp_scale: 1.0000 (1.0000)  time: 1.2080  data: 0.0523  max mem: 13835
[03:41:48.794459] [Train][Ep-66/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0002 (0.0002)  loss: 1132.7079 (1106.6242)  grad_norm: 667.4707 (682.5054)  amp_scale: 1.0000 (1.0000)  time: 1.0460  data: 0.3443  max mem: 13835
[03:43:44.837642] [Train][Ep-66/100]  [ 600/1435]  eta: 0:16:14  lr: 0.0002 (0.0002)  loss: 1103.4751 (1103.6558)  grad_norm: 679.8754 (680.6559)  amp_scale: 1.0000 (1.0000)  time: 1.1096  data: 0.2519  max mem: 13835
[03:45:38.976249] [Train][Ep-66/100]  [ 700/1435]  eta: 0:14:15  lr: 0.0002 (0.0002)  loss: 1115.1249 (1107.4030)  grad_norm: 695.6192 (682.8137)  amp_scale: 1.0000 (1.0000)  time: 1.2167  data: 0.0485  max mem: 13835
[03:47:35.485036] [Train][Ep-66/100]  [ 800/1435]  eta: 0:12:19  lr: 0.0002 (0.0002)  loss: 1069.8309 (1103.6193)  grad_norm: 676.7949 (682.7994)  amp_scale: 1.0000 (1.0000)  time: 1.1883  data: 0.6176  max mem: 13835
[03:49:36.630653] [Train][Ep-66/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0002 (0.0002)  loss: 1135.6047 (1105.9041)  grad_norm: 667.7224 (681.7527)  amp_scale: 1.0000 (1.0000)  time: 1.1452  data: 0.3812  max mem: 13835
[03:51:31.255555] [Train][Ep-66/100]  [1000/1435]  eta: 0:08:27  lr: 0.0002 (0.0002)  loss: 1091.1635 (1106.8351)  grad_norm: 686.5084 (682.0753)  amp_scale: 1.0000 (1.0000)  time: 1.1197  data: 0.5360  max mem: 13835
[03:53:21.998376] [Train][Ep-66/100]  [1100/1435]  eta: 0:06:29  lr: 0.0002 (0.0002)  loss: 1074.7687 (1104.2630)  grad_norm: 656.1772 (680.8862)  amp_scale: 1.0000 (1.0000)  time: 1.0948  data: 0.5104  max mem: 13835
[03:55:21.939030] [Train][Ep-66/100]  [1200/1435]  eta: 0:04:33  lr: 0.0002 (0.0002)  loss: 1106.1473 (1103.3188)  grad_norm: 676.8662 (681.6322)  amp_scale: 1.0000 (1.0000)  time: 1.2120  data: 0.6345  max mem: 13835
[03:57:24.352615] [Train][Ep-66/100]  [1300/1435]  eta: 0:02:37  lr: 0.0002 (0.0002)  loss: 1100.1365 (1102.2762)  grad_norm: 685.9773 (682.4594)  amp_scale: 1.0000 (1.0000)  time: 1.2375  data: 0.6705  max mem: 13835
[03:59:24.335002] [Train][Ep-66/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1090.3954 (1101.0188)  grad_norm: 680.2221 (682.6484)  amp_scale: 1.0000 (1.0000)  time: 1.1338  data: 0.5662  max mem: 13835
[04:00:00.607997] [Train][Ep-66/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1100.5184 (1101.4309)  grad_norm: 679.2040 (682.7858)  amp_scale: 1.0000 (1.0000)  time: 1.0688  data: 0.4806  max mem: 13835
[04:00:00.608963] [Train][Ep-66/100] Total time: 0:27:57 (1.1689 s / it)
[04:00:00.609481] Syncing meters...
[04:00:00.610770] Averaged stats: lr: 0.0002 (0.0002)  loss: 1100.5184 (1101.1403)  grad_norm: 679.2040 (682.7858)  amp_scale: 1.0000 (1.0000)
[04:00:07.959417] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 67)
[04:00:10.119579] [Train][Ep-67/100]  [   0/1435]  eta: 0:51:28  lr: 0.0002 (0.0002)  time: 2.1520  data: 1.6682  max mem: 13835
[04:01:59.339705] [Train][Ep-67/100]  [ 100/1435]  eta: 0:24:32  lr: 0.0002 (0.0002)  loss: 1072.9086 (1089.5971)  grad_norm: 679.5233 (683.7018)  amp_scale: 1.0000 (1.0000)  time: 1.1711  data: 0.6054  max mem: 13835
[04:03:54.813093] [Train][Ep-67/100]  [ 200/1435]  eta: 0:23:13  lr: 0.0002 (0.0002)  loss: 1091.3195 (1093.6871)  grad_norm: 693.7512 (687.6700)  amp_scale: 1.0000 (1.0000)  time: 1.0887  data: 0.5035  max mem: 13835
[04:05:46.820314] [Train][Ep-67/100]  [ 300/1435]  eta: 0:21:17  lr: 0.0002 (0.0002)  loss: 1091.4586 (1090.9573)  grad_norm: 677.9137 (683.6184)  amp_scale: 1.0000 (1.0000)  time: 1.1612  data: 0.3458  max mem: 13835
[04:07:44.128200] [Train][Ep-67/100]  [ 400/1435]  eta: 0:19:37  lr: 0.0002 (0.0002)  loss: 1068.5688 (1088.4630)  grad_norm: 679.9637 (683.6458)  amp_scale: 1.0000 (1.0000)  time: 1.1386  data: 0.2902  max mem: 13835
[04:09:38.660061] [Train][Ep-67/100]  [ 500/1435]  eta: 0:17:45  lr: 0.0002 (0.0002)  loss: 1035.4457 (1085.1394)  grad_norm: 682.1505 (683.1420)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.0155  max mem: 13835
[04:11:34.076726] [Train][Ep-67/100]  [ 600/1435]  eta: 0:15:53  lr: 0.0002 (0.0002)  loss: 1109.9656 (1086.0761)  grad_norm: 672.2065 (681.4920)  amp_scale: 1.0000 (1.0000)  time: 1.1551  data: 0.2190  max mem: 13835
[04:13:26.467141] [Train][Ep-67/100]  [ 700/1435]  eta: 0:13:57  lr: 0.0002 (0.0002)  loss: 1107.1565 (1089.6841)  grad_norm: 681.0076 (681.4560)  amp_scale: 1.0000 (1.0000)  time: 1.1000  data: 0.4199  max mem: 13835
[04:15:21.539607] [Train][Ep-67/100]  [ 800/1435]  eta: 0:12:04  lr: 0.0002 (0.0002)  loss: 1094.4917 (1091.2521)  grad_norm: 678.8322 (681.8028)  amp_scale: 1.0000 (1.0000)  time: 1.1487  data: 0.5825  max mem: 13835
[04:17:16.033032] [Train][Ep-67/100]  [ 900/1435]  eta: 0:10:10  lr: 0.0002 (0.0002)  loss: 1113.9863 (1094.1329)  grad_norm: 685.8356 (682.0778)  amp_scale: 1.0000 (1.0000)  time: 1.1574  data: 0.4166  max mem: 13835
[04:19:12.734615] [Train][Ep-67/100]  [1000/1435]  eta: 0:08:17  lr: 0.0002 (0.0002)  loss: 1085.2278 (1092.7622)  grad_norm: 672.2594 (681.3956)  amp_scale: 1.0000 (1.0000)  time: 1.1614  data: 0.0269  max mem: 13835
[04:21:09.563797] [Train][Ep-67/100]  [1100/1435]  eta: 0:06:23  lr: 0.0002 (0.0002)  loss: 1063.1935 (1092.2390)  grad_norm: 688.0092 (681.5092)  amp_scale: 1.0000 (1.0000)  time: 1.2232  data: 0.1037  max mem: 13835
[04:23:08.701882] [Train][Ep-67/100]  [1200/1435]  eta: 0:04:30  lr: 0.0002 (0.0002)  loss: 1092.0891 (1092.5005)  grad_norm: 674.6016 (681.1327)  amp_scale: 1.0000 (1.0000)  time: 1.1774  data: 0.0176  max mem: 13835
[04:25:08.762974] [Train][Ep-67/100]  [1300/1435]  eta: 0:02:35  lr: 0.0002 (0.0002)  loss: 1115.2871 (1093.0975)  grad_norm: 692.2967 (681.7390)  amp_scale: 1.0000 (1.0000)  time: 1.2647  data: 0.0017  max mem: 13835
[04:27:08.900667] [Train][Ep-67/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1112.1119 (1094.7762)  grad_norm: 677.2224 (682.8127)  amp_scale: 1.0000 (1.0000)  time: 1.2021  data: 0.6258  max mem: 13835
[04:27:48.893250] [Train][Ep-67/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1114.9122 (1096.1748)  grad_norm: 690.6852 (683.0232)  amp_scale: 1.0000 (1.0000)  time: 1.1316  data: 0.5610  max mem: 13835
[04:27:48.894255] [Train][Ep-67/100] Total time: 0:27:40 (1.1574 s / it)
[04:27:48.894722] Syncing meters...
[04:27:48.896032] Averaged stats: lr: 0.0002 (0.0002)  loss: 1114.9122 (1095.0748)  grad_norm: 690.6852 (683.0232)  amp_scale: 1.0000 (1.0000)
[04:27:58.708128] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 68)
[04:28:00.588163] [Train][Ep-68/100]  [   0/1435]  eta: 0:44:46  lr: 0.0002 (0.0002)  time: 1.8718  data: 1.3894  max mem: 13835
[04:29:59.847526] [Train][Ep-68/100]  [ 100/1435]  eta: 0:26:41  lr: 0.0002 (0.0002)  loss: 1118.8396 (1123.9177)  grad_norm: 664.4813 (666.9680)  amp_scale: 1.0000 (1.0000)  time: 1.1949  data: 0.6249  max mem: 13835
[04:31:58.115902] [Train][Ep-68/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0002 (0.0002)  loss: 1094.7129 (1115.7209)  grad_norm: 676.0359 (670.9213)  amp_scale: 1.0000 (1.0000)  time: 1.0858  data: 0.4765  max mem: 13835
[04:33:52.020255] [Train][Ep-68/100]  [ 300/1435]  eta: 0:22:12  lr: 0.0002 (0.0002)  loss: 1067.7169 (1103.7495)  grad_norm: 674.7136 (673.3183)  amp_scale: 1.0000 (1.0000)  time: 1.1392  data: 0.2650  max mem: 13835
[04:35:43.535017] [Train][Ep-68/100]  [ 400/1435]  eta: 0:19:59  lr: 0.0002 (0.0002)  loss: 1103.2462 (1100.5581)  grad_norm: 660.9841 (670.4336)  amp_scale: 1.0000 (1.0000)  time: 1.1737  data: 0.2953  max mem: 13835
[04:37:39.467615] [Train][Ep-68/100]  [ 500/1435]  eta: 0:18:03  lr: 0.0002 (0.0002)  loss: 1059.2369 (1099.3474)  grad_norm: 658.2383 (671.8725)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.0025  max mem: 13835
[04:39:39.214092] [Train][Ep-68/100]  [ 600/1435]  eta: 0:16:13  lr: 0.0002 (0.0002)  loss: 1096.9338 (1101.9678)  grad_norm: 705.8615 (675.5618)  amp_scale: 1.0000 (1.0000)  time: 1.1743  data: 0.0103  max mem: 13835
[04:41:33.961869] [Train][Ep-68/100]  [ 700/1435]  eta: 0:14:14  lr: 0.0002 (0.0002)  loss: 1064.2201 (1099.0156)  grad_norm: 658.5023 (676.1493)  amp_scale: 1.0000 (1.0000)  time: 1.1228  data: 0.2131  max mem: 13835
[04:43:28.748692] [Train][Ep-68/100]  [ 800/1435]  eta: 0:12:17  lr: 0.0002 (0.0002)  loss: 1061.2629 (1097.0735)  grad_norm: 685.6292 (678.0590)  amp_scale: 1.0000 (1.0000)  time: 1.1159  data: 0.1935  max mem: 13835
[04:45:27.960559] [Train][Ep-68/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0002 (0.0002)  loss: 1050.7628 (1095.4460)  grad_norm: 667.0256 (678.4898)  amp_scale: 1.0000 (1.0000)  time: 1.1811  data: 0.6104  max mem: 13835
[04:47:22.522939] [Train][Ep-68/100]  [1000/1435]  eta: 0:08:25  lr: 0.0002 (0.0002)  loss: 1090.8350 (1095.2872)  grad_norm: 681.9797 (679.3792)  amp_scale: 1.0000 (1.0000)  time: 1.0833  data: 0.5020  max mem: 13835
[04:49:15.642420] [Train][Ep-68/100]  [1100/1435]  eta: 0:06:28  lr: 0.0002 (0.0002)  loss: 1093.2507 (1097.7013)  grad_norm: 682.4905 (680.2599)  amp_scale: 1.0000 (1.0000)  time: 1.1641  data: 0.1162  max mem: 13835
[04:51:08.454428] [Train][Ep-68/100]  [1200/1435]  eta: 0:04:31  lr: 0.0002 (0.0002)  loss: 1070.4630 (1097.0970)  grad_norm: 675.8522 (680.5836)  amp_scale: 1.0000 (1.0000)  time: 1.1021  data: 0.3348  max mem: 13835
[04:53:06.287421] [Train][Ep-68/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1115.9135 (1100.1893)  grad_norm: 684.3597 (681.1529)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.0577  max mem: 13835
[04:55:08.565539] [Train][Ep-68/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1106.8478 (1100.8027)  grad_norm: 693.8120 (682.2590)  amp_scale: 1.0000 (1.0000)  time: 1.2477  data: 0.0009  max mem: 13835
[04:55:46.040208] [Train][Ep-68/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1094.8826 (1100.0979)  grad_norm: 685.0228 (682.3862)  amp_scale: 1.0000 (1.0000)  time: 1.0973  data: 0.2173  max mem: 13835
[04:55:46.041171] [Train][Ep-68/100] Total time: 0:27:47 (1.1619 s / it)
[04:55:46.041639] Syncing meters...
[04:55:46.514610] Averaged stats: lr: 0.0002 (0.0002)  loss: 1094.8826 (1096.3728)  grad_norm: 685.0228 (682.3862)  amp_scale: 1.0000 (1.0000)
[04:55:54.992223] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 69)
[04:55:56.962696] [Train][Ep-69/100]  [   0/1435]  eta: 0:46:56  lr: 0.0002 (0.0002)  time: 1.9625  data: 1.4794  max mem: 13835
[04:57:57.179943] [Train][Ep-69/100]  [ 100/1435]  eta: 0:26:54  lr: 0.0002 (0.0002)  loss: 1088.0563 (1083.7626)  grad_norm: 668.4505 (676.5431)  amp_scale: 1.0000 (1.0000)  time: 1.1793  data: 0.6011  max mem: 13835
[04:59:53.620648] [Train][Ep-69/100]  [ 200/1435]  eta: 0:24:26  lr: 0.0002 (0.0002)  loss: 1140.6396 (1095.9250)  grad_norm: 674.6815 (678.5481)  amp_scale: 1.0000 (1.0000)  time: 1.1524  data: 0.5799  max mem: 13835
[05:01:47.645807] [Train][Ep-69/100]  [ 300/1435]  eta: 0:22:09  lr: 0.0002 (0.0002)  loss: 1094.9923 (1102.7425)  grad_norm: 685.6246 (681.7914)  amp_scale: 1.0000 (1.0000)  time: 1.0958  data: 0.5018  max mem: 13835
[05:03:50.439480] [Train][Ep-69/100]  [ 400/1435]  eta: 0:20:27  lr: 0.0002 (0.0002)  loss: 1102.9038 (1104.1252)  grad_norm: 681.5765 (681.7181)  amp_scale: 1.0000 (1.0000)  time: 1.2316  data: 0.6628  max mem: 13835
[05:05:44.633469] [Train][Ep-69/100]  [ 500/1435]  eta: 0:18:20  lr: 0.0002 (0.0002)  loss: 1064.0828 (1100.9027)  grad_norm: 679.6638 (682.8768)  amp_scale: 1.0000 (1.0000)  time: 1.1006  data: 0.5361  max mem: 13835
[05:07:37.962379] [Train][Ep-69/100]  [ 600/1435]  eta: 0:16:16  lr: 0.0002 (0.0002)  loss: 1101.3223 (1102.0887)  grad_norm: 683.3104 (684.0037)  amp_scale: 1.0000 (1.0000)  time: 1.1543  data: 0.5907  max mem: 13835
[05:09:34.804972] [Train][Ep-69/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0002 (0.0002)  loss: 1062.4313 (1095.8475)  grad_norm: 662.9075 (682.8867)  amp_scale: 1.0000 (1.0000)  time: 1.1447  data: 0.5652  max mem: 13835
[05:11:31.112070] [Train][Ep-69/100]  [ 800/1435]  eta: 0:12:21  lr: 0.0002 (0.0002)  loss: 1091.9888 (1094.9104)  grad_norm: 700.2058 (683.6592)  amp_scale: 1.0000 (1.0000)  time: 1.1530  data: 0.5784  max mem: 13835
[05:13:28.517473] [Train][Ep-69/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0002 (0.0002)  loss: 1067.5176 (1094.9314)  grad_norm: 675.7617 (682.7677)  amp_scale: 1.0000 (1.0000)  time: 1.1463  data: 0.5510  max mem: 13835
[05:15:25.694597] [Train][Ep-69/100]  [1000/1435]  eta: 0:08:28  lr: 0.0002 (0.0002)  loss: 1073.6025 (1092.6664)  grad_norm: 693.6541 (683.8241)  amp_scale: 1.0000 (1.0000)  time: 1.2106  data: 0.3857  max mem: 13835
[05:17:29.315490] [Train][Ep-69/100]  [1100/1435]  eta: 0:06:33  lr: 0.0002 (0.0002)  loss: 1034.1240 (1089.7900)  grad_norm: 667.7916 (683.5234)  amp_scale: 1.0000 (1.0000)  time: 1.2352  data: 0.0080  max mem: 13835
[05:19:30.642828] [Train][Ep-69/100]  [1200/1435]  eta: 0:04:36  lr: 0.0002 (0.0002)  loss: 1063.9962 (1091.4438)  grad_norm: 683.3068 (683.3875)  amp_scale: 1.0000 (1.0000)  time: 1.1385  data: 0.1562  max mem: 13835
[05:21:32.409802] [Train][Ep-69/100]  [1300/1435]  eta: 0:02:39  lr: 0.0002 (0.0002)  loss: 1064.0736 (1091.2101)  grad_norm: 684.0034 (683.2094)  amp_scale: 1.0000 (1.0000)  time: 1.1992  data: 0.0675  max mem: 13835
[05:23:26.020306] [Train][Ep-69/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1062.8790 (1090.4943)  grad_norm: 689.1555 (682.9065)  amp_scale: 1.0000 (1.0000)  time: 1.2271  data: 0.1091  max mem: 13835
[05:24:05.757155] [Train][Ep-69/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1071.6003 (1090.0303)  grad_norm: 691.9009 (683.0271)  amp_scale: 1.0000 (1.0000)  time: 1.1783  data: 0.0967  max mem: 13835
[05:24:05.758051] [Train][Ep-69/100] Total time: 0:28:10 (1.1782 s / it)
[05:24:05.758557] Syncing meters...
[05:24:06.340165] Averaged stats: lr: 0.0002 (0.0002)  loss: 1071.6003 (1089.3376)  grad_norm: 691.9009 (683.0271)  amp_scale: 1.0000 (1.0000)
[05:24:14.068963] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 70)
[05:24:16.060479] [Train][Ep-70/100]  [   0/1435]  eta: 0:47:26  lr: 0.0002 (0.0002)  time: 1.9835  data: 1.4995  max mem: 13835
[05:26:10.130040] [Train][Ep-70/100]  [ 100/1435]  eta: 0:25:33  lr: 0.0002 (0.0002)  loss: 1077.0544 (1095.9544)  grad_norm: 671.8488 (667.7545)  amp_scale: 1.0000 (1.0000)  time: 1.1434  data: 0.5731  max mem: 13835
[05:28:06.691056] [Train][Ep-70/100]  [ 200/1435]  eta: 0:23:49  lr: 0.0002 (0.0002)  loss: 1078.4296 (1095.2708)  grad_norm: 694.0197 (678.4237)  amp_scale: 1.0000 (1.0000)  time: 1.1057  data: 0.5035  max mem: 13835
[05:30:00.988806] [Train][Ep-70/100]  [ 300/1435]  eta: 0:21:47  lr: 0.0002 (0.0002)  loss: 1063.5306 (1094.0348)  grad_norm: 676.0362 (677.4067)  amp_scale: 1.0000 (1.0000)  time: 1.1202  data: 0.5485  max mem: 13835
[05:31:55.355723] [Train][Ep-70/100]  [ 400/1435]  eta: 0:19:49  lr: 0.0002 (0.0002)  loss: 1050.3354 (1087.3914)  grad_norm: 673.7719 (677.9641)  amp_scale: 1.0000 (1.0000)  time: 1.1542  data: 0.4982  max mem: 13835
[05:33:47.952510] [Train][Ep-70/100]  [ 500/1435]  eta: 0:17:50  lr: 0.0002 (0.0002)  loss: 1048.6636 (1084.9258)  grad_norm: 673.4979 (679.7681)  amp_scale: 1.0000 (1.0000)  time: 1.1654  data: 0.2250  max mem: 13835
[05:35:45.814917] [Train][Ep-70/100]  [ 600/1435]  eta: 0:16:00  lr: 0.0002 (0.0002)  loss: 1094.9518 (1087.1686)  grad_norm: 673.3486 (680.8437)  amp_scale: 1.0000 (1.0000)  time: 1.1291  data: 0.1834  max mem: 13835
[05:37:38.440442] [Train][Ep-70/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0002 (0.0002)  loss: 1062.3574 (1086.4592)  grad_norm: 650.4150 (679.3560)  amp_scale: 1.0000 (1.0000)  time: 1.1219  data: 0.5376  max mem: 13835
[05:39:28.968597] [Train][Ep-70/100]  [ 800/1435]  eta: 0:12:04  lr: 0.0002 (0.0002)  loss: 1061.5071 (1084.9545)  grad_norm: 670.7518 (679.7362)  amp_scale: 1.0000 (1.0000)  time: 1.1504  data: 0.3467  max mem: 13835
[05:41:28.836577] [Train][Ep-70/100]  [ 900/1435]  eta: 0:10:14  lr: 0.0002 (0.0002)  loss: 1113.2363 (1088.2089)  grad_norm: 685.5421 (680.9038)  amp_scale: 1.0000 (1.0000)  time: 1.1704  data: 0.2779  max mem: 13835
[05:43:28.090495] [Train][Ep-70/100]  [1000/1435]  eta: 0:08:21  lr: 0.0002 (0.0002)  loss: 1062.1868 (1086.8267)  grad_norm: 687.3784 (682.2901)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.1289  max mem: 13835
[05:45:31.312361] [Train][Ep-70/100]  [1100/1435]  eta: 0:06:28  lr: 0.0002 (0.0002)  loss: 1058.9504 (1086.5306)  grad_norm: 675.0972 (682.2914)  amp_scale: 1.0000 (1.0000)  time: 1.2190  data: 0.0521  max mem: 13835
[05:47:22.277051] [Train][Ep-70/100]  [1200/1435]  eta: 0:04:31  lr: 0.0002 (0.0002)  loss: 1089.7610 (1086.2633)  grad_norm: 674.2456 (682.1326)  amp_scale: 1.0000 (1.0000)  time: 1.1223  data: 0.3840  max mem: 13835
[05:49:15.350098] [Train][Ep-70/100]  [1300/1435]  eta: 0:02:35  lr: 0.0002 (0.0002)  loss: 1083.3414 (1086.5651)  grad_norm: 688.5170 (682.4044)  amp_scale: 1.0000 (1.0000)  time: 1.1676  data: 0.5978  max mem: 13835
[05:51:15.550845] [Train][Ep-70/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1057.8230 (1084.5921)  grad_norm: 677.6196 (682.2818)  amp_scale: 1.0000 (1.0000)  time: 1.2422  data: 0.6636  max mem: 13835
[05:51:54.354083] [Train][Ep-70/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1061.4662 (1084.1672)  grad_norm: 675.6593 (682.8242)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.5580  max mem: 13835
[05:51:54.355039] [Train][Ep-70/100] Total time: 0:27:40 (1.1570 s / it)
[05:51:54.355453] Syncing meters...
[05:51:54.612344] Averaged stats: lr: 0.0002 (0.0002)  loss: 1061.4662 (1089.7343)  grad_norm: 675.6593 (682.8242)  amp_scale: 1.0000 (1.0000)
[05:51:56.636522] [Eval][Ep-70/100]  [  0/121]  eta: 0:04:04  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0166  data: 1.8567  max mem: 13835
[05:53:38.527229] [Eval][Ep-70/100]  [100/121]  eta: 0:00:21  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9238  data: 0.7640  max mem: 13835
[05:53:56.993805] [Eval][Ep-70/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9231  data: 0.7658  max mem: 13835
[05:53:56.994693] [Eval][Ep-70/100] Total time: 0:02:02 (1.0114 s / it)
[05:53:57.325800] [Eval][Ep-70/100] val_acc1_image=27.19 | val_acc1_audio=41.62 | val_acc1_fusion=38.55 | val_acc1_all=51.99
[05:54:07.017838] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 71)
[05:54:09.023548] [Train][Ep-71/100]  [   0/1435]  eta: 0:47:46  lr: 0.0002 (0.0002)  time: 1.9977  data: 1.5186  max mem: 13835
[05:56:05.064679] [Train][Ep-71/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0002 (0.0002)  loss: 1049.5055 (1076.1068)  grad_norm: 657.9664 (660.5444)  amp_scale: 1.0000 (1.0000)  time: 1.1583  data: 0.3554  max mem: 13835
[05:57:59.296149] [Train][Ep-71/100]  [ 200/1435]  eta: 0:23:47  lr: 0.0002 (0.0002)  loss: 1063.7928 (1074.7675)  grad_norm: 669.9023 (671.9922)  amp_scale: 1.0000 (1.0000)  time: 1.1318  data: 0.3201  max mem: 13835
[05:59:58.793380] [Train][Ep-71/100]  [ 300/1435]  eta: 0:22:06  lr: 0.0002 (0.0002)  loss: 1076.3844 (1078.6467)  grad_norm: 686.5472 (677.7008)  amp_scale: 1.0000 (1.0000)  time: 1.2286  data: 0.0246  max mem: 13835
[06:01:56.997083] [Train][Ep-71/100]  [ 400/1435]  eta: 0:20:12  lr: 0.0002 (0.0002)  loss: 1090.8867 (1078.4463)  grad_norm: 681.7861 (682.2487)  amp_scale: 1.0000 (1.0000)  time: 1.1641  data: 0.3640  max mem: 13835
[06:03:56.721713] [Train][Ep-71/100]  [ 500/1435]  eta: 0:18:20  lr: 0.0002 (0.0002)  loss: 1085.5065 (1080.7822)  grad_norm: 675.7876 (685.3953)  amp_scale: 1.0000 (1.0000)  time: 1.2339  data: 0.2194  max mem: 13835
[06:05:56.245422] [Train][Ep-71/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0002 (0.0002)  loss: 1087.5605 (1083.5597)  grad_norm: 676.3846 (683.8122)  amp_scale: 1.0000 (1.0000)  time: 1.1686  data: 0.5763  max mem: 13835
[06:07:56.833968] [Train][Ep-71/100]  [ 700/1435]  eta: 0:14:30  lr: 0.0002 (0.0002)  loss: 1058.7747 (1081.1020)  grad_norm: 675.9284 (684.5143)  amp_scale: 1.0000 (1.0000)  time: 1.2012  data: 0.6353  max mem: 13835
[06:09:47.911617] [Train][Ep-71/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0002 (0.0002)  loss: 1097.4834 (1080.5155)  grad_norm: 669.9116 (683.5954)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.5955  max mem: 13835
[06:11:47.022797] [Train][Ep-71/100]  [ 900/1435]  eta: 0:10:29  lr: 0.0002 (0.0002)  loss: 1065.9624 (1080.6687)  grad_norm: 688.4822 (684.6758)  amp_scale: 1.0000 (1.0000)  time: 1.2463  data: 0.6787  max mem: 13835
[06:13:44.007093] [Train][Ep-71/100]  [1000/1435]  eta: 0:08:31  lr: 0.0002 (0.0002)  loss: 1083.8052 (1081.2939)  grad_norm: 662.1290 (683.3605)  amp_scale: 1.0000 (1.0000)  time: 1.1091  data: 0.5411  max mem: 13835
[06:15:41.123957] [Train][Ep-71/100]  [1100/1435]  eta: 0:06:33  lr: 0.0002 (0.0002)  loss: 1074.3790 (1082.2048)  grad_norm: 685.1394 (684.3331)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.3570  max mem: 13835
[06:17:42.110241] [Train][Ep-71/100]  [1200/1435]  eta: 0:04:36  lr: 0.0002 (0.0002)  loss: 1057.9147 (1081.8418)  grad_norm: 683.6889 (684.5590)  amp_scale: 1.0000 (1.0000)  time: 1.1836  data: 0.0960  max mem: 13835
[06:19:39.738579] [Train][Ep-71/100]  [1300/1435]  eta: 0:02:39  lr: 0.0002 (0.0002)  loss: 1068.5939 (1081.6155)  grad_norm: 675.9238 (684.1527)  amp_scale: 1.0000 (1.0000)  time: 1.1152  data: 0.0718  max mem: 13835
[06:21:37.225799] [Train][Ep-71/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1079.3221 (1081.6765)  grad_norm: 682.8563 (684.4303)  amp_scale: 1.0000 (1.0000)  time: 1.1883  data: 0.3881  max mem: 13835
[06:22:16.582485] [Train][Ep-71/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1079.3221 (1081.6107)  grad_norm: 684.5042 (684.7871)  amp_scale: 1.0000 (1.0000)  time: 1.1839  data: 0.1437  max mem: 13835
[06:22:16.583491] [Train][Ep-71/100] Total time: 0:28:09 (1.1774 s / it)
[06:22:16.583942] Syncing meters...
[06:22:17.538392] Averaged stats: lr: 0.0002 (0.0002)  loss: 1079.3221 (1079.7782)  grad_norm: 684.5042 (684.7871)  amp_scale: 1.0000 (1.0000)
[06:22:26.076349] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 72)
[06:22:27.859670] [Train][Ep-72/100]  [   0/1435]  eta: 0:42:27  lr: 0.0002 (0.0002)  time: 1.7750  data: 1.2931  max mem: 13835
[06:24:24.702648] [Train][Ep-72/100]  [ 100/1435]  eta: 0:26:07  lr: 0.0002 (0.0002)  loss: 1099.5682 (1078.2363)  grad_norm: 684.7309 (682.8147)  amp_scale: 1.0000 (1.0000)  time: 1.2303  data: 0.6617  max mem: 13835
[06:26:22.417067] [Train][Ep-72/100]  [ 200/1435]  eta: 0:24:11  lr: 0.0002 (0.0002)  loss: 1093.9357 (1083.0566)  grad_norm: 677.9274 (684.5760)  amp_scale: 1.0000 (1.0000)  time: 1.1702  data: 0.2697  max mem: 13835
[06:28:20.158904] [Train][Ep-72/100]  [ 300/1435]  eta: 0:22:15  lr: 0.0002 (0.0002)  loss: 1098.2128 (1087.0005)  grad_norm: 668.7209 (679.7326)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.6016  max mem: 13835
[06:30:13.212654] [Train][Ep-72/100]  [ 400/1435]  eta: 0:20:05  lr: 0.0002 (0.0002)  loss: 1091.1465 (1090.3908)  grad_norm: 675.7143 (679.9947)  amp_scale: 1.0000 (1.0000)  time: 1.1789  data: 0.0017  max mem: 13835
[06:32:08.596960] [Train][Ep-72/100]  [ 500/1435]  eta: 0:18:07  lr: 0.0002 (0.0002)  loss: 1064.5211 (1084.1288)  grad_norm: 674.7166 (678.8450)  amp_scale: 1.0000 (1.0000)  time: 1.2080  data: 0.0068  max mem: 13835
[06:34:06.495037] [Train][Ep-72/100]  [ 600/1435]  eta: 0:16:13  lr: 0.0002 (0.0002)  loss: 1038.2363 (1080.4402)  grad_norm: 671.0562 (679.1261)  amp_scale: 1.0000 (1.0000)  time: 1.1975  data: 0.0175  max mem: 13835
[06:36:03.431902] [Train][Ep-72/100]  [ 700/1435]  eta: 0:14:16  lr: 0.0002 (0.0002)  loss: 1070.2496 (1080.1065)  grad_norm: 672.2578 (681.3466)  amp_scale: 1.0000 (1.0000)  time: 1.0806  data: 0.2588  max mem: 13835
[06:37:56.154549] [Train][Ep-72/100]  [ 800/1435]  eta: 0:12:17  lr: 0.0002 (0.0002)  loss: 1064.1281 (1080.4272)  grad_norm: 686.0176 (681.6654)  amp_scale: 1.0000 (1.0000)  time: 1.1156  data: 0.0380  max mem: 13835
[06:39:49.489128] [Train][Ep-72/100]  [ 900/1435]  eta: 0:10:19  lr: 0.0002 (0.0002)  loss: 1083.7965 (1083.7009)  grad_norm: 675.9179 (682.2736)  amp_scale: 1.0000 (1.0000)  time: 1.1895  data: 0.1988  max mem: 13835
[06:41:46.205412] [Train][Ep-72/100]  [1000/1435]  eta: 0:08:24  lr: 0.0002 (0.0002)  loss: 1127.9565 (1086.3857)  grad_norm: 684.3365 (683.6791)  amp_scale: 1.0000 (1.0000)  time: 1.1533  data: 0.2066  max mem: 13835
[06:43:45.600161] [Train][Ep-72/100]  [1100/1435]  eta: 0:06:29  lr: 0.0002 (0.0002)  loss: 1089.1227 (1084.7494)  grad_norm: 675.3263 (682.7650)  amp_scale: 1.0000 (1.0000)  time: 1.1590  data: 0.0446  max mem: 13835
[06:45:45.039732] [Train][Ep-72/100]  [1200/1435]  eta: 0:04:33  lr: 0.0002 (0.0002)  loss: 1075.9695 (1083.8604)  grad_norm: 676.4095 (683.2046)  amp_scale: 1.0000 (1.0000)  time: 1.1738  data: 0.1229  max mem: 13835
[06:47:37.477582] [Train][Ep-72/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1082.3722 (1084.9945)  grad_norm: 693.1663 (684.7067)  amp_scale: 1.0000 (1.0000)  time: 1.2048  data: 0.5089  max mem: 13835
[06:49:32.459463] [Train][Ep-72/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1058.9021 (1082.9411)  grad_norm: 674.3469 (684.9082)  amp_scale: 1.0000 (1.0000)  time: 1.1748  data: 0.5991  max mem: 13835
[06:50:10.423420] [Train][Ep-72/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1056.8019 (1082.0343)  grad_norm: 681.5046 (685.4849)  amp_scale: 1.0000 (1.0000)  time: 1.0657  data: 0.4958  max mem: 13835
[06:50:10.424297] [Train][Ep-72/100] Total time: 0:27:44 (1.1598 s / it)
[06:50:10.424679] Syncing meters...
[06:50:10.776943] Averaged stats: lr: 0.0002 (0.0002)  loss: 1056.8019 (1077.9958)  grad_norm: 681.5046 (685.4849)  amp_scale: 1.0000 (1.0000)
[06:50:21.021384] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 73)
[06:50:22.812085] [Train][Ep-73/100]  [   0/1435]  eta: 0:42:38  lr: 0.0002 (0.0002)  time: 1.7829  data: 1.2994  max mem: 13835
[06:52:16.079188] [Train][Ep-73/100]  [ 100/1435]  eta: 0:25:20  lr: 0.0002 (0.0002)  loss: 1086.2841 (1078.0553)  grad_norm: 694.2014 (697.3688)  amp_scale: 1.0000 (1.0000)  time: 1.1437  data: 0.1786  max mem: 13835
[06:54:08.912791] [Train][Ep-73/100]  [ 200/1435]  eta: 0:23:20  lr: 0.0002 (0.0002)  loss: 1066.6812 (1077.0850)  grad_norm: 671.4891 (685.8042)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.1280  max mem: 13835
[06:56:08.899869] [Train][Ep-73/100]  [ 300/1435]  eta: 0:21:51  lr: 0.0002 (0.0002)  loss: 1080.1315 (1081.2214)  grad_norm: 658.7127 (681.6421)  amp_scale: 1.0000 (1.0000)  time: 1.2100  data: 0.1721  max mem: 13835
[06:58:05.282531] [Train][Ep-73/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0002 (0.0002)  loss: 1086.1301 (1079.9101)  grad_norm: 683.6428 (685.2408)  amp_scale: 1.0000 (1.0000)  time: 1.1329  data: 0.3348  max mem: 13835
[07:00:01.494345] [Train][Ep-73/100]  [ 500/1435]  eta: 0:18:03  lr: 0.0002 (0.0002)  loss: 1074.5607 (1076.4949)  grad_norm: 680.4181 (685.1868)  amp_scale: 1.0000 (1.0000)  time: 1.1564  data: 0.0020  max mem: 13835
[07:01:59.979124] [Train][Ep-73/100]  [ 600/1435]  eta: 0:16:11  lr: 0.0001 (0.0002)  loss: 1062.9371 (1074.8938)  grad_norm: 683.3768 (685.4002)  amp_scale: 1.0000 (1.0000)  time: 1.3183  data: 0.7475  max mem: 13835
[07:03:58.981023] [Train][Ep-73/100]  [ 700/1435]  eta: 0:14:17  lr: 0.0001 (0.0002)  loss: 1044.1375 (1074.2994)  grad_norm: 688.1326 (685.9319)  amp_scale: 1.0000 (1.0000)  time: 1.1458  data: 0.5264  max mem: 13835
[07:05:52.263277] [Train][Ep-73/100]  [ 800/1435]  eta: 0:12:18  lr: 0.0001 (0.0002)  loss: 1082.1804 (1074.7955)  grad_norm: 682.3200 (685.5310)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.5694  max mem: 13835
[07:07:48.997685] [Train][Ep-73/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0001 (0.0002)  loss: 1080.1049 (1076.5829)  grad_norm: 667.8289 (685.4181)  amp_scale: 1.0000 (1.0000)  time: 1.1346  data: 0.5624  max mem: 13835
[07:09:43.879426] [Train][Ep-73/100]  [1000/1435]  eta: 0:08:25  lr: 0.0001 (0.0001)  loss: 1074.7957 (1079.0962)  grad_norm: 679.9933 (686.5803)  amp_scale: 1.0000 (1.0000)  time: 1.1703  data: 0.5993  max mem: 13835
[07:11:43.782237] [Train][Ep-73/100]  [1100/1435]  eta: 0:06:30  lr: 0.0001 (0.0001)  loss: 1049.2102 (1079.4067)  grad_norm: 681.1381 (686.9355)  amp_scale: 1.0000 (1.0000)  time: 1.2175  data: 0.6454  max mem: 13835
[07:13:40.031876] [Train][Ep-73/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1095.3303 (1080.1626)  grad_norm: 679.0228 (686.1661)  amp_scale: 1.0000 (1.0000)  time: 1.1847  data: 0.5039  max mem: 13835
[07:15:38.263166] [Train][Ep-73/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1085.7682 (1080.3223)  grad_norm: 699.8123 (687.0234)  amp_scale: 1.0000 (1.0000)  time: 1.1896  data: 0.6245  max mem: 13835
[07:17:38.110789] [Train][Ep-73/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1064.8607 (1079.1044)  grad_norm: 665.8484 (686.4249)  amp_scale: 1.0000 (1.0000)  time: 1.1539  data: 0.5846  max mem: 13835
[07:18:15.841963] [Train][Ep-73/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1095.8291 (1080.1046)  grad_norm: 672.7346 (686.9940)  amp_scale: 1.0000 (1.0000)  time: 1.0882  data: 0.5227  max mem: 13835
[07:18:15.842979] [Train][Ep-73/100] Total time: 0:27:54 (1.1671 s / it)
[07:18:15.843400] Syncing meters...
[07:18:15.844677] Averaged stats: lr: 0.0001 (0.0001)  loss: 1095.8291 (1075.4335)  grad_norm: 672.7346 (686.9940)  amp_scale: 1.0000 (1.0000)
[07:18:24.414808] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 74)
[07:18:26.135831] [Train][Ep-74/100]  [   0/1435]  eta: 0:40:57  lr: 0.0001 (0.0001)  time: 1.7128  data: 1.2298  max mem: 13835
[07:20:25.767766] [Train][Ep-74/100]  [ 100/1435]  eta: 0:26:43  lr: 0.0001 (0.0001)  loss: 1036.7373 (1065.9071)  grad_norm: 670.7004 (669.2362)  amp_scale: 1.0000 (1.0000)  time: 1.1857  data: 0.6133  max mem: 13835
[07:22:21.520231] [Train][Ep-74/100]  [ 200/1435]  eta: 0:24:16  lr: 0.0001 (0.0001)  loss: 1070.8716 (1073.1802)  grad_norm: 669.0848 (678.2122)  amp_scale: 1.0000 (1.0000)  time: 1.1211  data: 0.5551  max mem: 13835
[07:24:17.098055] [Train][Ep-74/100]  [ 300/1435]  eta: 0:22:09  lr: 0.0001 (0.0001)  loss: 1074.9073 (1075.8318)  grad_norm: 687.0739 (681.0485)  amp_scale: 1.0000 (1.0000)  time: 1.0713  data: 0.3896  max mem: 13835
[07:26:12.321179] [Train][Ep-74/100]  [ 400/1435]  eta: 0:20:07  lr: 0.0001 (0.0001)  loss: 1045.8702 (1074.0270)  grad_norm: 683.0305 (683.6352)  amp_scale: 1.0000 (1.0000)  time: 1.1421  data: 0.5588  max mem: 13835
[07:28:09.063102] [Train][Ep-74/100]  [ 500/1435]  eta: 0:18:11  lr: 0.0001 (0.0001)  loss: 1067.8639 (1077.0041)  grad_norm: 694.8993 (686.9467)  amp_scale: 1.0000 (1.0000)  time: 1.2185  data: 0.0762  max mem: 13835
[07:30:06.341363] [Train][Ep-74/100]  [ 600/1435]  eta: 0:16:15  lr: 0.0001 (0.0001)  loss: 1081.4321 (1077.3382)  grad_norm: 671.0911 (687.5400)  amp_scale: 1.0000 (1.0000)  time: 1.1553  data: 0.5842  max mem: 13835
[07:31:58.848243] [Train][Ep-74/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0001 (0.0001)  loss: 1061.1025 (1074.9000)  grad_norm: 694.8861 (686.7827)  amp_scale: 1.0000 (1.0000)  time: 1.1936  data: 0.2379  max mem: 13835
[07:33:49.994352] [Train][Ep-74/100]  [ 800/1435]  eta: 0:12:13  lr: 0.0001 (0.0001)  loss: 1061.0123 (1075.6069)  grad_norm: 676.8716 (689.5304)  amp_scale: 1.0000 (1.0000)  time: 1.1328  data: 0.2273  max mem: 13835
[07:35:45.821686] [Train][Ep-74/100]  [ 900/1435]  eta: 0:10:18  lr: 0.0001 (0.0001)  loss: 1103.2327 (1078.9921)  grad_norm: 682.9180 (689.0679)  amp_scale: 1.0000 (1.0000)  time: 1.0868  data: 0.1272  max mem: 13835
[07:37:45.722666] [Train][Ep-74/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1063.7031 (1077.2325)  grad_norm: 709.1299 (690.2114)  amp_scale: 1.0000 (1.0000)  time: 1.2060  data: 0.0023  max mem: 13835
[07:39:43.791936] [Train][Ep-74/100]  [1100/1435]  eta: 0:06:29  lr: 0.0001 (0.0001)  loss: 1056.3773 (1077.1293)  grad_norm: 700.1324 (690.0765)  amp_scale: 1.0000 (1.0000)  time: 1.2011  data: 0.0181  max mem: 13835
[07:41:42.728151] [Train][Ep-74/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1028.9495 (1073.3480)  grad_norm: 683.9789 (690.5498)  amp_scale: 1.0000 (1.0000)  time: 1.1714  data: 0.0575  max mem: 13835
[07:43:38.907242] [Train][Ep-74/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1051.3071 (1071.9897)  grad_norm: 692.0512 (690.7990)  amp_scale: 1.0000 (1.0000)  time: 1.1516  data: 0.0872  max mem: 13835
[07:45:35.089968] [Train][Ep-74/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1060.4261 (1071.4670)  grad_norm: 693.3913 (691.2905)  amp_scale: 1.0000 (1.0000)  time: 1.1819  data: 0.3002  max mem: 13835
[07:46:13.304373] [Train][Ep-74/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1066.5817 (1071.2485)  grad_norm: 682.1238 (690.7891)  amp_scale: 1.0000 (1.0000)  time: 1.0767  data: 0.4742  max mem: 13835
[07:46:13.305300] [Train][Ep-74/100] Total time: 0:27:48 (1.1630 s / it)
[07:46:13.305728] Syncing meters...
[07:46:13.658616] Averaged stats: lr: 0.0001 (0.0001)  loss: 1066.5817 (1073.5991)  grad_norm: 682.1238 (690.7891)  amp_scale: 1.0000 (1.0000)
[07:46:23.428499] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 75)
[07:46:25.343770] [Train][Ep-75/100]  [   0/1435]  eta: 0:45:36  lr: 0.0001 (0.0001)  time: 1.9072  data: 1.4245  max mem: 13835
[07:48:18.006159] [Train][Ep-75/100]  [ 100/1435]  eta: 0:25:14  lr: 0.0001 (0.0001)  loss: 1046.4498 (1043.9744)  grad_norm: 667.6732 (674.9066)  amp_scale: 1.0000 (1.0000)  time: 1.1319  data: 0.4651  max mem: 13835
[07:50:09.924281] [Train][Ep-75/100]  [ 200/1435]  eta: 0:23:11  lr: 0.0001 (0.0001)  loss: 1066.3706 (1053.5916)  grad_norm: 683.1339 (674.8390)  amp_scale: 1.0000 (1.0000)  time: 1.0771  data: 0.5111  max mem: 13835
[07:51:59.402728] [Train][Ep-75/100]  [ 300/1435]  eta: 0:21:06  lr: 0.0001 (0.0001)  loss: 1059.3702 (1058.0731)  grad_norm: 677.9271 (675.5331)  amp_scale: 1.0000 (1.0000)  time: 1.1213  data: 0.3104  max mem: 13835
[07:53:53.067838] [Train][Ep-75/100]  [ 400/1435]  eta: 0:19:20  lr: 0.0001 (0.0001)  loss: 1080.2322 (1063.1684)  grad_norm: 674.2017 (679.7929)  amp_scale: 1.0000 (1.0000)  time: 1.1373  data: 0.5377  max mem: 13835
[07:55:49.422396] [Train][Ep-75/100]  [ 500/1435]  eta: 0:17:36  lr: 0.0001 (0.0001)  loss: 1055.3497 (1063.8818)  grad_norm: 694.1585 (682.5491)  amp_scale: 1.0000 (1.0000)  time: 1.1579  data: 0.5918  max mem: 13835
[07:57:43.150983] [Train][Ep-75/100]  [ 600/1435]  eta: 0:15:44  lr: 0.0001 (0.0001)  loss: 1061.0547 (1063.3839)  grad_norm: 683.4650 (683.7365)  amp_scale: 1.0000 (1.0000)  time: 1.0990  data: 0.3705  max mem: 13835
[07:59:39.791817] [Train][Ep-75/100]  [ 700/1435]  eta: 0:13:54  lr: 0.0001 (0.0001)  loss: 1072.8368 (1065.4839)  grad_norm: 681.8165 (684.2436)  amp_scale: 1.0000 (1.0000)  time: 1.0899  data: 0.3565  max mem: 13835
[08:01:32.068415] [Train][Ep-75/100]  [ 800/1435]  eta: 0:12:00  lr: 0.0001 (0.0001)  loss: 1074.7268 (1066.3376)  grad_norm: 670.8585 (683.4874)  amp_scale: 1.0000 (1.0000)  time: 1.1617  data: 0.3164  max mem: 13835
[08:03:24.096207] [Train][Ep-75/100]  [ 900/1435]  eta: 0:10:06  lr: 0.0001 (0.0001)  loss: 1076.2246 (1065.8088)  grad_norm: 683.0988 (684.4614)  amp_scale: 1.0000 (1.0000)  time: 1.1901  data: 0.0025  max mem: 13835
[08:05:24.802261] [Train][Ep-75/100]  [1000/1435]  eta: 0:08:15  lr: 0.0001 (0.0001)  loss: 1120.7286 (1068.9109)  grad_norm: 684.4984 (685.6166)  amp_scale: 1.0000 (1.0000)  time: 1.2192  data: 0.0035  max mem: 13835
[08:07:19.414929] [Train][Ep-75/100]  [1100/1435]  eta: 0:06:22  lr: 0.0001 (0.0001)  loss: 1063.4381 (1069.6031)  grad_norm: 667.4498 (684.6036)  amp_scale: 1.0000 (1.0000)  time: 1.1324  data: 0.4802  max mem: 13835
[08:09:15.050265] [Train][Ep-75/100]  [1200/1435]  eta: 0:04:28  lr: 0.0001 (0.0001)  loss: 1096.8340 (1071.5775)  grad_norm: 683.6251 (685.1487)  amp_scale: 1.0000 (1.0000)  time: 1.1658  data: 0.5943  max mem: 13835
[08:11:09.857371] [Train][Ep-75/100]  [1300/1435]  eta: 0:02:34  lr: 0.0001 (0.0001)  loss: 1097.4412 (1073.7097)  grad_norm: 686.9404 (685.5008)  amp_scale: 1.0000 (1.0000)  time: 1.1724  data: 0.3911  max mem: 13835
[08:13:03.452917] [Train][Ep-75/100]  [1400/1435]  eta: 0:00:39  lr: 0.0001 (0.0001)  loss: 1117.6398 (1075.6567)  grad_norm: 682.8393 (685.1455)  amp_scale: 1.0000 (1.0000)  time: 1.1357  data: 0.3697  max mem: 13835
[08:13:41.228100] [Train][Ep-75/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1109.3944 (1076.5059)  grad_norm: 688.3077 (685.4750)  amp_scale: 1.0000 (1.0000)  time: 1.1214  data: 0.3722  max mem: 13835
[08:13:41.229008] [Train][Ep-75/100] Total time: 0:27:17 (1.1413 s / it)
[08:13:41.229646] Syncing meters...
[08:13:41.584203] Averaged stats: lr: 0.0001 (0.0001)  loss: 1109.3944 (1071.1714)  grad_norm: 688.3077 (685.4750)  amp_scale: 1.0000 (1.0000)
[08:13:49.361335] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 76)
[08:13:51.277506] [Train][Ep-76/100]  [   0/1435]  eta: 0:45:37  lr: 0.0001 (0.0001)  time: 1.9078  data: 1.4244  max mem: 13835
[08:15:41.443072] [Train][Ep-76/100]  [ 100/1435]  eta: 0:24:41  lr: 0.0001 (0.0001)  loss: 1075.5526 (1076.5925)  grad_norm: 690.6432 (692.2083)  amp_scale: 1.0000 (1.0000)  time: 1.0680  data: 0.3124  max mem: 13835
[08:17:36.728479] [Train][Ep-76/100]  [ 200/1435]  eta: 0:23:16  lr: 0.0001 (0.0001)  loss: 1036.7903 (1059.2211)  grad_norm: 673.6120 (685.7418)  amp_scale: 1.0000 (1.0000)  time: 1.1257  data: 0.0014  max mem: 13835
[08:19:31.937253] [Train][Ep-76/100]  [ 300/1435]  eta: 0:21:31  lr: 0.0001 (0.0001)  loss: 1033.3906 (1059.7455)  grad_norm: 681.8786 (684.3047)  amp_scale: 1.0000 (1.0000)  time: 1.1349  data: 0.0757  max mem: 13835
[08:21:25.102727] [Train][Ep-76/100]  [ 400/1435]  eta: 0:19:36  lr: 0.0001 (0.0001)  loss: 1063.2777 (1061.6396)  grad_norm: 690.0909 (684.8301)  amp_scale: 1.0000 (1.0000)  time: 1.1347  data: 0.0851  max mem: 13835
[08:23:24.761721] [Train][Ep-76/100]  [ 500/1435]  eta: 0:17:53  lr: 0.0001 (0.0001)  loss: 1045.0208 (1064.0031)  grad_norm: 688.0105 (685.4727)  amp_scale: 1.0000 (1.0000)  time: 1.2721  data: 0.1566  max mem: 13835
[08:25:18.088072] [Train][Ep-76/100]  [ 600/1435]  eta: 0:15:56  lr: 0.0001 (0.0001)  loss: 1047.3710 (1065.0529)  grad_norm: 693.7499 (686.4145)  amp_scale: 1.0000 (1.0000)  time: 1.1986  data: 0.0043  max mem: 13835
[08:27:17.722766] [Train][Ep-76/100]  [ 700/1435]  eta: 0:14:07  lr: 0.0001 (0.0001)  loss: 1048.8401 (1064.5290)  grad_norm: 717.8699 (688.7629)  amp_scale: 1.0000 (1.0000)  time: 1.1632  data: 0.1843  max mem: 13835
[08:29:15.012727] [Train][Ep-76/100]  [ 800/1435]  eta: 0:12:13  lr: 0.0001 (0.0001)  loss: 1048.8690 (1064.0744)  grad_norm: 686.4244 (688.2874)  amp_scale: 1.0000 (1.0000)  time: 1.1619  data: 0.3575  max mem: 13835
[08:31:09.805082] [Train][Ep-76/100]  [ 900/1435]  eta: 0:10:17  lr: 0.0001 (0.0001)  loss: 1048.7939 (1064.0709)  grad_norm: 678.8179 (688.7629)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.1906  max mem: 13835
[08:33:09.373881] [Train][Ep-76/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1059.1290 (1064.5715)  grad_norm: 673.7136 (687.6546)  amp_scale: 1.0000 (1.0000)  time: 1.1747  data: 0.0091  max mem: 13835
[08:35:05.072565] [Train][Ep-76/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1053.4165 (1065.0104)  grad_norm: 674.8752 (686.3879)  amp_scale: 1.0000 (1.0000)  time: 1.1003  data: 0.3153  max mem: 13835
[08:36:57.577650] [Train][Ep-76/100]  [1200/1435]  eta: 0:04:31  lr: 0.0001 (0.0001)  loss: 1023.2015 (1062.0724)  grad_norm: 670.8608 (685.1495)  amp_scale: 1.0000 (1.0000)  time: 1.2062  data: 0.3749  max mem: 13835
[08:38:59.715656] [Train][Ep-76/100]  [1300/1435]  eta: 0:02:36  lr: 0.0001 (0.0001)  loss: 1032.9252 (1061.0178)  grad_norm: 685.4137 (684.9195)  amp_scale: 1.0000 (1.0000)  time: 1.2326  data: 0.1523  max mem: 13835
[08:40:59.185545] [Train][Ep-76/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1077.9121 (1062.9864)  grad_norm: 670.8624 (684.4715)  amp_scale: 1.0000 (1.0000)  time: 1.2485  data: 0.0002  max mem: 13835
[08:41:37.339690] [Train][Ep-76/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1077.9121 (1062.4243)  grad_norm: 669.5952 (684.2973)  amp_scale: 1.0000 (1.0000)  time: 1.1479  data: 0.0063  max mem: 13835
[08:41:37.340599] [Train][Ep-76/100] Total time: 0:27:47 (1.1623 s / it)
[08:41:37.341022] Syncing meters...
[08:41:39.430602] Averaged stats: lr: 0.0001 (0.0001)  loss: 1077.9121 (1068.6717)  grad_norm: 669.5952 (684.2973)  amp_scale: 1.0000 (1.0000)
[08:41:47.230405] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 77)
[08:41:49.220071] [Train][Ep-77/100]  [   0/1435]  eta: 0:47:23  lr: 0.0001 (0.0001)  time: 1.9817  data: 1.4983  max mem: 13835
[08:43:41.779726] [Train][Ep-77/100]  [ 100/1435]  eta: 0:25:13  lr: 0.0001 (0.0001)  loss: 1088.3265 (1080.0224)  grad_norm: 667.3278 (686.6802)  amp_scale: 1.0000 (1.0000)  time: 1.0892  data: 0.5058  max mem: 13835
[08:45:34.383002] [Train][Ep-77/100]  [ 200/1435]  eta: 0:23:15  lr: 0.0001 (0.0001)  loss: 1062.7740 (1079.4053)  grad_norm: 673.9470 (680.1814)  amp_scale: 1.0000 (1.0000)  time: 1.1101  data: 0.4928  max mem: 13835
[08:47:28.199376] [Train][Ep-77/100]  [ 300/1435]  eta: 0:21:25  lr: 0.0001 (0.0001)  loss: 1055.8749 (1070.5655)  grad_norm: 672.4773 (677.6013)  amp_scale: 1.0000 (1.0000)  time: 1.2109  data: 0.6391  max mem: 13835
[08:49:28.730823] [Train][Ep-77/100]  [ 400/1435]  eta: 0:19:51  lr: 0.0001 (0.0001)  loss: 1068.8732 (1073.0231)  grad_norm: 668.7872 (677.7632)  amp_scale: 1.0000 (1.0000)  time: 1.2585  data: 0.6907  max mem: 13835
[08:51:30.980982] [Train][Ep-77/100]  [ 500/1435]  eta: 0:18:09  lr: 0.0001 (0.0001)  loss: 1088.0411 (1075.7451)  grad_norm: 671.5984 (677.3454)  amp_scale: 1.0000 (1.0000)  time: 1.1861  data: 0.6134  max mem: 13835
[08:53:27.049378] [Train][Ep-77/100]  [ 600/1435]  eta: 0:16:12  lr: 0.0001 (0.0001)  loss: 1044.6100 (1072.2416)  grad_norm: 690.3158 (679.9422)  amp_scale: 1.0000 (1.0000)  time: 1.1263  data: 0.5593  max mem: 13835
[08:55:16.622333] [Train][Ep-77/100]  [ 700/1435]  eta: 0:14:08  lr: 0.0001 (0.0001)  loss: 1028.0948 (1070.0445)  grad_norm: 662.4212 (679.3180)  amp_scale: 1.0000 (1.0000)  time: 1.1010  data: 0.2392  max mem: 13835
[08:57:14.036480] [Train][Ep-77/100]  [ 800/1435]  eta: 0:12:14  lr: 0.0001 (0.0001)  loss: 1052.8165 (1069.2817)  grad_norm: 682.7202 (678.6403)  amp_scale: 1.0000 (1.0000)  time: 1.2267  data: 0.0188  max mem: 13835
[08:59:15.319464] [Train][Ep-77/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0001 (0.0001)  loss: 1068.3997 (1069.6503)  grad_norm: 685.9423 (678.9809)  amp_scale: 1.0000 (1.0000)  time: 1.1817  data: 0.2957  max mem: 13835
[09:01:07.326468] [Train][Ep-77/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1055.5310 (1068.4554)  grad_norm: 685.5444 (679.9004)  amp_scale: 1.0000 (1.0000)  time: 1.1721  data: 0.1304  max mem: 13835
[09:03:04.983835] [Train][Ep-77/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1050.0424 (1068.6927)  grad_norm: 680.8638 (680.3783)  amp_scale: 1.0000 (1.0000)  time: 1.1948  data: 0.0498  max mem: 13835
[09:05:00.564697] [Train][Ep-77/100]  [1200/1435]  eta: 0:04:32  lr: 0.0001 (0.0001)  loss: 1040.0863 (1067.2801)  grad_norm: 676.2872 (680.4338)  amp_scale: 1.0000 (1.0000)  time: 1.2188  data: 0.0076  max mem: 13835
[09:07:04.646941] [Train][Ep-77/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1063.9473 (1068.5135)  grad_norm: 690.6628 (680.7903)  amp_scale: 1.0000 (1.0000)  time: 1.2578  data: 0.0698  max mem: 13835
[09:08:57.312856] [Train][Ep-77/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1042.4799 (1068.4735)  grad_norm: 671.2648 (680.5644)  amp_scale: 1.0000 (1.0000)  time: 1.0917  data: 0.1968  max mem: 13835
[09:09:34.025815] [Train][Ep-77/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1106.0312 (1069.4987)  grad_norm: 671.2648 (680.7780)  amp_scale: 1.0000 (1.0000)  time: 1.1053  data: 0.2705  max mem: 13835
[09:09:34.026645] [Train][Ep-77/100] Total time: 0:27:46 (1.1615 s / it)
[09:09:34.027097] Syncing meters...
[09:09:35.271368] Averaged stats: lr: 0.0001 (0.0001)  loss: 1106.0312 (1066.0572)  grad_norm: 671.2648 (680.7780)  amp_scale: 1.0000 (1.0000)
[09:09:45.086185] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 78)
[09:09:47.173146] [Train][Ep-78/100]  [   0/1435]  eta: 0:49:42  lr: 0.0001 (0.0001)  time: 2.0784  data: 1.5910  max mem: 13835
[09:11:47.640307] [Train][Ep-78/100]  [ 100/1435]  eta: 0:26:59  lr: 0.0001 (0.0001)  loss: 1073.5684 (1081.5237)  grad_norm: 678.2129 (680.7870)  amp_scale: 1.0000 (1.0000)  time: 1.1954  data: 0.6225  max mem: 13835
[09:13:43.307236] [Train][Ep-78/100]  [ 200/1435]  eta: 0:24:23  lr: 0.0001 (0.0001)  loss: 1045.7581 (1074.4192)  grad_norm: 671.4186 (681.7467)  amp_scale: 1.0000 (1.0000)  time: 1.2428  data: 0.1784  max mem: 13835
[09:15:42.788284] [Train][Ep-78/100]  [ 300/1435]  eta: 0:22:28  lr: 0.0001 (0.0001)  loss: 1062.1049 (1071.1116)  grad_norm: 676.7352 (681.4530)  amp_scale: 1.0000 (1.0000)  time: 1.1741  data: 0.5286  max mem: 13835
[09:17:39.758006] [Train][Ep-78/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0001 (0.0001)  loss: 1049.1520 (1067.1341)  grad_norm: 677.6065 (681.9397)  amp_scale: 1.0000 (1.0000)  time: 1.1788  data: 0.6108  max mem: 13835
[09:19:35.396510] [Train][Ep-78/100]  [ 500/1435]  eta: 0:18:21  lr: 0.0001 (0.0001)  loss: 1061.0901 (1068.3377)  grad_norm: 706.9278 (685.3450)  amp_scale: 1.0000 (1.0000)  time: 1.1725  data: 0.2974  max mem: 13835
[09:21:34.762579] [Train][Ep-78/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0001 (0.0001)  loss: 1033.9883 (1063.4679)  grad_norm: 690.1887 (685.7502)  amp_scale: 1.0000 (1.0000)  time: 1.2030  data: 0.0268  max mem: 13835
[09:23:33.686387] [Train][Ep-78/100]  [ 700/1435]  eta: 0:14:28  lr: 0.0001 (0.0001)  loss: 1070.0062 (1064.8673)  grad_norm: 676.2644 (685.8639)  amp_scale: 1.0000 (1.0000)  time: 1.1442  data: 0.3306  max mem: 13835
[09:25:31.486502] [Train][Ep-78/100]  [ 800/1435]  eta: 0:12:30  lr: 0.0001 (0.0001)  loss: 1055.8590 (1066.7918)  grad_norm: 698.7422 (687.7652)  amp_scale: 1.0000 (1.0000)  time: 1.1300  data: 0.1105  max mem: 13835
[09:27:23.909375] [Train][Ep-78/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0001 (0.0001)  loss: 1058.4919 (1067.1394)  grad_norm: 692.6091 (687.2334)  amp_scale: 1.0000 (1.0000)  time: 1.1054  data: 0.0346  max mem: 13835
[09:29:16.883554] [Train][Ep-78/100]  [1000/1435]  eta: 0:08:29  lr: 0.0001 (0.0001)  loss: 1080.2261 (1067.7624)  grad_norm: 689.1031 (687.9189)  amp_scale: 1.0000 (1.0000)  time: 1.1285  data: 0.3634  max mem: 13835
[09:31:13.377300] [Train][Ep-78/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1053.7640 (1066.8509)  grad_norm: 676.1855 (687.1998)  amp_scale: 1.0000 (1.0000)  time: 1.1427  data: 0.5750  max mem: 13835
[09:33:04.625955] [Train][Ep-78/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1004.3859 (1063.4362)  grad_norm: 681.3525 (686.8003)  amp_scale: 1.0000 (1.0000)  time: 1.1036  data: 0.5283  max mem: 13835
[09:35:03.585823] [Train][Ep-78/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1042.9982 (1063.1865)  grad_norm: 688.8528 (686.9233)  amp_scale: 1.0000 (1.0000)  time: 1.1690  data: 0.6003  max mem: 13835
[09:36:54.609409] [Train][Ep-78/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1037.4054 (1062.2467)  grad_norm: 658.2577 (686.5463)  amp_scale: 1.0000 (1.0000)  time: 1.0861  data: 0.4899  max mem: 13835
[09:37:32.714789] [Train][Ep-78/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1095.9562 (1062.9366)  grad_norm: 669.9415 (686.4729)  amp_scale: 1.0000 (1.0000)  time: 1.1134  data: 0.5469  max mem: 13835
[09:37:32.715741] [Train][Ep-78/100] Total time: 0:27:47 (1.1621 s / it)
[09:37:32.716203] Syncing meters...
[09:37:32.717470] Averaged stats: lr: 0.0001 (0.0001)  loss: 1095.9562 (1061.8564)  grad_norm: 669.9415 (686.4729)  amp_scale: 1.0000 (1.0000)
[09:37:41.390434] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 79)
[09:37:43.319330] [Train][Ep-79/100]  [   0/1435]  eta: 0:45:56  lr: 0.0001 (0.0001)  time: 1.9206  data: 1.4379  max mem: 13835
[09:39:35.412589] [Train][Ep-79/100]  [ 100/1435]  eta: 0:25:06  lr: 0.0001 (0.0001)  loss: 1075.6271 (1077.9155)  grad_norm: 691.8361 (691.5871)  amp_scale: 1.0000 (1.0000)  time: 1.1271  data: 0.5256  max mem: 13835
[09:41:30.214994] [Train][Ep-79/100]  [ 200/1435]  eta: 0:23:25  lr: 0.0001 (0.0001)  loss: 1056.9100 (1057.3663)  grad_norm: 691.3990 (688.8930)  amp_scale: 1.0000 (1.0000)  time: 1.1431  data: 0.5047  max mem: 13835
[09:43:25.893980] [Train][Ep-79/100]  [ 300/1435]  eta: 0:21:38  lr: 0.0001 (0.0001)  loss: 1066.0895 (1056.4752)  grad_norm: 668.0392 (686.2576)  amp_scale: 1.0000 (1.0000)  time: 1.1193  data: 0.5019  max mem: 13835
[09:45:16.905373] [Train][Ep-79/100]  [ 400/1435]  eta: 0:19:35  lr: 0.0001 (0.0001)  loss: 1074.5405 (1059.5953)  grad_norm: 672.1364 (684.5847)  amp_scale: 1.0000 (1.0000)  time: 1.1070  data: 0.3735  max mem: 13835
[09:47:09.781767] [Train][Ep-79/100]  [ 500/1435]  eta: 0:17:40  lr: 0.0001 (0.0001)  loss: 1036.1951 (1060.2562)  grad_norm: 685.3734 (685.9475)  amp_scale: 1.0000 (1.0000)  time: 1.0880  data: 0.4221  max mem: 13835
[09:49:05.895290] [Train][Ep-79/100]  [ 600/1435]  eta: 0:15:50  lr: 0.0001 (0.0001)  loss: 1049.6255 (1061.3403)  grad_norm: 672.3503 (685.8824)  amp_scale: 1.0000 (1.0000)  time: 1.1866  data: 0.0601  max mem: 13835
[09:51:07.755222] [Train][Ep-79/100]  [ 700/1435]  eta: 0:14:05  lr: 0.0001 (0.0001)  loss: 1007.2571 (1058.0352)  grad_norm: 683.5502 (687.2428)  amp_scale: 1.0000 (1.0000)  time: 1.2548  data: 0.0062  max mem: 13835
[09:53:02.800258] [Train][Ep-79/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0001 (0.0001)  loss: 1020.8529 (1054.1573)  grad_norm: 698.4409 (687.6023)  amp_scale: 1.0000 (1.0000)  time: 1.0797  data: 0.4584  max mem: 13835
[09:54:57.333333] [Train][Ep-79/100]  [ 900/1435]  eta: 0:10:15  lr: 0.0001 (0.0001)  loss: 1085.8020 (1055.7350)  grad_norm: 690.1385 (688.7566)  amp_scale: 1.0000 (1.0000)  time: 1.1598  data: 0.5921  max mem: 13835
[09:56:55.258431] [Train][Ep-79/100]  [1000/1435]  eta: 0:08:21  lr: 0.0001 (0.0001)  loss: 1058.5009 (1057.8414)  grad_norm: 684.8213 (688.1478)  amp_scale: 1.0000 (1.0000)  time: 1.2177  data: 0.6333  max mem: 13835
[09:58:57.100705] [Train][Ep-79/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1028.6218 (1055.5678)  grad_norm: 681.9256 (688.2843)  amp_scale: 1.0000 (1.0000)  time: 1.2079  data: 0.6314  max mem: 13835
[10:00:53.256606] [Train][Ep-79/100]  [1200/1435]  eta: 0:04:32  lr: 0.0001 (0.0001)  loss: 1062.0582 (1055.9893)  grad_norm: 689.3698 (688.5946)  amp_scale: 1.0000 (1.0000)  time: 1.1534  data: 0.5777  max mem: 13835
[10:02:46.651889] [Train][Ep-79/100]  [1300/1435]  eta: 0:02:36  lr: 0.0001 (0.0001)  loss: 1033.4692 (1056.5706)  grad_norm: 658.1625 (687.1655)  amp_scale: 1.0000 (1.0000)  time: 1.1527  data: 0.5840  max mem: 13835
[10:04:43.073004] [Train][Ep-79/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1038.5288 (1055.6458)  grad_norm: 677.1042 (687.0914)  amp_scale: 1.0000 (1.0000)  time: 1.1412  data: 0.5715  max mem: 13835
[10:05:21.973486] [Train][Ep-79/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1043.9067 (1055.3142)  grad_norm: 677.8640 (687.2157)  amp_scale: 1.0000 (1.0000)  time: 1.1148  data: 0.5488  max mem: 13835
[10:05:21.974338] [Train][Ep-79/100] Total time: 0:27:40 (1.1572 s / it)
[10:05:21.974822] Syncing meters...
[10:05:21.976139] Averaged stats: lr: 0.0001 (0.0001)  loss: 1043.9067 (1060.7140)  grad_norm: 677.8640 (687.2157)  amp_scale: 1.0000 (1.0000)
[10:05:31.624297] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 80)
[10:05:33.610398] [Train][Ep-80/100]  [   0/1435]  eta: 0:47:18  lr: 0.0001 (0.0001)  time: 1.9778  data: 1.4948  max mem: 13835
[10:07:32.570720] [Train][Ep-80/100]  [ 100/1435]  eta: 0:26:38  lr: 0.0001 (0.0001)  loss: 1013.6853 (1020.0942)  grad_norm: 676.9301 (680.9753)  amp_scale: 1.0000 (1.0000)  time: 1.2807  data: 0.7061  max mem: 13835
[10:09:28.028407] [Train][Ep-80/100]  [ 200/1435]  eta: 0:24:12  lr: 0.0001 (0.0001)  loss: 1042.6077 (1033.4379)  grad_norm: 683.8710 (678.6549)  amp_scale: 1.0000 (1.0000)  time: 1.0954  data: 0.5310  max mem: 13835
[10:11:23.852108] [Train][Ep-80/100]  [ 300/1435]  eta: 0:22:08  lr: 0.0001 (0.0001)  loss: 1032.0151 (1038.6062)  grad_norm: 694.4518 (680.4232)  amp_scale: 1.0000 (1.0000)  time: 1.2110  data: 0.6102  max mem: 13835
[10:13:22.980480] [Train][Ep-80/100]  [ 400/1435]  eta: 0:20:16  lr: 0.0001 (0.0001)  loss: 1042.3802 (1043.9232)  grad_norm: 684.9913 (683.6532)  amp_scale: 1.0000 (1.0000)  time: 1.1452  data: 0.5723  max mem: 13835
[10:15:17.760977] [Train][Ep-80/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0001 (0.0001)  loss: 1042.9098 (1047.0901)  grad_norm: 701.4368 (687.4443)  amp_scale: 1.0000 (1.0000)  time: 1.1880  data: 0.2156  max mem: 13835
[10:17:13.314663] [Train][Ep-80/100]  [ 600/1435]  eta: 0:16:14  lr: 0.0001 (0.0001)  loss: 1040.3986 (1047.8682)  grad_norm: 671.9547 (686.7670)  amp_scale: 1.0000 (1.0000)  time: 1.1779  data: 0.6108  max mem: 13835
[10:19:09.815316] [Train][Ep-80/100]  [ 700/1435]  eta: 0:14:17  lr: 0.0001 (0.0001)  loss: 1061.6506 (1051.3265)  grad_norm: 692.4233 (685.5701)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.2998  max mem: 13835
[10:21:08.984131] [Train][Ep-80/100]  [ 800/1435]  eta: 0:12:23  lr: 0.0001 (0.0001)  loss: 1059.7665 (1054.0582)  grad_norm: 674.9196 (685.4845)  amp_scale: 1.0000 (1.0000)  time: 1.1511  data: 0.3267  max mem: 13835
[10:23:05.083972] [Train][Ep-80/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0001 (0.0001)  loss: 1034.3602 (1053.1918)  grad_norm: 676.2615 (684.5965)  amp_scale: 1.0000 (1.0000)  time: 1.1285  data: 0.2698  max mem: 13835
[10:25:03.532513] [Train][Ep-80/100]  [1000/1435]  eta: 0:08:29  lr: 0.0001 (0.0001)  loss: 1055.5342 (1054.3581)  grad_norm: 693.8558 (685.5413)  amp_scale: 1.0000 (1.0000)  time: 1.3181  data: 0.7443  max mem: 13835
[10:27:08.729747] [Train][Ep-80/100]  [1100/1435]  eta: 0:06:34  lr: 0.0001 (0.0001)  loss: 1047.1354 (1055.1103)  grad_norm: 667.6718 (684.3177)  amp_scale: 1.0000 (1.0000)  time: 1.2210  data: 0.6525  max mem: 13835
[10:29:03.753567] [Train][Ep-80/100]  [1200/1435]  eta: 0:04:36  lr: 0.0001 (0.0001)  loss: 1065.5798 (1056.5909)  grad_norm: 682.6656 (684.6242)  amp_scale: 1.0000 (1.0000)  time: 1.1468  data: 0.5656  max mem: 13835
[10:31:02.695022] [Train][Ep-80/100]  [1300/1435]  eta: 0:02:38  lr: 0.0001 (0.0001)  loss: 1089.2003 (1057.9319)  grad_norm: 683.8950 (684.4241)  amp_scale: 1.0000 (1.0000)  time: 1.2099  data: 0.6401  max mem: 13835
[10:32:57.484896] [Train][Ep-80/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1033.3420 (1056.5953)  grad_norm: 674.7248 (684.3830)  amp_scale: 1.0000 (1.0000)  time: 1.1188  data: 0.5511  max mem: 13835
[10:33:34.450392] [Train][Ep-80/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1016.9595 (1056.0541)  grad_norm: 667.1282 (684.2069)  amp_scale: 1.0000 (1.0000)  time: 1.0844  data: 0.4706  max mem: 13835
[10:33:34.451314] [Train][Ep-80/100] Total time: 0:28:02 (1.1727 s / it)
[10:33:34.451701] Syncing meters...
[10:33:35.066104] Averaged stats: lr: 0.0001 (0.0001)  loss: 1016.9595 (1055.2081)  grad_norm: 667.1282 (684.2069)  amp_scale: 1.0000 (1.0000)
[10:33:37.194133] [Eval][Ep-80/100]  [  0/121]  eta: 0:04:16  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.1197  data: 1.9596  max mem: 13835
[10:35:21.348146] [Eval][Ep-80/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9488  data: 0.7889  max mem: 13835
[10:35:39.897389] [Eval][Ep-80/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9274  data: 0.7702  max mem: 13835
[10:35:39.898408] [Eval][Ep-80/100] Total time: 0:02:04 (1.0316 s / it)
[10:35:40.211528] [Eval][Ep-80/100] val_acc1_image=26.56 | val_acc1_audio=42.19 | val_acc1_fusion=38.85 | val_acc1_all=52.74
[10:35:51.654095] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 81)
[10:35:53.723633] [Train][Ep-81/100]  [   0/1435]  eta: 0:49:17  lr: 0.0001 (0.0001)  time: 2.0610  data: 1.5809  max mem: 13835
[10:37:51.202394] [Train][Ep-81/100]  [ 100/1435]  eta: 0:26:20  lr: 0.0001 (0.0001)  loss: 1027.5261 (1046.5609)  grad_norm: 673.4800 (681.4575)  amp_scale: 1.0000 (1.0000)  time: 1.2021  data: 0.3688  max mem: 13835
[10:39:44.745884] [Train][Ep-81/100]  [ 200/1435]  eta: 0:23:51  lr: 0.0001 (0.0001)  loss: 1023.4033 (1034.9870)  grad_norm: 674.6576 (680.9622)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.2711  max mem: 13835
[10:41:37.464030] [Train][Ep-81/100]  [ 300/1435]  eta: 0:21:43  lr: 0.0001 (0.0001)  loss: 1047.2469 (1041.1178)  grad_norm: 677.3405 (681.5164)  amp_scale: 1.0000 (1.0000)  time: 1.0924  data: 0.5185  max mem: 13835
[10:43:35.976830] [Train][Ep-81/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0001 (0.0001)  loss: 1056.5643 (1045.3714)  grad_norm: 677.3985 (683.8157)  amp_scale: 1.0000 (1.0000)  time: 1.1424  data: 0.5725  max mem: 13835
[10:45:29.782749] [Train][Ep-81/100]  [ 500/1435]  eta: 0:17:58  lr: 0.0001 (0.0001)  loss: 1052.8258 (1048.7640)  grad_norm: 694.3003 (685.3914)  amp_scale: 1.0000 (1.0000)  time: 1.1305  data: 0.1100  max mem: 13835
[10:47:24.234752] [Train][Ep-81/100]  [ 600/1435]  eta: 0:16:02  lr: 0.0001 (0.0001)  loss: 1095.7434 (1052.8001)  grad_norm: 676.0696 (684.0411)  amp_scale: 1.0000 (1.0000)  time: 1.2068  data: 0.1636  max mem: 13835
[10:49:25.559887] [Train][Ep-81/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0001 (0.0001)  loss: 1025.0859 (1052.2789)  grad_norm: 685.8870 (684.0933)  amp_scale: 1.0000 (1.0000)  time: 1.2254  data: 0.4842  max mem: 13835
[10:51:25.944404] [Train][Ep-81/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0001 (0.0001)  loss: 1066.2548 (1054.4824)  grad_norm: 694.9993 (685.3000)  amp_scale: 1.0000 (1.0000)  time: 1.2756  data: 0.2522  max mem: 13835
[10:53:24.297254] [Train][Ep-81/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0001 (0.0001)  loss: 1022.0170 (1052.6112)  grad_norm: 688.1995 (686.1200)  amp_scale: 1.0000 (1.0000)  time: 1.1159  data: 0.4954  max mem: 13835
[10:55:22.508553] [Train][Ep-81/100]  [1000/1435]  eta: 0:08:28  lr: 0.0001 (0.0001)  loss: 1079.3434 (1054.3538)  grad_norm: 686.4280 (686.5337)  amp_scale: 1.0000 (1.0000)  time: 1.1483  data: 0.1902  max mem: 13835
[10:57:17.936334] [Train][Ep-81/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1031.7186 (1054.1831)  grad_norm: 676.3654 (686.1713)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.2764  max mem: 13835
[10:59:13.720542] [Train][Ep-81/100]  [1200/1435]  eta: 0:04:34  lr: 0.0001 (0.0001)  loss: 1037.9371 (1054.3216)  grad_norm: 689.1494 (686.9735)  amp_scale: 1.0000 (1.0000)  time: 1.1841  data: 0.4193  max mem: 13835
[11:01:06.919837] [Train][Ep-81/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1017.3674 (1052.2882)  grad_norm: 682.2766 (687.4952)  amp_scale: 1.0000 (1.0000)  time: 1.1354  data: 0.5128  max mem: 13835
[11:03:06.304775] [Train][Ep-81/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1039.7408 (1052.5695)  grad_norm: 686.8774 (687.4911)  amp_scale: 1.0000 (1.0000)  time: 1.2113  data: 0.6350  max mem: 13835
[11:03:44.498957] [Train][Ep-81/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1067.7977 (1053.1848)  grad_norm: 686.8774 (687.4577)  amp_scale: 1.0000 (1.0000)  time: 1.1201  data: 0.5383  max mem: 13835
[11:03:44.499846] [Train][Ep-81/100] Total time: 0:27:52 (1.1657 s / it)
[11:03:44.500313] Syncing meters...
[11:03:44.501666] Averaged stats: lr: 0.0001 (0.0001)  loss: 1067.7977 (1054.4260)  grad_norm: 686.8774 (687.4577)  amp_scale: 1.0000 (1.0000)
[11:03:53.128377] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 82)
[11:03:55.436315] [Train][Ep-82/100]  [   0/1435]  eta: 0:54:59  lr: 0.0001 (0.0001)  time: 2.2993  data: 1.8156  max mem: 13835
[11:05:51.792606] [Train][Ep-82/100]  [ 100/1435]  eta: 0:26:08  lr: 0.0001 (0.0001)  loss: 1065.6921 (1079.9443)  grad_norm: 689.2148 (692.6549)  amp_scale: 1.0000 (1.0000)  time: 1.1710  data: 0.4609  max mem: 13835
[11:07:49.128672] [Train][Ep-82/100]  [ 200/1435]  eta: 0:24:09  lr: 0.0001 (0.0001)  loss: 1082.2977 (1074.0205)  grad_norm: 691.2278 (693.8195)  amp_scale: 1.0000 (1.0000)  time: 1.0946  data: 0.3843  max mem: 13835
[11:09:50.270065] [Train][Ep-82/100]  [ 300/1435]  eta: 0:22:26  lr: 0.0001 (0.0001)  loss: 1062.3594 (1068.9349)  grad_norm: 683.0720 (691.4716)  amp_scale: 1.0000 (1.0000)  time: 1.1403  data: 0.1697  max mem: 13835
[11:11:44.668501] [Train][Ep-82/100]  [ 400/1435]  eta: 0:20:16  lr: 0.0001 (0.0001)  loss: 1050.4236 (1064.7872)  grad_norm: 674.7180 (688.5417)  amp_scale: 1.0000 (1.0000)  time: 1.1697  data: 0.3592  max mem: 13835
[11:13:41.479793] [Train][Ep-82/100]  [ 500/1435]  eta: 0:18:17  lr: 0.0001 (0.0001)  loss: 1035.4126 (1056.1363)  grad_norm: 659.9878 (685.5563)  amp_scale: 1.0000 (1.0000)  time: 1.1708  data: 0.5907  max mem: 13835
[11:15:39.952991] [Train][Ep-82/100]  [ 600/1435]  eta: 0:16:21  lr: 0.0001 (0.0001)  loss: 1073.6638 (1056.9512)  grad_norm: 695.6469 (686.4631)  amp_scale: 1.0000 (1.0000)  time: 1.1807  data: 0.5520  max mem: 13835
[11:17:37.302293] [Train][Ep-82/100]  [ 700/1435]  eta: 0:14:24  lr: 0.0001 (0.0001)  loss: 1063.2588 (1056.5258)  grad_norm: 672.0790 (685.3524)  amp_scale: 1.0000 (1.0000)  time: 1.1237  data: 0.5554  max mem: 13835
[11:19:34.795533] [Train][Ep-82/100]  [ 800/1435]  eta: 0:12:26  lr: 0.0001 (0.0001)  loss: 1055.6086 (1056.5106)  grad_norm: 692.8446 (685.5180)  amp_scale: 1.0000 (1.0000)  time: 1.1827  data: 0.6143  max mem: 13835
[11:21:31.639960] [Train][Ep-82/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0001 (0.0001)  loss: 1060.9070 (1057.0228)  grad_norm: 682.1191 (686.4732)  amp_scale: 1.0000 (1.0000)  time: 1.1639  data: 0.1657  max mem: 13835
[11:23:32.256883] [Train][Ep-82/100]  [1000/1435]  eta: 0:08:32  lr: 0.0001 (0.0001)  loss: 1064.6490 (1058.0537)  grad_norm: 667.5654 (686.2222)  amp_scale: 1.0000 (1.0000)  time: 1.2419  data: 0.0669  max mem: 13835
[11:25:35.491976] [Train][Ep-82/100]  [1100/1435]  eta: 0:06:36  lr: 0.0001 (0.0001)  loss: 1034.4622 (1057.2600)  grad_norm: 668.5081 (685.2854)  amp_scale: 1.0000 (1.0000)  time: 1.2130  data: 0.2487  max mem: 13835
[11:27:36.085242] [Train][Ep-82/100]  [1200/1435]  eta: 0:04:38  lr: 0.0001 (0.0001)  loss: 1058.5295 (1056.6802)  grad_norm: 680.3416 (685.9134)  amp_scale: 1.0000 (1.0000)  time: 1.1938  data: 0.0156  max mem: 13835
[11:29:40.987143] [Train][Ep-82/100]  [1300/1435]  eta: 0:02:40  lr: 0.0001 (0.0001)  loss: 1062.8964 (1057.8955)  grad_norm: 675.3878 (685.6195)  amp_scale: 1.0000 (1.0000)  time: 1.2446  data: 0.0804  max mem: 13835
[11:31:40.725500] [Train][Ep-82/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1081.8910 (1058.9300)  grad_norm: 696.1475 (686.8689)  amp_scale: 1.0000 (1.0000)  time: 1.1865  data: 0.0006  max mem: 13835
[11:32:20.162281] [Train][Ep-82/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1060.0399 (1057.9759)  grad_norm: 697.3923 (686.8310)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.0693  max mem: 13835
[11:32:20.163188] [Train][Ep-82/100] Total time: 0:28:27 (1.1896 s / it)
[11:32:20.163667] Syncing meters...
[11:32:20.561638] Averaged stats: lr: 0.0001 (0.0001)  loss: 1060.0399 (1055.3000)  grad_norm: 697.3923 (686.8310)  amp_scale: 1.0000 (1.0000)
[11:32:30.361812] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 83)
[11:32:32.100962] [Train][Ep-83/100]  [   0/1435]  eta: 0:41:23  lr: 0.0001 (0.0001)  time: 1.7309  data: 1.2474  max mem: 13835
[11:34:29.946114] [Train][Ep-83/100]  [ 100/1435]  eta: 0:26:20  lr: 0.0001 (0.0001)  loss: 1036.9167 (1047.4537)  grad_norm: 688.0511 (691.2917)  amp_scale: 1.0000 (1.0000)  time: 1.1555  data: 0.5867  max mem: 13835
[11:36:27.918322] [Train][Ep-83/100]  [ 200/1435]  eta: 0:24:19  lr: 0.0001 (0.0001)  loss: 1041.2346 (1052.2202)  grad_norm: 693.4244 (697.1421)  amp_scale: 1.0000 (1.0000)  time: 1.1420  data: 0.5731  max mem: 13835
[11:38:24.506443] [Train][Ep-83/100]  [ 300/1435]  eta: 0:22:15  lr: 0.0001 (0.0001)  loss: 1050.2529 (1053.2012)  grad_norm: 680.9946 (692.1386)  amp_scale: 1.0000 (1.0000)  time: 1.1514  data: 0.4148  max mem: 13835
[11:40:22.959139] [Train][Ep-83/100]  [ 400/1435]  eta: 0:20:19  lr: 0.0001 (0.0001)  loss: 1067.5342 (1053.0981)  grad_norm: 673.6266 (691.8336)  amp_scale: 1.0000 (1.0000)  time: 1.1258  data: 0.0448  max mem: 13835
[11:42:20.749730] [Train][Ep-83/100]  [ 500/1435]  eta: 0:18:21  lr: 0.0001 (0.0001)  loss: 1034.6433 (1056.1050)  grad_norm: 681.6315 (690.0896)  amp_scale: 1.0000 (1.0000)  time: 1.1491  data: 0.0003  max mem: 13835
[11:44:16.196618] [Train][Ep-83/100]  [ 600/1435]  eta: 0:16:20  lr: 0.0001 (0.0001)  loss: 1046.0170 (1055.9000)  grad_norm: 676.1472 (689.2113)  amp_scale: 1.0000 (1.0000)  time: 1.1351  data: 0.0961  max mem: 13835
[11:46:15.835797] [Train][Ep-83/100]  [ 700/1435]  eta: 0:14:25  lr: 0.0001 (0.0001)  loss: 1040.1155 (1057.2309)  grad_norm: 661.7728 (687.2778)  amp_scale: 1.0000 (1.0000)  time: 1.2241  data: 0.1891  max mem: 13835
[11:48:13.683633] [Train][Ep-83/100]  [ 800/1435]  eta: 0:12:27  lr: 0.0001 (0.0001)  loss: 1054.9755 (1056.3391)  grad_norm: 678.4384 (686.7973)  amp_scale: 1.0000 (1.0000)  time: 1.2418  data: 0.0198  max mem: 13835
[11:50:13.519316] [Train][Ep-83/100]  [ 900/1435]  eta: 0:10:31  lr: 0.0001 (0.0001)  loss: 1034.9675 (1057.3174)  grad_norm: 680.5355 (686.6651)  amp_scale: 1.0000 (1.0000)  time: 1.2009  data: 0.1306  max mem: 13835
[11:52:15.151252] [Train][Ep-83/100]  [1000/1435]  eta: 0:08:34  lr: 0.0001 (0.0001)  loss: 1026.3903 (1056.3259)  grad_norm: 690.4045 (687.1541)  amp_scale: 1.0000 (1.0000)  time: 1.2262  data: 0.0031  max mem: 13835
[11:54:18.897929] [Train][Ep-83/100]  [1100/1435]  eta: 0:06:38  lr: 0.0001 (0.0001)  loss: 1057.5925 (1057.7507)  grad_norm: 680.5691 (686.5587)  amp_scale: 1.0000 (1.0000)  time: 1.2365  data: 0.0030  max mem: 13835
[11:56:17.795339] [Train][Ep-83/100]  [1200/1435]  eta: 0:04:39  lr: 0.0001 (0.0001)  loss: 1066.3077 (1058.4945)  grad_norm: 678.3096 (686.9267)  amp_scale: 1.0000 (1.0000)  time: 1.1908  data: 0.1758  max mem: 13835
[11:58:12.225888] [Train][Ep-83/100]  [1300/1435]  eta: 0:02:39  lr: 0.0001 (0.0001)  loss: 1041.1040 (1056.6067)  grad_norm: 696.1634 (687.9327)  amp_scale: 1.0000 (1.0000)  time: 1.1137  data: 0.5300  max mem: 13835
[12:00:11.531152] [Train][Ep-83/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1088.6390 (1057.4694)  grad_norm: 692.2389 (688.9125)  amp_scale: 1.0000 (1.0000)  time: 1.2029  data: 0.6373  max mem: 13835
[12:00:49.324151] [Train][Ep-83/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1053.8075 (1056.4536)  grad_norm: 694.0981 (689.2371)  amp_scale: 1.0000 (1.0000)  time: 1.0528  data: 0.4834  max mem: 13835
[12:00:49.325172] [Train][Ep-83/100] Total time: 0:28:18 (1.1839 s / it)
[12:00:49.325639] Syncing meters...
[12:00:49.327036] Averaged stats: lr: 0.0001 (0.0001)  loss: 1053.8075 (1055.0060)  grad_norm: 694.0981 (689.2371)  amp_scale: 1.0000 (1.0000)
[12:00:58.985362] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 84)
[12:01:00.837038] [Train][Ep-84/100]  [   0/1435]  eta: 0:44:05  lr: 0.0001 (0.0001)  time: 1.8434  data: 1.3597  max mem: 13835
[12:03:01.157419] [Train][Ep-84/100]  [ 100/1435]  eta: 0:26:54  lr: 0.0001 (0.0001)  loss: 1037.4651 (1029.1276)  grad_norm: 686.5132 (692.0082)  amp_scale: 1.0000 (1.0000)  time: 1.1848  data: 0.6049  max mem: 13835
[12:04:52.475499] [Train][Ep-84/100]  [ 200/1435]  eta: 0:23:54  lr: 0.0001 (0.0001)  loss: 1016.3312 (1035.3600)  grad_norm: 679.8607 (695.0947)  amp_scale: 1.0000 (1.0000)  time: 1.1015  data: 0.5165  max mem: 13835
[12:06:49.511724] [Train][Ep-84/100]  [ 300/1435]  eta: 0:22:01  lr: 0.0001 (0.0001)  loss: 1063.0984 (1044.7947)  grad_norm: 682.6522 (690.0884)  amp_scale: 1.0000 (1.0000)  time: 1.2448  data: 0.0007  max mem: 13835
[12:08:50.606117] [Train][Ep-84/100]  [ 400/1435]  eta: 0:20:17  lr: 0.0001 (0.0001)  loss: 1037.1849 (1043.7847)  grad_norm: 676.3447 (686.5445)  amp_scale: 1.0000 (1.0000)  time: 1.2276  data: 0.6530  max mem: 13835
[12:10:45.015759] [Train][Ep-84/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0001 (0.0001)  loss: 1073.8920 (1044.7574)  grad_norm: 692.1842 (686.2865)  amp_scale: 1.0000 (1.0000)  time: 1.0429  data: 0.4568  max mem: 13835
[12:12:41.444491] [Train][Ep-84/100]  [ 600/1435]  eta: 0:16:15  lr: 0.0001 (0.0001)  loss: 1047.9431 (1050.1239)  grad_norm: 691.9242 (687.4208)  amp_scale: 1.0000 (1.0000)  time: 1.1622  data: 0.0431  max mem: 13835
[12:14:38.945229] [Train][Ep-84/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0001 (0.0001)  loss: 1037.6948 (1049.1462)  grad_norm: 695.4940 (689.8350)  amp_scale: 1.0000 (1.0000)  time: 1.1842  data: 0.1616  max mem: 13835
[12:16:35.258476] [Train][Ep-84/100]  [ 800/1435]  eta: 0:12:22  lr: 0.0001 (0.0001)  loss: 1058.9448 (1050.7913)  grad_norm: 677.8946 (688.8985)  amp_scale: 1.0000 (1.0000)  time: 1.1663  data: 0.3081  max mem: 13835
[12:18:33.032979] [Train][Ep-84/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0001 (0.0001)  loss: 1022.7520 (1050.3188)  grad_norm: 688.2394 (689.6954)  amp_scale: 1.0000 (1.0000)  time: 1.0957  data: 0.5012  max mem: 13835
[12:20:27.456957] [Train][Ep-84/100]  [1000/1435]  eta: 0:08:27  lr: 0.0001 (0.0001)  loss: 1086.4203 (1052.5765)  grad_norm: 702.2612 (690.2956)  amp_scale: 1.0000 (1.0000)  time: 1.1409  data: 0.3227  max mem: 13835
[12:22:25.701438] [Train][Ep-84/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1025.3740 (1049.8959)  grad_norm: 682.6564 (690.6962)  amp_scale: 1.0000 (1.0000)  time: 1.2787  data: 0.7102  max mem: 13835
[12:24:25.435923] [Train][Ep-84/100]  [1200/1435]  eta: 0:04:35  lr: 0.0001 (0.0001)  loss: 1050.0243 (1050.3052)  grad_norm: 677.3208 (690.6102)  amp_scale: 1.0000 (1.0000)  time: 1.1736  data: 0.5984  max mem: 13835
[12:26:24.510412] [Train][Ep-84/100]  [1300/1435]  eta: 0:02:38  lr: 0.0001 (0.0001)  loss: 1007.5735 (1048.5358)  grad_norm: 674.5454 (690.4139)  amp_scale: 1.0000 (1.0000)  time: 1.1623  data: 0.5957  max mem: 13835
[12:28:21.301227] [Train][Ep-84/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1039.5994 (1047.8084)  grad_norm: 697.3521 (689.8787)  amp_scale: 1.0000 (1.0000)  time: 1.2199  data: 0.0072  max mem: 13835
[12:29:00.136686] [Train][Ep-84/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1063.7827 (1048.3719)  grad_norm: 693.5850 (689.9171)  amp_scale: 1.0000 (1.0000)  time: 1.1950  data: 0.0134  max mem: 13835
[12:29:00.137582] [Train][Ep-84/100] Total time: 0:28:01 (1.1715 s / it)
[12:29:00.138037] Syncing meters...
[12:29:00.352121] Averaged stats: lr: 0.0001 (0.0001)  loss: 1063.7827 (1048.1204)  grad_norm: 693.5850 (689.9171)  amp_scale: 1.0000 (1.0000)
[12:29:10.346707] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 85)
[12:29:12.465175] [Train][Ep-85/100]  [   0/1435]  eta: 0:50:27  lr: 0.0001 (0.0001)  time: 2.1095  data: 1.6265  max mem: 13835
[12:31:08.891961] [Train][Ep-85/100]  [ 100/1435]  eta: 0:26:06  lr: 0.0001 (0.0001)  loss: 1032.3246 (1037.0614)  grad_norm: 684.9958 (691.8228)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.5902  max mem: 13835
[12:33:00.702804] [Train][Ep-85/100]  [ 200/1435]  eta: 0:23:35  lr: 0.0000 (0.0001)  loss: 1030.9352 (1044.4835)  grad_norm: 670.0302 (687.4771)  amp_scale: 1.0000 (1.0000)  time: 1.1218  data: 0.3696  max mem: 13835
[12:34:57.201082] [Train][Ep-85/100]  [ 300/1435]  eta: 0:21:47  lr: 0.0000 (0.0000)  loss: 1039.9890 (1039.8793)  grad_norm: 669.1379 (687.5740)  amp_scale: 1.0000 (1.0000)  time: 1.2049  data: 0.0192  max mem: 13835
[12:36:55.962059] [Train][Ep-85/100]  [ 400/1435]  eta: 0:20:01  lr: 0.0000 (0.0000)  loss: 1047.7690 (1041.9872)  grad_norm: 686.3672 (688.6730)  amp_scale: 1.0000 (1.0000)  time: 1.1439  data: 0.0012  max mem: 13835
[12:38:51.348592] [Train][Ep-85/100]  [ 500/1435]  eta: 0:18:04  lr: 0.0000 (0.0000)  loss: 1020.8358 (1039.9724)  grad_norm: 670.1310 (686.5050)  amp_scale: 1.0000 (1.0000)  time: 1.0910  data: 0.3832  max mem: 13835
[12:40:46.277088] [Train][Ep-85/100]  [ 600/1435]  eta: 0:16:06  lr: 0.0000 (0.0000)  loss: 1035.4883 (1041.2013)  grad_norm: 682.5488 (685.5152)  amp_scale: 1.0000 (1.0000)  time: 1.1704  data: 0.5902  max mem: 13835
[12:42:44.869770] [Train][Ep-85/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0000 (0.0000)  loss: 1036.7031 (1043.5844)  grad_norm: 685.5567 (686.3149)  amp_scale: 1.0000 (1.0000)  time: 1.2202  data: 0.6458  max mem: 13835
[12:44:42.328619] [Train][Ep-85/100]  [ 800/1435]  eta: 0:12:18  lr: 0.0000 (0.0000)  loss: 1027.8790 (1042.4043)  grad_norm: 687.5089 (686.9376)  amp_scale: 1.0000 (1.0000)  time: 1.1844  data: 0.2223  max mem: 13835
[12:46:42.582843] [Train][Ep-85/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0000 (0.0000)  loss: 1024.7742 (1042.9702)  grad_norm: 681.3967 (686.2231)  amp_scale: 1.0000 (1.0000)  time: 1.1439  data: 0.0789  max mem: 13835
[12:48:39.843685] [Train][Ep-85/100]  [1000/1435]  eta: 0:08:28  lr: 0.0000 (0.0000)  loss: 1050.5979 (1043.4655)  grad_norm: 685.2410 (686.9412)  amp_scale: 1.0000 (1.0000)  time: 1.1698  data: 0.1355  max mem: 13835
[12:50:37.105839] [Train][Ep-85/100]  [1100/1435]  eta: 0:06:31  lr: 0.0000 (0.0000)  loss: 1055.7356 (1045.8224)  grad_norm: 677.6981 (686.5965)  amp_scale: 1.0000 (1.0000)  time: 1.1105  data: 0.4907  max mem: 13835
[12:52:29.385765] [Train][Ep-85/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1033.7764 (1046.5535)  grad_norm: 690.6086 (686.5514)  amp_scale: 1.0000 (1.0000)  time: 1.1273  data: 0.1021  max mem: 13835
[12:54:24.228808] [Train][Ep-85/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1025.6887 (1046.1680)  grad_norm: 691.6834 (686.7935)  amp_scale: 1.0000 (1.0000)  time: 1.1989  data: 0.0449  max mem: 13835
[12:56:22.381752] [Train][Ep-85/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1045.3629 (1046.9381)  grad_norm: 677.1940 (686.5532)  amp_scale: 1.0000 (1.0000)  time: 1.1998  data: 0.1354  max mem: 13835
[12:57:02.588144] [Train][Ep-85/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1037.1481 (1046.3477)  grad_norm: 677.8402 (686.5897)  amp_scale: 1.0000 (1.0000)  time: 1.1822  data: 0.0126  max mem: 13835
[12:57:02.589118] [Train][Ep-85/100] Total time: 0:27:52 (1.1653 s / it)
[12:57:02.589582] Syncing meters...
[12:57:02.909838] Averaged stats: lr: 0.0000 (0.0000)  loss: 1037.1481 (1047.7032)  grad_norm: 677.8402 (686.5897)  amp_scale: 1.0000 (1.0000)
[12:57:11.862081] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 86)
[12:57:13.772707] [Train][Ep-86/100]  [   0/1435]  eta: 0:45:29  lr: 0.0000 (0.0000)  time: 1.9020  data: 1.4188  max mem: 13835
[12:59:11.809697] [Train][Ep-86/100]  [ 100/1435]  eta: 0:26:25  lr: 0.0000 (0.0000)  loss: 1038.5996 (1053.1040)  grad_norm: 685.8779 (697.1996)  amp_scale: 1.0000 (1.0000)  time: 1.1525  data: 0.5795  max mem: 13835
[13:01:11.273914] [Train][Ep-86/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0000 (0.0000)  loss: 1068.8082 (1052.0811)  grad_norm: 680.7664 (692.2754)  amp_scale: 1.0000 (1.0000)  time: 1.2291  data: 0.6602  max mem: 13835
[13:03:11.332569] [Train][Ep-86/100]  [ 300/1435]  eta: 0:22:35  lr: 0.0000 (0.0000)  loss: 1051.6527 (1046.2138)  grad_norm: 683.5003 (690.4604)  amp_scale: 1.0000 (1.0000)  time: 1.1831  data: 0.3647  max mem: 13835
[13:05:07.985060] [Train][Ep-86/100]  [ 400/1435]  eta: 0:20:28  lr: 0.0000 (0.0000)  loss: 1028.2860 (1044.5433)  grad_norm: 674.6774 (689.2773)  amp_scale: 1.0000 (1.0000)  time: 1.1613  data: 0.3649  max mem: 13835
[13:07:05.655753] [Train][Ep-86/100]  [ 500/1435]  eta: 0:18:28  lr: 0.0000 (0.0000)  loss: 1035.6721 (1047.6627)  grad_norm: 685.7780 (686.8714)  amp_scale: 1.0000 (1.0000)  time: 1.1965  data: 0.3023  max mem: 13835
[13:09:00.752527] [Train][Ep-86/100]  [ 600/1435]  eta: 0:16:24  lr: 0.0000 (0.0000)  loss: 1057.4012 (1047.9201)  grad_norm: 684.8220 (686.8907)  amp_scale: 1.0000 (1.0000)  time: 1.1743  data: 0.2339  max mem: 13835
[13:10:55.803210] [Train][Ep-86/100]  [ 700/1435]  eta: 0:14:23  lr: 0.0000 (0.0000)  loss: 1060.5358 (1047.8241)  grad_norm: 675.7974 (686.2343)  amp_scale: 1.0000 (1.0000)  time: 1.1744  data: 0.2391  max mem: 13835
[13:12:51.827339] [Train][Ep-86/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0000 (0.0000)  loss: 1030.4380 (1045.2025)  grad_norm: 669.1720 (685.1610)  amp_scale: 1.0000 (1.0000)  time: 1.2459  data: 0.6741  max mem: 13835
[13:14:48.998700] [Train][Ep-86/100]  [ 900/1435]  eta: 0:10:27  lr: 0.0000 (0.0000)  loss: 1023.2675 (1044.9327)  grad_norm: 693.1122 (686.5553)  amp_scale: 1.0000 (1.0000)  time: 1.1037  data: 0.4267  max mem: 13835
[13:16:46.079831] [Train][Ep-86/100]  [1000/1435]  eta: 0:08:30  lr: 0.0000 (0.0000)  loss: 1067.7552 (1046.4358)  grad_norm: 675.7424 (685.9051)  amp_scale: 1.0000 (1.0000)  time: 1.1149  data: 0.0962  max mem: 13835
[13:18:41.996931] [Train][Ep-86/100]  [1100/1435]  eta: 0:06:32  lr: 0.0000 (0.0000)  loss: 1057.8873 (1047.2745)  grad_norm: 666.6590 (685.8187)  amp_scale: 1.0000 (1.0000)  time: 1.1386  data: 0.5567  max mem: 13835
[13:20:32.114891] [Train][Ep-86/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1023.3745 (1046.3672)  grad_norm: 665.7202 (685.1797)  amp_scale: 1.0000 (1.0000)  time: 1.0968  data: 0.4875  max mem: 13835
[13:22:26.555961] [Train][Ep-86/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1051.5106 (1047.5703)  grad_norm: 688.4891 (685.6853)  amp_scale: 1.0000 (1.0000)  time: 1.0988  data: 0.5110  max mem: 13835
[13:24:21.238928] [Train][Ep-86/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1030.6261 (1047.3028)  grad_norm: 675.1705 (685.9671)  amp_scale: 1.0000 (1.0000)  time: 1.1689  data: 0.5861  max mem: 13835
[13:24:58.402719] [Train][Ep-86/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1040.1255 (1046.2252)  grad_norm: 703.4389 (686.2306)  amp_scale: 1.0000 (1.0000)  time: 1.0722  data: 0.5023  max mem: 13835
[13:24:58.403605] [Train][Ep-86/100] Total time: 0:27:46 (1.1613 s / it)
[13:24:58.404075] Syncing meters...
[13:24:58.405380] Averaged stats: lr: 0.0000 (0.0000)  loss: 1040.1255 (1041.4928)  grad_norm: 703.4389 (686.2306)  amp_scale: 1.0000 (1.0000)
[13:25:08.454318] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 87)
[13:25:10.226537] [Train][Ep-87/100]  [   0/1435]  eta: 0:42:10  lr: 0.0000 (0.0000)  time: 1.7635  data: 1.2786  max mem: 13835
[13:27:07.783824] [Train][Ep-87/100]  [ 100/1435]  eta: 0:26:17  lr: 0.0000 (0.0000)  loss: 1046.3768 (1065.8442)  grad_norm: 673.1435 (684.5685)  amp_scale: 1.0000 (1.0000)  time: 1.1425  data: 0.5701  max mem: 13835
[13:29:01.258179] [Train][Ep-87/100]  [ 200/1435]  eta: 0:23:50  lr: 0.0000 (0.0000)  loss: 1048.3359 (1052.5653)  grad_norm: 697.4131 (687.7000)  amp_scale: 1.0000 (1.0000)  time: 1.0917  data: 0.5122  max mem: 13835
[13:30:58.148282] [Train][Ep-87/100]  [ 300/1435]  eta: 0:21:58  lr: 0.0000 (0.0000)  loss: 1019.9345 (1052.9179)  grad_norm: 695.4122 (691.2045)  amp_scale: 1.0000 (1.0000)  time: 1.1638  data: 0.0834  max mem: 13835
[13:32:50.712734] [Train][Ep-87/100]  [ 400/1435]  eta: 0:19:52  lr: 0.0000 (0.0000)  loss: 1066.8832 (1051.5635)  grad_norm: 691.5726 (693.9207)  amp_scale: 1.0000 (1.0000)  time: 1.2439  data: 0.3610  max mem: 13835
[13:34:52.255655] [Train][Ep-87/100]  [ 500/1435]  eta: 0:18:09  lr: 0.0000 (0.0000)  loss: 1046.0082 (1049.6693)  grad_norm: 676.7221 (691.5571)  amp_scale: 1.0000 (1.0000)  time: 1.1933  data: 0.2721  max mem: 13835
[13:36:53.189587] [Train][Ep-87/100]  [ 600/1435]  eta: 0:16:19  lr: 0.0000 (0.0000)  loss: 1022.1531 (1047.1437)  grad_norm: 682.3405 (691.2902)  amp_scale: 1.0000 (1.0000)  time: 1.1732  data: 0.0029  max mem: 13835
[13:38:51.374027] [Train][Ep-87/100]  [ 700/1435]  eta: 0:14:22  lr: 0.0000 (0.0000)  loss: 1013.0623 (1045.5176)  grad_norm: 687.7368 (690.5590)  amp_scale: 1.0000 (1.0000)  time: 1.1626  data: 0.0144  max mem: 13835
[13:40:49.514700] [Train][Ep-87/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0000 (0.0000)  loss: 1061.6868 (1044.4630)  grad_norm: 678.5020 (689.2275)  amp_scale: 1.0000 (1.0000)  time: 1.1405  data: 0.1074  max mem: 13835
[13:42:53.775941] [Train][Ep-87/100]  [ 900/1435]  eta: 0:10:32  lr: 0.0000 (0.0000)  loss: 1051.0430 (1044.6077)  grad_norm: 683.9012 (688.8209)  amp_scale: 1.0000 (1.0000)  time: 1.2574  data: 0.0096  max mem: 13835
[13:44:49.008130] [Train][Ep-87/100]  [1000/1435]  eta: 0:08:32  lr: 0.0000 (0.0000)  loss: 992.8718 (1041.3005)  grad_norm: 658.2999 (686.4424)  amp_scale: 1.0000 (1.0000)  time: 1.1001  data: 0.3655  max mem: 13835
[13:46:43.534570] [Train][Ep-87/100]  [1100/1435]  eta: 0:06:34  lr: 0.0000 (0.0000)  loss: 1029.7373 (1040.4822)  grad_norm: 693.2322 (687.0006)  amp_scale: 1.0000 (1.0000)  time: 1.1784  data: 0.1604  max mem: 13835
[13:48:39.372914] [Train][Ep-87/100]  [1200/1435]  eta: 0:04:36  lr: 0.0000 (0.0000)  loss: 1047.3623 (1041.3496)  grad_norm: 708.3586 (688.3349)  amp_scale: 1.0000 (1.0000)  time: 1.1744  data: 0.3651  max mem: 13835
[13:50:41.208129] [Train][Ep-87/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1045.3641 (1041.7633)  grad_norm: 685.2417 (688.7363)  amp_scale: 1.0000 (1.0000)  time: 1.1902  data: 0.3189  max mem: 13835
[13:52:41.405513] [Train][Ep-87/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1005.4277 (1040.7844)  grad_norm: 680.9670 (688.4327)  amp_scale: 1.0000 (1.0000)  time: 1.1572  data: 0.4810  max mem: 13835
[13:53:17.214469] [Train][Ep-87/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1005.4277 (1040.3656)  grad_norm: 671.6999 (688.2159)  amp_scale: 1.0000 (1.0000)  time: 1.0576  data: 0.3392  max mem: 13835
[13:53:17.215425] [Train][Ep-87/100] Total time: 0:28:08 (1.1768 s / it)
[13:53:17.215933] Syncing meters...
[13:53:17.830178] Averaged stats: lr: 0.0000 (0.0000)  loss: 1005.4277 (1041.9730)  grad_norm: 671.6999 (688.2159)  amp_scale: 1.0000 (1.0000)
[13:53:26.494113] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 88)
[13:53:28.300679] [Train][Ep-88/100]  [   0/1435]  eta: 0:43:00  lr: 0.0000 (0.0000)  time: 1.7986  data: 1.3148  max mem: 13835
[13:55:21.833079] [Train][Ep-88/100]  [ 100/1435]  eta: 0:25:24  lr: 0.0000 (0.0000)  loss: 1044.3242 (1036.6810)  grad_norm: 676.9260 (680.0479)  amp_scale: 1.0000 (1.0000)  time: 1.1088  data: 0.3289  max mem: 13835
[13:57:17.102916] [Train][Ep-88/100]  [ 200/1435]  eta: 0:23:36  lr: 0.0000 (0.0000)  loss: 1022.8857 (1036.7455)  grad_norm: 681.8529 (686.7960)  amp_scale: 1.0000 (1.0000)  time: 1.1747  data: 0.2507  max mem: 13835
[13:59:13.922628] [Train][Ep-88/100]  [ 300/1435]  eta: 0:21:49  lr: 0.0000 (0.0000)  loss: 1053.2175 (1049.6859)  grad_norm: 694.3609 (687.9269)  amp_scale: 1.0000 (1.0000)  time: 1.1375  data: 0.2672  max mem: 13835
[14:01:08.993516] [Train][Ep-88/100]  [ 400/1435]  eta: 0:19:53  lr: 0.0000 (0.0000)  loss: 1044.6399 (1051.2444)  grad_norm: 676.0596 (686.4988)  amp_scale: 1.0000 (1.0000)  time: 1.1017  data: 0.3260  max mem: 13835
[14:02:59.694876] [Train][Ep-88/100]  [ 500/1435]  eta: 0:17:49  lr: 0.0000 (0.0000)  loss: 1030.4739 (1048.8521)  grad_norm: 672.4248 (683.9258)  amp_scale: 1.0000 (1.0000)  time: 1.0994  data: 0.4879  max mem: 13835
[14:04:55.053524] [Train][Ep-88/100]  [ 600/1435]  eta: 0:15:56  lr: 0.0000 (0.0000)  loss: 1088.6609 (1055.3716)  grad_norm: 676.6422 (683.3897)  amp_scale: 1.0000 (1.0000)  time: 1.0998  data: 0.3570  max mem: 13835
[14:06:50.567586] [Train][Ep-88/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0000 (0.0000)  loss: 1034.5046 (1053.5251)  grad_norm: 684.5194 (685.2840)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.2223  max mem: 13835
[14:08:47.540134] [Train][Ep-88/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0000 (0.0000)  loss: 1070.9097 (1055.0514)  grad_norm: 693.6207 (685.6958)  amp_scale: 1.0000 (1.0000)  time: 1.1767  data: 0.2750  max mem: 13835
[14:10:43.777901] [Train][Ep-88/100]  [ 900/1435]  eta: 0:10:15  lr: 0.0000 (0.0000)  loss: 1022.0554 (1054.1829)  grad_norm: 693.2846 (687.4980)  amp_scale: 1.0000 (1.0000)  time: 1.1644  data: 0.1769  max mem: 13835
[14:12:39.024789] [Train][Ep-88/100]  [1000/1435]  eta: 0:08:20  lr: 0.0000 (0.0000)  loss: 1029.0023 (1050.9074)  grad_norm: 697.9297 (689.2650)  amp_scale: 1.0000 (1.0000)  time: 1.1633  data: 0.0865  max mem: 13835
[14:14:34.060265] [Train][Ep-88/100]  [1100/1435]  eta: 0:06:25  lr: 0.0000 (0.0000)  loss: 1039.5924 (1051.2526)  grad_norm: 682.3528 (688.6286)  amp_scale: 1.0000 (1.0000)  time: 1.1437  data: 0.3978  max mem: 13835
[14:16:22.910432] [Train][Ep-88/100]  [1200/1435]  eta: 0:04:29  lr: 0.0000 (0.0000)  loss: 1034.0973 (1049.9162)  grad_norm: 689.0880 (688.7684)  amp_scale: 1.0000 (1.0000)  time: 1.0936  data: 0.5192  max mem: 13835
[14:18:14.987920] [Train][Ep-88/100]  [1300/1435]  eta: 0:02:34  lr: 0.0000 (0.0000)  loss: 1043.1112 (1049.3883)  grad_norm: 684.8789 (688.7174)  amp_scale: 1.0000 (1.0000)  time: 1.0602  data: 0.2970  max mem: 13835
[14:20:12.155278] [Train][Ep-88/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1011.4458 (1046.9837)  grad_norm: 676.8829 (689.5012)  amp_scale: 1.0000 (1.0000)  time: 1.1890  data: 0.0030  max mem: 13835
[14:20:51.774660] [Train][Ep-88/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1041.1111 (1047.0984)  grad_norm: 675.2147 (689.5868)  amp_scale: 1.0000 (1.0000)  time: 1.1996  data: 0.0244  max mem: 13835
[14:20:51.775334] [Train][Ep-88/100] Total time: 0:27:25 (1.1465 s / it)
[14:20:51.775739] Syncing meters...
[14:20:52.076403] Averaged stats: lr: 0.0000 (0.0000)  loss: 1041.1111 (1044.3352)  grad_norm: 675.2147 (689.5868)  amp_scale: 1.0000 (1.0000)
[14:21:00.606048] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 89)
[14:21:02.465104] [Train][Ep-89/100]  [   0/1435]  eta: 0:44:14  lr: 0.0000 (0.0000)  time: 1.8499  data: 1.3666  max mem: 13835
[14:22:57.874525] [Train][Ep-89/100]  [ 100/1435]  eta: 0:25:49  lr: 0.0000 (0.0000)  loss: 1044.3843 (1058.3486)  grad_norm: 672.8142 (685.2027)  amp_scale: 1.0000 (1.0000)  time: 1.2205  data: 0.6435  max mem: 13835
[14:24:52.683437] [Train][Ep-89/100]  [ 200/1435]  eta: 0:23:45  lr: 0.0000 (0.0000)  loss: 1070.2810 (1062.6409)  grad_norm: 671.2813 (679.6532)  amp_scale: 1.0000 (1.0000)  time: 1.0992  data: 0.5270  max mem: 13835
[14:26:54.107027] [Train][Ep-89/100]  [ 300/1435]  eta: 0:22:12  lr: 0.0000 (0.0000)  loss: 1022.8433 (1051.5715)  grad_norm: 680.2551 (679.7684)  amp_scale: 1.0000 (1.0000)  time: 1.1297  data: 0.0596  max mem: 13835
[14:28:51.428854] [Train][Ep-89/100]  [ 400/1435]  eta: 0:20:15  lr: 0.0000 (0.0000)  loss: 1011.4242 (1045.4508)  grad_norm: 683.8036 (679.1342)  amp_scale: 1.0000 (1.0000)  time: 1.1615  data: 0.5924  max mem: 13835
[14:30:48.205678] [Train][Ep-89/100]  [ 500/1435]  eta: 0:18:16  lr: 0.0000 (0.0000)  loss: 987.1630 (1039.3826)  grad_norm: 676.5896 (679.0829)  amp_scale: 1.0000 (1.0000)  time: 1.2255  data: 0.4918  max mem: 13835
[14:32:49.114041] [Train][Ep-89/100]  [ 600/1435]  eta: 0:16:24  lr: 0.0000 (0.0000)  loss: 1071.7961 (1045.2787)  grad_norm: 676.8076 (681.5640)  amp_scale: 1.0000 (1.0000)  time: 1.2730  data: 0.0264  max mem: 13835
[14:34:48.434647] [Train][Ep-89/100]  [ 700/1435]  eta: 0:14:27  lr: 0.0000 (0.0000)  loss: 1041.2651 (1046.3171)  grad_norm: 690.9594 (682.7321)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.0336  max mem: 13835
[14:36:45.942115] [Train][Ep-89/100]  [ 800/1435]  eta: 0:12:29  lr: 0.0000 (0.0000)  loss: 1044.6366 (1045.9794)  grad_norm: 693.5831 (684.4638)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.1833  max mem: 13835
[14:38:49.701347] [Train][Ep-89/100]  [ 900/1435]  eta: 0:10:34  lr: 0.0000 (0.0000)  loss: 1059.5424 (1046.7903)  grad_norm: 667.5660 (683.7922)  amp_scale: 1.0000 (1.0000)  time: 1.2140  data: 0.0031  max mem: 13835
[14:40:46.058167] [Train][Ep-89/100]  [1000/1435]  eta: 0:08:35  lr: 0.0000 (0.0000)  loss: 1034.2963 (1047.2273)  grad_norm: 678.5451 (683.4848)  amp_scale: 1.0000 (1.0000)  time: 1.1741  data: 0.1355  max mem: 13835
[14:42:43.542458] [Train][Ep-89/100]  [1100/1435]  eta: 0:06:36  lr: 0.0000 (0.0000)  loss: 1049.7594 (1047.1071)  grad_norm: 682.7642 (683.8149)  amp_scale: 1.0000 (1.0000)  time: 1.1871  data: 0.2188  max mem: 13835
[14:44:41.856363] [Train][Ep-89/100]  [1200/1435]  eta: 0:04:38  lr: 0.0000 (0.0000)  loss: 1046.5824 (1048.2740)  grad_norm: 690.8344 (684.9089)  amp_scale: 1.0000 (1.0000)  time: 1.1832  data: 0.3135  max mem: 13835
[14:46:34.395262] [Train][Ep-89/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1077.1398 (1049.2909)  grad_norm: 675.8359 (684.6187)  amp_scale: 1.0000 (1.0000)  time: 1.0951  data: 0.3631  max mem: 13835
[14:48:31.946608] [Train][Ep-89/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1008.1614 (1047.5969)  grad_norm: 678.1177 (684.6396)  amp_scale: 1.0000 (1.0000)  time: 1.1550  data: 0.5475  max mem: 13835
[14:49:08.763555] [Train][Ep-89/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1025.2970 (1048.6576)  grad_norm: 682.7498 (684.8238)  amp_scale: 1.0000 (1.0000)  time: 1.0815  data: 0.3709  max mem: 13835
[14:49:08.764409] [Train][Ep-89/100] Total time: 0:28:08 (1.1764 s / it)
[14:49:08.764936] Syncing meters...
[14:49:09.417221] Averaged stats: lr: 0.0000 (0.0000)  loss: 1025.2970 (1043.1638)  grad_norm: 682.7498 (684.8238)  amp_scale: 1.0000 (1.0000)
[14:49:19.345789] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 90)
[14:49:21.430949] [Train][Ep-90/100]  [   0/1435]  eta: 0:49:39  lr: 0.0000 (0.0000)  time: 2.0764  data: 1.5931  max mem: 13835
[14:51:19.186360] [Train][Ep-90/100]  [ 100/1435]  eta: 0:26:23  lr: 0.0000 (0.0000)  loss: 1018.0597 (1046.9417)  grad_norm: 690.2184 (695.8047)  amp_scale: 1.0000 (1.0000)  time: 1.1763  data: 0.6083  max mem: 13835
[14:53:14.834658] [Train][Ep-90/100]  [ 200/1435]  eta: 0:24:06  lr: 0.0000 (0.0000)  loss: 1054.3846 (1047.8973)  grad_norm: 689.5854 (693.5289)  amp_scale: 1.0000 (1.0000)  time: 1.1077  data: 0.4109  max mem: 13835
[14:55:12.429384] [Train][Ep-90/100]  [ 300/1435]  eta: 0:22:11  lr: 0.0000 (0.0000)  loss: 1009.0563 (1040.8785)  grad_norm: 685.6714 (688.0569)  amp_scale: 1.0000 (1.0000)  time: 1.2086  data: 0.0016  max mem: 13835
[14:57:11.073457] [Train][Ep-90/100]  [ 400/1435]  eta: 0:20:17  lr: 0.0000 (0.0000)  loss: 1074.0533 (1043.5307)  grad_norm: 689.9850 (686.8446)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.0743  max mem: 13835
[14:59:04.973388] [Train][Ep-90/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0000 (0.0000)  loss: 1043.2811 (1042.9948)  grad_norm: 675.5085 (686.9049)  amp_scale: 1.0000 (1.0000)  time: 1.1711  data: 0.4007  max mem: 13835
[15:00:57.584380] [Train][Ep-90/100]  [ 600/1435]  eta: 0:16:10  lr: 0.0000 (0.0000)  loss: 1056.8767 (1047.5596)  grad_norm: 673.9120 (686.8009)  amp_scale: 1.0000 (1.0000)  time: 1.1155  data: 0.4369  max mem: 13835
[15:02:56.050756] [Train][Ep-90/100]  [ 700/1435]  eta: 0:14:16  lr: 0.0000 (0.0000)  loss: 1041.8849 (1048.1090)  grad_norm: 677.7430 (686.3642)  amp_scale: 1.0000 (1.0000)  time: 1.2669  data: 0.0204  max mem: 13835
[15:04:52.390388] [Train][Ep-90/100]  [ 800/1435]  eta: 0:12:19  lr: 0.0000 (0.0000)  loss: 1068.3574 (1048.0691)  grad_norm: 687.3744 (687.2542)  amp_scale: 1.0000 (1.0000)  time: 1.1544  data: 0.4618  max mem: 13835
[15:06:50.073794] [Train][Ep-90/100]  [ 900/1435]  eta: 0:10:23  lr: 0.0000 (0.0000)  loss: 1059.0930 (1047.6384)  grad_norm: 694.8636 (688.1581)  amp_scale: 1.0000 (1.0000)  time: 1.1462  data: 0.2481  max mem: 13835
[15:08:47.877868] [Train][Ep-90/100]  [1000/1435]  eta: 0:08:27  lr: 0.0000 (0.0000)  loss: 1028.6443 (1047.5221)  grad_norm: 675.7928 (688.6104)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.6073  max mem: 13835
[15:10:44.408712] [Train][Ep-90/100]  [1100/1435]  eta: 0:06:30  lr: 0.0000 (0.0000)  loss: 1028.0148 (1046.2631)  grad_norm: 687.2970 (689.1628)  amp_scale: 1.0000 (1.0000)  time: 1.1322  data: 0.5648  max mem: 13835
[15:12:40.172357] [Train][Ep-90/100]  [1200/1435]  eta: 0:04:34  lr: 0.0000 (0.0000)  loss: 988.2571 (1044.5620)  grad_norm: 696.7550 (690.2583)  amp_scale: 1.0000 (1.0000)  time: 1.1287  data: 0.5574  max mem: 13835
[15:14:38.236403] [Train][Ep-90/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1037.1760 (1043.9288)  grad_norm: 687.2938 (690.0098)  amp_scale: 1.0000 (1.0000)  time: 1.1584  data: 0.5849  max mem: 13835
[15:16:32.558681] [Train][Ep-90/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 999.5195 (1042.6853)  grad_norm: 668.1635 (689.2106)  amp_scale: 1.0000 (1.0000)  time: 1.1771  data: 0.0675  max mem: 13835
[15:17:10.975838] [Train][Ep-90/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1006.8777 (1042.4880)  grad_norm: 667.1482 (688.7867)  amp_scale: 1.0000 (1.0000)  time: 1.1637  data: 0.0006  max mem: 13835
[15:17:10.976699] [Train][Ep-90/100] Total time: 0:27:51 (1.1649 s / it)
[15:17:10.977204] Syncing meters...
[15:17:11.839917] Averaged stats: lr: 0.0000 (0.0000)  loss: 1006.8777 (1037.6939)  grad_norm: 667.1482 (688.7867)  amp_scale: 1.0000 (1.0000)
[15:17:13.897455] [Eval][Ep-90/100]  [  0/121]  eta: 0:04:07  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0493  data: 1.8888  max mem: 13835
[15:18:57.666784] [Eval][Ep-90/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9204  data: 0.7608  max mem: 13835
[15:19:15.618778] [Eval][Ep-90/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8973  data: 0.7410  max mem: 13835
[15:19:15.619636] [Eval][Ep-90/100] Total time: 0:02:03 (1.0229 s / it)
[15:19:16.602522] [Eval][Ep-90/100] val_acc1_image=27.05 | val_acc1_audio=42.39 | val_acc1_fusion=38.96 | val_acc1_all=52.76
[15:19:25.243199] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 91)
[15:19:27.026348] [Train][Ep-91/100]  [   0/1435]  eta: 0:42:27  lr: 0.0000 (0.0000)  time: 1.7749  data: 1.2968  max mem: 13835
[15:21:24.737922] [Train][Ep-91/100]  [ 100/1435]  eta: 0:26:19  lr: 0.0000 (0.0000)  loss: 1058.3104 (1060.0267)  grad_norm: 699.7507 (693.0439)  amp_scale: 1.0000 (1.0000)  time: 1.1350  data: 0.5645  max mem: 13835
[15:23:26.600955] [Train][Ep-91/100]  [ 200/1435]  eta: 0:24:42  lr: 0.0000 (0.0000)  loss: 1021.3115 (1052.0068)  grad_norm: 681.0566 (689.5035)  amp_scale: 1.0000 (1.0000)  time: 1.2333  data: 0.6626  max mem: 13835
[15:25:23.491809] [Train][Ep-91/100]  [ 300/1435]  eta: 0:22:30  lr: 0.0000 (0.0000)  loss: 1019.3780 (1041.0345)  grad_norm: 693.1761 (689.1814)  amp_scale: 1.0000 (1.0000)  time: 1.1717  data: 0.0004  max mem: 13835
[15:27:22.446396] [Train][Ep-91/100]  [ 400/1435]  eta: 0:20:31  lr: 0.0000 (0.0000)  loss: 1040.8284 (1038.8478)  grad_norm: 661.2055 (686.3625)  amp_scale: 1.0000 (1.0000)  time: 1.1569  data: 0.3322  max mem: 13835
[15:29:23.325293] [Train][Ep-91/100]  [ 500/1435]  eta: 0:18:36  lr: 0.0000 (0.0000)  loss: 1032.9207 (1038.6778)  grad_norm: 682.3907 (688.5326)  amp_scale: 1.0000 (1.0000)  time: 1.2623  data: 0.6918  max mem: 13835
[15:31:20.353300] [Train][Ep-91/100]  [ 600/1435]  eta: 0:16:33  lr: 0.0000 (0.0000)  loss: 1042.1659 (1040.3096)  grad_norm: 702.3462 (690.3302)  amp_scale: 1.0000 (1.0000)  time: 1.1208  data: 0.5530  max mem: 13835
[15:33:20.860335] [Train][Ep-91/100]  [ 700/1435]  eta: 0:14:35  lr: 0.0000 (0.0000)  loss: 1027.4221 (1039.4649)  grad_norm: 693.5027 (689.7125)  amp_scale: 1.0000 (1.0000)  time: 1.1199  data: 0.5512  max mem: 13835
[15:35:18.632615] [Train][Ep-91/100]  [ 800/1435]  eta: 0:12:35  lr: 0.0000 (0.0000)  loss: 1020.1177 (1038.2862)  grad_norm: 656.9213 (687.2891)  amp_scale: 1.0000 (1.0000)  time: 1.2105  data: 0.6391  max mem: 13835
[15:37:13.602018] [Train][Ep-91/100]  [ 900/1435]  eta: 0:10:34  lr: 0.0000 (0.0000)  loss: 1018.9918 (1038.2151)  grad_norm: 689.8151 (687.7284)  amp_scale: 1.0000 (1.0000)  time: 1.2133  data: 0.1572  max mem: 13835
[15:39:11.152342] [Train][Ep-91/100]  [1000/1435]  eta: 0:08:35  lr: 0.0000 (0.0000)  loss: 1016.1370 (1037.8871)  grad_norm: 687.3805 (688.1759)  amp_scale: 1.0000 (1.0000)  time: 1.1224  data: 0.5209  max mem: 13835
[15:41:09.600561] [Train][Ep-91/100]  [1100/1435]  eta: 0:06:36  lr: 0.0000 (0.0000)  loss: 1046.8663 (1038.7738)  grad_norm: 671.2575 (687.1574)  amp_scale: 1.0000 (1.0000)  time: 1.1742  data: 0.0003  max mem: 13835
[15:43:08.041255] [Train][Ep-91/100]  [1200/1435]  eta: 0:04:38  lr: 0.0000 (0.0000)  loss: 1029.7483 (1038.6521)  grad_norm: 681.3149 (686.6932)  amp_scale: 1.0000 (1.0000)  time: 1.0964  data: 0.2561  max mem: 13835
[15:45:03.009251] [Train][Ep-91/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1030.3435 (1037.7177)  grad_norm: 704.2490 (687.5130)  amp_scale: 1.0000 (1.0000)  time: 1.2016  data: 0.2767  max mem: 13835
[15:46:57.302832] [Train][Ep-91/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1015.4683 (1036.4219)  grad_norm: 677.1710 (687.4269)  amp_scale: 1.0000 (1.0000)  time: 1.1815  data: 0.0014  max mem: 13835
[15:47:36.060065] [Train][Ep-91/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1003.2601 (1036.3582)  grad_norm: 680.5489 (687.3858)  amp_scale: 1.0000 (1.0000)  time: 1.1617  data: 0.0032  max mem: 13835
[15:47:36.061097] [Train][Ep-91/100] Total time: 0:28:10 (1.1783 s / it)
[15:47:36.061642] Syncing meters...
[15:47:37.180804] Averaged stats: lr: 0.0000 (0.0000)  loss: 1003.2601 (1038.4661)  grad_norm: 680.5489 (687.3858)  amp_scale: 1.0000 (1.0000)
[15:47:47.306091] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 92)
[15:47:49.170410] [Train][Ep-92/100]  [   0/1435]  eta: 0:44:23  lr: 0.0000 (0.0000)  time: 1.8560  data: 1.3721  max mem: 13835
[15:49:50.524733] [Train][Ep-92/100]  [ 100/1435]  eta: 0:27:08  lr: 0.0000 (0.0000)  loss: 1057.2474 (1066.6517)  grad_norm: 683.0812 (678.5123)  amp_scale: 1.0000 (1.0000)  time: 1.2444  data: 0.6671  max mem: 13835
[15:51:48.454718] [Train][Ep-92/100]  [ 200/1435]  eta: 0:24:41  lr: 0.0000 (0.0000)  loss: 1010.6580 (1045.1887)  grad_norm: 677.3713 (680.9049)  amp_scale: 1.0000 (1.0000)  time: 1.1822  data: 0.4129  max mem: 13835
[15:53:42.207712] [Train][Ep-92/100]  [ 300/1435]  eta: 0:22:18  lr: 0.0000 (0.0000)  loss: 1019.6140 (1047.2678)  grad_norm: 675.4378 (683.5288)  amp_scale: 1.0000 (1.0000)  time: 1.1414  data: 0.1644  max mem: 13835
[15:55:42.766548] [Train][Ep-92/100]  [ 400/1435]  eta: 0:20:27  lr: 0.0000 (0.0000)  loss: 1054.7814 (1050.6206)  grad_norm: 679.1863 (684.6030)  amp_scale: 1.0000 (1.0000)  time: 1.2008  data: 0.0711  max mem: 13835
[15:57:44.154703] [Train][Ep-92/100]  [ 500/1435]  eta: 0:18:33  lr: 0.0000 (0.0000)  loss: 1010.6891 (1051.2749)  grad_norm: 691.5418 (684.8245)  amp_scale: 1.0000 (1.0000)  time: 1.1976  data: 0.0942  max mem: 13835
[15:59:45.882406] [Train][Ep-92/100]  [ 600/1435]  eta: 0:16:38  lr: 0.0000 (0.0000)  loss: 1070.5314 (1053.5464)  grad_norm: 684.3048 (687.2246)  amp_scale: 1.0000 (1.0000)  time: 1.2385  data: 0.0083  max mem: 13835
[16:01:45.519780] [Train][Ep-92/100]  [ 700/1435]  eta: 0:14:38  lr: 0.0000 (0.0000)  loss: 1052.9805 (1051.7282)  grad_norm: 688.4907 (685.7864)  amp_scale: 1.0000 (1.0000)  time: 1.1601  data: 0.4002  max mem: 13835
[16:03:39.386384] [Train][Ep-92/100]  [ 800/1435]  eta: 0:12:34  lr: 0.0000 (0.0000)  loss: 1016.1696 (1048.3396)  grad_norm: 674.8895 (685.3338)  amp_scale: 1.0000 (1.0000)  time: 1.0901  data: 0.2543  max mem: 13835
[16:05:40.140157] [Train][Ep-92/100]  [ 900/1435]  eta: 0:10:36  lr: 0.0000 (0.0000)  loss: 1014.9756 (1045.7902)  grad_norm: 670.2061 (684.0826)  amp_scale: 1.0000 (1.0000)  time: 1.1897  data: 0.3745  max mem: 13835
[16:07:41.625595] [Train][Ep-92/100]  [1000/1435]  eta: 0:08:38  lr: 0.0000 (0.0000)  loss: 1075.2240 (1047.4346)  grad_norm: 685.4911 (685.5615)  amp_scale: 1.0000 (1.0000)  time: 1.2308  data: 0.1851  max mem: 13835
[16:09:40.104963] [Train][Ep-92/100]  [1100/1435]  eta: 0:06:39  lr: 0.0000 (0.0000)  loss: 1006.4642 (1045.2126)  grad_norm: 668.0442 (684.2597)  amp_scale: 1.0000 (1.0000)  time: 1.1882  data: 0.4305  max mem: 13835
[16:11:39.801074] [Train][Ep-92/100]  [1200/1435]  eta: 0:04:40  lr: 0.0000 (0.0000)  loss: 1047.8424 (1044.0918)  grad_norm: 667.2503 (683.1438)  amp_scale: 1.0000 (1.0000)  time: 1.3017  data: 0.1237  max mem: 13835
[16:13:37.735427] [Train][Ep-92/100]  [1300/1435]  eta: 0:02:40  lr: 0.0000 (0.0000)  loss: 1040.0382 (1043.6111)  grad_norm: 648.7163 (682.2448)  amp_scale: 1.0000 (1.0000)  time: 1.2739  data: 0.0090  max mem: 13835
[16:15:42.338688] [Train][Ep-92/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1017.2565 (1042.5260)  grad_norm: 674.6912 (682.6439)  amp_scale: 1.0000 (1.0000)  time: 1.2663  data: 0.0676  max mem: 13835
[16:16:21.493328] [Train][Ep-92/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1060.3232 (1043.0004)  grad_norm: 676.5320 (682.8493)  amp_scale: 1.0000 (1.0000)  time: 1.2035  data: 0.0485  max mem: 13835
[16:16:21.494291] [Train][Ep-92/100] Total time: 0:28:34 (1.1946 s / it)
[16:16:21.494792] Syncing meters...
[16:16:21.525473] Averaged stats: lr: 0.0000 (0.0000)  loss: 1060.3232 (1039.3157)  grad_norm: 676.5320 (682.8493)  amp_scale: 1.0000 (1.0000)
[16:16:29.945568] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 93)
[16:16:32.262487] [Train][Ep-93/100]  [   0/1435]  eta: 0:55:13  lr: 0.0000 (0.0000)  time: 2.3089  data: 1.8247  max mem: 13835
[16:18:27.987525] [Train][Ep-93/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0000 (0.0000)  loss: 1028.7723 (1059.6512)  grad_norm: 693.0610 (695.2351)  amp_scale: 1.0000 (1.0000)  time: 1.2064  data: 0.6397  max mem: 13835
[16:20:22.420916] [Train][Ep-93/100]  [ 200/1435]  eta: 0:23:48  lr: 0.0000 (0.0000)  loss: 1043.2965 (1052.4677)  grad_norm: 692.4954 (691.3307)  amp_scale: 1.0000 (1.0000)  time: 1.1150  data: 0.5467  max mem: 13835
[16:22:18.461369] [Train][Ep-93/100]  [ 300/1435]  eta: 0:21:54  lr: 0.0000 (0.0000)  loss: 1031.5286 (1042.5353)  grad_norm: 683.2239 (689.8266)  amp_scale: 1.0000 (1.0000)  time: 1.1838  data: 0.2078  max mem: 13835
[16:24:15.825117] [Train][Ep-93/100]  [ 400/1435]  eta: 0:20:02  lr: 0.0000 (0.0000)  loss: 1067.7140 (1050.2239)  grad_norm: 673.2133 (687.1666)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.2085  max mem: 13835
[16:26:06.282680] [Train][Ep-93/100]  [ 500/1435]  eta: 0:17:55  lr: 0.0000 (0.0000)  loss: 1002.6200 (1042.5280)  grad_norm: 684.6287 (689.0856)  amp_scale: 1.0000 (1.0000)  time: 1.0926  data: 0.2805  max mem: 13835
[16:27:59.819079] [Train][Ep-93/100]  [ 600/1435]  eta: 0:15:58  lr: 0.0000 (0.0000)  loss: 1036.4144 (1039.9794)  grad_norm: 687.0436 (688.5356)  amp_scale: 1.0000 (1.0000)  time: 1.1803  data: 0.0975  max mem: 13835
[16:29:54.471297] [Train][Ep-93/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0000 (0.0000)  loss: 1024.3550 (1038.1429)  grad_norm: 692.0770 (688.6133)  amp_scale: 1.0000 (1.0000)  time: 1.1492  data: 0.0012  max mem: 13835
[16:31:47.931326] [Train][Ep-93/100]  [ 800/1435]  eta: 0:12:07  lr: 0.0000 (0.0000)  loss: 1019.9468 (1038.3308)  grad_norm: 685.5335 (688.6297)  amp_scale: 1.0000 (1.0000)  time: 1.1123  data: 0.3898  max mem: 13835
[16:33:41.568133] [Train][Ep-93/100]  [ 900/1435]  eta: 0:10:12  lr: 0.0000 (0.0000)  loss: 1029.0623 (1037.7890)  grad_norm: 675.2800 (688.3026)  amp_scale: 1.0000 (1.0000)  time: 1.1413  data: 0.0045  max mem: 13835
[16:35:37.510918] [Train][Ep-93/100]  [1000/1435]  eta: 0:08:18  lr: 0.0000 (0.0000)  loss: 1023.0925 (1036.6743)  grad_norm: 681.0762 (687.4295)  amp_scale: 1.0000 (1.0000)  time: 1.1296  data: 0.0005  max mem: 13835
[16:37:33.256533] [Train][Ep-93/100]  [1100/1435]  eta: 0:06:24  lr: 0.0000 (0.0000)  loss: 998.5464 (1036.6411)  grad_norm: 683.3638 (687.2253)  amp_scale: 1.0000 (1.0000)  time: 1.2039  data: 0.0061  max mem: 13835
[16:39:32.807371] [Train][Ep-93/100]  [1200/1435]  eta: 0:04:30  lr: 0.0000 (0.0000)  loss: 1017.3079 (1036.8920)  grad_norm: 680.1188 (687.2577)  amp_scale: 1.0000 (1.0000)  time: 1.2072  data: 0.0781  max mem: 13835
[16:41:30.638579] [Train][Ep-93/100]  [1300/1435]  eta: 0:02:35  lr: 0.0000 (0.0000)  loss: 1039.8811 (1037.0755)  grad_norm: 671.2246 (687.9321)  amp_scale: 1.0000 (1.0000)  time: 1.2132  data: 0.0485  max mem: 13835
[16:43:28.143159] [Train][Ep-93/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1039.6119 (1038.7855)  grad_norm: 683.9150 (688.1719)  amp_scale: 1.0000 (1.0000)  time: 1.1692  data: 0.1047  max mem: 13835
[16:44:08.111841] [Train][Ep-93/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1039.6119 (1038.9768)  grad_norm: 665.5785 (687.6130)  amp_scale: 1.0000 (1.0000)  time: 1.2549  data: 0.0413  max mem: 13835
[16:44:08.112705] [Train][Ep-93/100] Total time: 0:27:38 (1.1555 s / it)
[16:44:08.113144] Syncing meters...
[16:44:08.924209] Averaged stats: lr: 0.0000 (0.0000)  loss: 1039.6119 (1040.6151)  grad_norm: 665.5785 (687.6130)  amp_scale: 1.0000 (1.0000)
[16:44:19.093803] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 94)
[16:44:20.916597] [Train][Ep-94/100]  [   0/1435]  eta: 0:43:23  lr: 0.0000 (0.0000)  time: 1.8141  data: 1.3298  max mem: 13835
[16:46:21.890447] [Train][Ep-94/100]  [ 100/1435]  eta: 0:27:02  lr: 0.0000 (0.0000)  loss: 1002.2646 (1017.2102)  grad_norm: 670.2369 (677.9366)  amp_scale: 1.0000 (1.0000)  time: 1.2366  data: 0.6489  max mem: 13835
[16:48:20.365745] [Train][Ep-94/100]  [ 200/1435]  eta: 0:24:42  lr: 0.0000 (0.0000)  loss: 1008.0977 (1027.9484)  grad_norm: 675.3788 (682.3818)  amp_scale: 1.0000 (1.0000)  time: 1.1773  data: 0.6053  max mem: 13835
[16:50:15.999476] [Train][Ep-94/100]  [ 300/1435]  eta: 0:22:25  lr: 0.0000 (0.0000)  loss: 1014.4496 (1025.2106)  grad_norm: 676.0103 (686.9278)  amp_scale: 1.0000 (1.0000)  time: 1.1325  data: 0.3635  max mem: 13835
[16:52:11.929166] [Train][Ep-94/100]  [ 400/1435]  eta: 0:20:20  lr: 0.0000 (0.0000)  loss: 1065.4513 (1032.1317)  grad_norm: 685.8506 (686.8369)  amp_scale: 1.0000 (1.0000)  time: 1.1462  data: 0.0005  max mem: 13835
[16:54:04.409999] [Train][Ep-94/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0000 (0.0000)  loss: 997.8099 (1033.6481)  grad_norm: 689.6369 (685.6572)  amp_scale: 1.0000 (1.0000)  time: 1.1432  data: 0.0629  max mem: 13835
[16:56:02.698703] [Train][Ep-94/100]  [ 600/1435]  eta: 0:16:17  lr: 0.0000 (0.0000)  loss: 1040.5801 (1033.3222)  grad_norm: 680.6720 (686.1425)  amp_scale: 1.0000 (1.0000)  time: 1.2227  data: 0.2743  max mem: 13835
[16:58:00.604886] [Train][Ep-94/100]  [ 700/1435]  eta: 0:14:21  lr: 0.0000 (0.0000)  loss: 1029.2286 (1033.4133)  grad_norm: 675.6702 (686.5044)  amp_scale: 1.0000 (1.0000)  time: 1.1919  data: 0.2458  max mem: 13835
[16:59:55.023101] [Train][Ep-94/100]  [ 800/1435]  eta: 0:12:21  lr: 0.0000 (0.0000)  loss: 1017.9693 (1033.3765)  grad_norm: 670.7971 (684.8105)  amp_scale: 1.0000 (1.0000)  time: 1.0774  data: 0.4861  max mem: 13835
[17:01:47.302378] [Train][Ep-94/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0000 (0.0000)  loss: 1008.0033 (1031.9066)  grad_norm: 680.5552 (684.8359)  amp_scale: 1.0000 (1.0000)  time: 1.0933  data: 0.4776  max mem: 13835
[17:03:42.455856] [Train][Ep-94/100]  [1000/1435]  eta: 0:08:25  lr: 0.0000 (0.0000)  loss: 1047.7445 (1033.3282)  grad_norm: 715.3280 (686.6351)  amp_scale: 1.0000 (1.0000)  time: 1.2059  data: 0.2188  max mem: 13835
[17:05:36.919611] [Train][Ep-94/100]  [1100/1435]  eta: 0:06:28  lr: 0.0000 (0.0000)  loss: 1023.7452 (1033.9005)  grad_norm: 683.6233 (687.4306)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.5298  max mem: 13835
[17:07:35.835346] [Train][Ep-94/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1010.1006 (1032.9840)  grad_norm: 676.2141 (687.0490)  amp_scale: 1.0000 (1.0000)  time: 1.1960  data: 0.1088  max mem: 13835
[17:09:31.640993] [Train][Ep-94/100]  [1300/1435]  eta: 0:02:36  lr: 0.0000 (0.0000)  loss: 1041.2435 (1033.3227)  grad_norm: 665.5724 (686.5855)  amp_scale: 1.0000 (1.0000)  time: 1.1524  data: 0.1197  max mem: 13835
[17:13:36.002511] job dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound
[17:13:36.003368] env.world_size: 4
[17:13:36.003679] env.rank: 0
[17:13:36.003936] env.dist_url: tcp://localhost:50001
[17:13:36.004177] env.dist_backend: nccl
[17:13:36.004466] env.port: 50001
[17:13:36.004713] env.node: localhost
[17:13:36.004936] env.distributed: True
[17:13:36.005173] env.seed: None
[17:13:36.005428] env.gpu: None
[17:13:36.005668] env.ngpu: 4
[17:13:36.005888] env.mem_gb: 240
[17:13:36.006122] env.workers: 8
[17:13:36.006417] env.slurm: True
[17:13:36.006691] env.slurm_suffix: 
[17:13:36.006986] env.slurm_partition: NORMAL
[17:13:36.007224] env.slurm_comment: DeepAVFusion
[17:13:36.007455] env.slurm_timeout: 1440
[17:13:36.007706] env.nodelist: node14
[17:13:36.007980] env.exclude: 
[17:13:36.008243] log.print_freq: 100
[17:13:36.008482] log.save_freq: 50
[17:13:36.008726] log.eval_freq: 10
[17:13:36.008939] log.wandb_watch_freq: 0
[17:13:36.009150] log.debug: False
[17:13:36.009381] log.use_wandb: True
[17:13:36.009614] log.wandb_entity: audiovisual_diagnostics
[17:13:36.009980] log.wandb_project: efav
[17:13:36.010599] worker: eval_finetune
[17:13:36.010850] output_dir: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200
[17:13:36.011090] job_name: finetune_vggsound
[17:13:36.011380] pretrain_job_name: deepavfusion_vitb_vggsound_ep200
[17:13:36.011649] checkpoint: None
[17:13:36.011900] encoder_prefix: encoder.
[17:13:36.012158] pretrain_resume_epoch: latest
[17:13:36.012417] eval: False
[17:13:36.012669] debug: False
[17:13:36.012980] model.image.backbone: vit_base
[17:13:36.013235] model.image.pretrained: vit_base_mae_in1k
[17:13:36.013508] model.audio.backbone: vit_base
[17:13:36.013762] model.audio.pretrained: vit_base_audiomae_as2m
[17:13:36.014121] model.fusion.arch: factorized_mmi
[17:13:36.014369] model.fusion.layers: all
[17:13:36.014598] model.fusion.num_fusion_tkns: 16
[17:13:36.014822] model.fusion.num_aggr_image_tkns: 8
[17:13:36.015046] model.fusion.num_aggr_audio_tkns: 8
[17:13:36.015298] model.fusion.mlp_ratio: 1.0
[17:13:36.015534] model.fusion.attn_ratio: 0.25
[17:13:36.015786] model.fusion.num_heads: 12
[17:13:36.016056] opt.resume: True
[17:13:36.016282] opt.joint_loss: True
[17:13:36.016506] opt.batch_size: 32
[17:13:36.016734] opt.epochs: 100
[17:13:36.016958] opt.warmup_epochs: 20
[17:13:36.017182] opt.accum_iter: 4
[17:13:36.017439] opt.clip_grad: None
[17:13:36.017666] opt.weight_decay: 0.05
[17:13:36.017911] opt.layer_decay: 0.75
[17:13:36.018167] opt.smoothing: 0.1
[17:13:36.018400] opt.lr: None
[17:13:36.018645] opt.blr: 0.0003
[17:13:36.018912] opt.min_lr: 0.0
[17:13:36.019292] opt.drop_path: 0.2
[17:13:36.019542] opt.attn_drop: 0.0
[17:13:36.019799] opt.proj_drop: 0.0
[17:13:36.020026] opt.use_amp: False
[17:13:36.020292] data.dataset: vggsound
[17:13:36.020546] data.data_path: /tmp/zverev/vggsound
[17:13:36.020792] data.audio_rate: 16000
[17:13:36.021034] data.audio_dur: 3.0
[17:13:36.021783] data.audio_mels: 128
[17:13:36.022080] data.image_size: 224
[17:13:36.022707] data.crop_min: 0.5
[17:13:36.023706] data.mixup: 1.0
[17:13:36.024463] data.cutmix: 0.0
[17:13:36.024923] data.cutmix_minmax: None
[17:13:36.025171] data.mixup_prob: 1.0
[17:13:36.025414] data.mixup_switch_prob: 0.5
[17:13:36.025645] data.mixup_mode: batch
[17:13:36.026248] base lr: 3.00e-04
[17:13:36.026555] actual lr: 6.00e-04
[17:13:36.026835] accumulate grad iterations: 4
[17:13:36.027052] effective batch size: 512
[17:13:50.815229] Video CN7qu_1k3F8 not found
[17:14:04.165115] Video RflAC1ror3c not found
[17:14:29.988890] Video wvHLlBCYe8c not found
[17:14:34.159818] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 183727
 - Num classes: 310
[17:14:39.588951] VideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:45.030287] DenseVideoDataset
  - Path: /tmp/zverev/vggsound/video
  - No Samples: 15445
 - Num classes: 310
[17:14:45.030931] Mixup is activated!
[17:14:45.992062] Loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth
[17:14:45.992681] Error loading checkpoint from https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth: [Errno 2] No such file or directory: 'https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth' 
 Loading from torch hub ...
[17:14:47.896348] Loading checkpoint from assets/models/vitbase_audiomae_as2m.pth
[17:14:52.696360] Model = AVClassifier(
  (encoder): DeepAVFusion(
    (image): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (audio): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
    (fusion_blocks): ModuleList(
      (0-11): 12 x FusionBlock_FactorizedAVInteractions(
        (norm1_mm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_aud): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm1_img): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): CrossAttention_FactorizedAVInteractions(
          (attn_v): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (attn_a): CrossAttention(
            (q): Linear(in_features=768, out_features=768, bias=True)
            (kv): Linear(in_features=768, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (q): Linear(in_features=768, out_features=192, bias=True)
          (k): Linear(in_features=1536, out_features=192, bias=True)
          (v): Linear(in_features=1536, out_features=768, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=768, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (fusion_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (image_head): Linear(in_features=768, out_features=310, bias=True)
  (audio_head): Linear(in_features=768, out_features=310, bias=True)
  (fusion_head): Linear(in_features=768, out_features=310, bias=True)
)
[17:14:57.045768] Loaded pre-trained checkpoint: /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/checkpoints/checkpoint_latest.pth
[17:14:57.079670] criterion = SoftTargetCrossEntropy()
[17:14:57.127829] Loading /home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth
[17:15:16.646454] => loaded checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 94)
[17:15:19.183868] Start training for 100 epochs
[17:15:27.854103] [Train][Ep-94/100]  [   0/1435]  eta: 3:27:14  lr: 0.0000 (0.0000)  time: 8.6653  data: 7.6423  max mem: 12727
[17:17:54.036033] [Train][Ep-94/100]  [ 100/1435]  eta: 0:34:06  lr: 0.0000 (0.0000)  loss: 1028.7607 (1054.4064)  grad_norm: 681.0569 (686.5943)  amp_scale: 1.0000 (1.0000)  time: 1.4609  data: 0.8992  max mem: 13835
[17:20:09.821856] [Train][Ep-94/100]  [ 200/1435]  eta: 0:29:45  lr: 0.0000 (0.0000)  loss: 1049.4426 (1043.4536)  grad_norm: 663.4958 (679.1235)  amp_scale: 1.0000 (1.0000)  time: 1.3252  data: 0.7522  max mem: 13835
[17:22:23.845041] [Train][Ep-94/100]  [ 300/1435]  eta: 0:26:41  lr: 0.0000 (0.0000)  loss: 1037.7882 (1047.8117)  grad_norm: 689.3660 (685.1084)  amp_scale: 1.0000 (1.0000)  time: 1.3558  data: 0.6924  max mem: 13835
[17:24:40.480697] [Train][Ep-94/100]  [ 400/1435]  eta: 0:24:08  lr: 0.0000 (0.0000)  loss: 1017.7035 (1044.0492)  grad_norm: 672.9575 (684.4746)  amp_scale: 1.0000 (1.0000)  time: 1.3626  data: 0.7401  max mem: 13835
[17:26:53.997654] [Train][Ep-94/100]  [ 500/1435]  eta: 0:21:36  lr: 0.0000 (0.0000)  loss: 1013.9903 (1038.6376)  grad_norm: 679.4110 (684.7640)  amp_scale: 1.0000 (1.0000)  time: 1.3409  data: 0.7244  max mem: 13835
[17:29:04.276945] [Train][Ep-94/100]  [ 600/1435]  eta: 0:19:06  lr: 0.0000 (0.0000)  loss: 1053.2765 (1037.8641)  grad_norm: 676.3755 (683.0225)  amp_scale: 1.0000 (1.0000)  time: 1.3421  data: 0.7755  max mem: 13835
[17:31:12.774705] [Train][Ep-94/100]  [ 700/1435]  eta: 0:16:39  lr: 0.0000 (0.0000)  loss: 1020.3615 (1038.0825)  grad_norm: 675.7573 (682.2139)  amp_scale: 1.0000 (1.0000)  time: 1.3128  data: 0.2851  max mem: 13835
[17:33:25.951828] [Train][Ep-94/100]  [ 800/1435]  eta: 0:14:21  lr: 0.0000 (0.0000)  loss: 1024.1042 (1039.6793)  grad_norm: 670.0941 (683.2114)  amp_scale: 1.0000 (1.0000)  time: 1.3411  data: 0.7756  max mem: 13835
[17:35:36.963566] [Train][Ep-94/100]  [ 900/1435]  eta: 0:12:03  lr: 0.0000 (0.0000)  loss: 1028.9525 (1039.5040)  grad_norm: 673.8909 (683.4460)  amp_scale: 1.0000 (1.0000)  time: 1.3139  data: 0.7485  max mem: 13835
[17:37:42.987093] [Train][Ep-94/100]  [1000/1435]  eta: 0:09:43  lr: 0.0000 (0.0000)  loss: 1007.3251 (1038.3564)  grad_norm: 686.1594 (683.3431)  amp_scale: 1.0000 (1.0000)  time: 1.3104  data: 0.2118  max mem: 13835
[17:39:54.709505] [Train][Ep-94/100]  [1100/1435]  eta: 0:07:28  lr: 0.0000 (0.0000)  loss: 1022.4802 (1037.1532)  grad_norm: 674.0745 (683.0396)  amp_scale: 1.0000 (1.0000)  time: 1.3736  data: 0.0248  max mem: 13835
[17:42:05.307090] [Train][Ep-94/100]  [1200/1435]  eta: 0:05:14  lr: 0.0000 (0.0000)  loss: 1024.8801 (1036.3813)  grad_norm: 680.4291 (683.9666)  amp_scale: 1.0000 (1.0000)  time: 1.2525  data: 0.4414  max mem: 13835
[17:44:23.054300] [Train][Ep-94/100]  [1300/1435]  eta: 0:03:00  lr: 0.0000 (0.0000)  loss: 1012.8833 (1036.6298)  grad_norm: 699.0513 (684.8097)  amp_scale: 1.0000 (1.0000)  time: 1.3754  data: 0.1276  max mem: 13835
[17:46:30.916233] [Train][Ep-94/100]  [1400/1435]  eta: 0:00:46  lr: 0.0000 (0.0000)  loss: 1021.9155 (1036.5814)  grad_norm: 674.6180 (685.4792)  amp_scale: 1.0000 (1.0000)  time: 1.2236  data: 0.4120  max mem: 13835
[17:47:11.859606] [Train][Ep-94/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1020.7795 (1036.1205)  grad_norm: 658.8064 (685.2466)  amp_scale: 1.0000 (1.0000)  time: 1.1919  data: 0.2810  max mem: 13835
[17:47:11.860558] [Train][Ep-94/100] Total time: 0:31:52 (1.3329 s / it)
[17:47:11.860995] Syncing meters...
[17:47:13.347129] Averaged stats: lr: 0.0000 (0.0000)  loss: 1020.7795 (1040.6980)  grad_norm: 658.8064 (685.2466)  amp_scale: 1.0000 (1.0000)
[17:47:19.138814] [Eval][Ep-94/100]  [  0/121]  eta: 0:11:39  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 5.7837  data: 5.6236  max mem: 13835
[17:49:02.333060] [Eval][Ep-94/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9435  data: 0.7841  max mem: 13835
[17:49:20.359650] [Eval][Ep-94/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9011  data: 0.7445  max mem: 13835
[17:49:20.360452] [Eval][Ep-94/100] Total time: 0:02:07 (1.0496 s / it)
[17:49:20.669364] [Eval][Ep-94/100] val_acc1_image=26.83 | val_acc1_audio=42.46 | val_acc1_fusion=39.16 | val_acc1_all=52.88
[17:49:29.325994] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 95)
[17:49:31.291735] [Train][Ep-95/100]  [   0/1435]  eta: 0:46:48  lr: 0.0000 (0.0000)  time: 1.9571  data: 1.4762  max mem: 13835
[17:51:35.631785] [Train][Ep-95/100]  [ 100/1435]  eta: 0:27:49  lr: 0.0000 (0.0000)  loss: 1036.7585 (1040.1829)  grad_norm: 678.0992 (688.0612)  amp_scale: 1.0000 (1.0000)  time: 1.2964  data: 0.7306  max mem: 13835
[17:53:46.896012] [Train][Ep-95/100]  [ 200/1435]  eta: 0:26:22  lr: 0.0000 (0.0000)  loss: 1010.6780 (1038.1304)  grad_norm: 690.4009 (689.1255)  amp_scale: 1.0000 (1.0000)  time: 1.3738  data: 0.0364  max mem: 13835
[17:56:00.210702] [Train][Ep-95/100]  [ 300/1435]  eta: 0:24:33  lr: 0.0000 (0.0000)  loss: 1006.1030 (1031.8504)  grad_norm: 685.2559 (686.8698)  amp_scale: 1.0000 (1.0000)  time: 1.2718  data: 0.1503  max mem: 13835
[17:58:17.920381] [Train][Ep-95/100]  [ 400/1435]  eta: 0:22:44  lr: 0.0000 (0.0000)  loss: 1065.9679 (1030.0822)  grad_norm: 685.8933 (687.8291)  amp_scale: 1.0000 (1.0000)  time: 1.3463  data: 0.2167  max mem: 13835
[18:00:35.754488] [Train][Ep-95/100]  [ 500/1435]  eta: 0:20:43  lr: 0.0000 (0.0000)  loss: 1018.9875 (1031.2920)  grad_norm: 688.1768 (687.0040)  amp_scale: 1.0000 (1.0000)  time: 1.4352  data: 0.0188  max mem: 13835
[18:02:52.681769] [Train][Ep-95/100]  [ 600/1435]  eta: 0:18:36  lr: 0.0000 (0.0000)  loss: 1032.4274 (1031.0167)  grad_norm: 674.0734 (685.7610)  amp_scale: 1.0000 (1.0000)  time: 1.3591  data: 0.0009  max mem: 13835
[18:05:02.651755] [Train][Ep-95/100]  [ 700/1435]  eta: 0:16:18  lr: 0.0000 (0.0000)  loss: 1011.3115 (1030.5948)  grad_norm: 682.8092 (686.3205)  amp_scale: 1.0000 (1.0000)  time: 1.3460  data: 0.0039  max mem: 13835
[18:07:18.059389] [Train][Ep-95/100]  [ 800/1435]  eta: 0:14:07  lr: 0.0000 (0.0000)  loss: 1032.3424 (1032.4453)  grad_norm: 705.4583 (687.1584)  amp_scale: 1.0000 (1.0000)  time: 1.3306  data: 0.0002  max mem: 13835
[18:09:29.317118] [Train][Ep-95/100]  [ 900/1435]  eta: 0:11:52  lr: 0.0000 (0.0000)  loss: 1027.3492 (1032.8554)  grad_norm: 694.9360 (687.9861)  amp_scale: 1.0000 (1.0000)  time: 1.2867  data: 0.0003  max mem: 13835
[18:11:43.823354] [Train][Ep-95/100]  [1000/1435]  eta: 0:09:39  lr: 0.0000 (0.0000)  loss: 1043.7090 (1031.5098)  grad_norm: 683.8582 (687.1800)  amp_scale: 1.0000 (1.0000)  time: 1.4090  data: 0.0935  max mem: 13835
[18:14:02.233555] [Train][Ep-95/100]  [1100/1435]  eta: 0:07:28  lr: 0.0000 (0.0000)  loss: 1030.6921 (1031.6016)  grad_norm: 675.9677 (687.3771)  amp_scale: 1.0000 (1.0000)  time: 1.3238  data: 0.0216  max mem: 13835
[18:16:18.369406] [Train][Ep-95/100]  [1200/1435]  eta: 0:05:14  lr: 0.0000 (0.0000)  loss: 1046.4640 (1032.2312)  grad_norm: 676.0037 (687.5843)  amp_scale: 1.0000 (1.0000)  time: 1.3962  data: 0.0233  max mem: 13835
[18:18:31.891748] [Train][Ep-95/100]  [1300/1435]  eta: 0:03:00  lr: 0.0000 (0.0000)  loss: 1030.3813 (1032.5005)  grad_norm: 680.7020 (687.0390)  amp_scale: 1.0000 (1.0000)  time: 1.3394  data: 0.0862  max mem: 13835
[18:20:43.780630] [Train][Ep-95/100]  [1400/1435]  eta: 0:00:46  lr: 0.0000 (0.0000)  loss: 1012.5472 (1030.9751)  grad_norm: 688.8481 (686.7969)  amp_scale: 1.0000 (1.0000)  time: 1.3438  data: 0.0283  max mem: 13835
[18:21:26.659890] [Train][Ep-95/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1021.0917 (1031.0701)  grad_norm: 682.4998 (686.7729)  amp_scale: 1.0000 (1.0000)  time: 1.2544  data: 0.0903  max mem: 13835
[18:21:26.660856] [Train][Ep-95/100] Total time: 0:31:57 (1.3361 s / it)
[18:21:26.661397] Syncing meters...
[18:21:28.681569] Averaged stats: lr: 0.0000 (0.0000)  loss: 1021.0917 (1037.1113)  grad_norm: 682.4998 (686.7729)  amp_scale: 1.0000 (1.0000)
[18:21:38.296228] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 96)
[18:21:40.295097] [Train][Ep-96/100]  [   0/1435]  eta: 0:47:35  lr: 0.0000 (0.0000)  time: 1.9899  data: 1.5076  max mem: 13835
[18:23:41.899967] [Train][Ep-96/100]  [ 100/1435]  eta: 0:27:13  lr: 0.0000 (0.0000)  loss: 1018.0555 (1019.9843)  grad_norm: 678.4828 (681.4550)  amp_scale: 1.0000 (1.0000)  time: 1.2596  data: 0.6846  max mem: 13835
[18:25:48.632595] [Train][Ep-96/100]  [ 200/1435]  eta: 0:25:38  lr: 0.0000 (0.0000)  loss: 983.4357 (1016.9156)  grad_norm: 685.6272 (683.1020)  amp_scale: 1.0000 (1.0000)  time: 1.1604  data: 0.5666  max mem: 13835
[18:27:56.762033] [Train][Ep-96/100]  [ 300/1435]  eta: 0:23:47  lr: 0.0000 (0.0000)  loss: 1031.7312 (1021.1039)  grad_norm: 692.6030 (685.9128)  amp_scale: 1.0000 (1.0000)  time: 1.2730  data: 0.0171  max mem: 13835
[18:29:58.637342] [Train][Ep-96/100]  [ 400/1435]  eta: 0:21:31  lr: 0.0000 (0.0000)  loss: 1013.2450 (1022.5338)  grad_norm: 683.5967 (688.3139)  amp_scale: 1.0000 (1.0000)  time: 1.2319  data: 0.1393  max mem: 13835
[18:32:08.092451] [Train][Ep-96/100]  [ 500/1435]  eta: 0:19:35  lr: 0.0000 (0.0000)  loss: 1064.6776 (1025.7540)  grad_norm: 665.0565 (684.2755)  amp_scale: 1.0000 (1.0000)  time: 1.2886  data: 0.1676  max mem: 13835
[18:34:13.900147] [Train][Ep-96/100]  [ 600/1435]  eta: 0:17:29  lr: 0.0000 (0.0000)  loss: 1073.5785 (1031.5852)  grad_norm: 683.4323 (684.4729)  amp_scale: 1.0000 (1.0000)  time: 1.2670  data: 0.0054  max mem: 13835
[18:36:19.439412] [Train][Ep-96/100]  [ 700/1435]  eta: 0:15:23  lr: 0.0000 (0.0000)  loss: 1063.1370 (1035.4648)  grad_norm: 694.6241 (685.8169)  amp_scale: 1.0000 (1.0000)  time: 1.3397  data: 0.1194  max mem: 13835
[18:38:24.212478] [Train][Ep-96/100]  [ 800/1435]  eta: 0:13:17  lr: 0.0000 (0.0000)  loss: 1047.0433 (1035.7473)  grad_norm: 693.8911 (688.0075)  amp_scale: 1.0000 (1.0000)  time: 1.2681  data: 0.7005  max mem: 13835
[18:40:26.877788] [Train][Ep-96/100]  [ 900/1435]  eta: 0:11:10  lr: 0.0000 (0.0000)  loss: 1030.3319 (1036.3236)  grad_norm: 680.3100 (687.2301)  amp_scale: 1.0000 (1.0000)  time: 1.2926  data: 0.0815  max mem: 13835
[18:42:41.040798] [Train][Ep-96/100]  [1000/1435]  eta: 0:09:08  lr: 0.0000 (0.0000)  loss: 1011.5995 (1035.8634)  grad_norm: 689.1064 (687.6296)  amp_scale: 1.0000 (1.0000)  time: 1.2999  data: 0.0033  max mem: 13835
[18:44:56.591665] [Train][Ep-96/100]  [1100/1435]  eta: 0:07:05  lr: 0.0000 (0.0000)  loss: 1065.3088 (1037.6304)  grad_norm: 691.0024 (687.3745)  amp_scale: 1.0000 (1.0000)  time: 1.3155  data: 0.0002  max mem: 13835
[18:47:08.602315] [Train][Ep-96/100]  [1200/1435]  eta: 0:04:59  lr: 0.0000 (0.0000)  loss: 1047.8236 (1038.8557)  grad_norm: 656.9741 (686.6238)  amp_scale: 1.0000 (1.0000)  time: 1.2987  data: 0.0177  max mem: 13835
[18:49:16.266645] [Train][Ep-96/100]  [1300/1435]  eta: 0:02:52  lr: 0.0000 (0.0000)  loss: 1030.1012 (1039.5999)  grad_norm: 671.0575 (686.3154)  amp_scale: 1.0000 (1.0000)  time: 1.2804  data: 0.0547  max mem: 13835
[18:51:20.720588] [Train][Ep-96/100]  [1400/1435]  eta: 0:00:44  lr: 0.0000 (0.0000)  loss: 1012.5652 (1038.8986)  grad_norm: 658.9276 (686.2085)  amp_scale: 1.0000 (1.0000)  time: 1.2459  data: 0.1147  max mem: 13835
[18:52:01.151002] [Train][Ep-96/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1012.5652 (1039.5191)  grad_norm: 658.9276 (685.9726)  amp_scale: 1.0000 (1.0000)  time: 1.2517  data: 0.0305  max mem: 13835
[18:52:01.151843] [Train][Ep-96/100] Total time: 0:30:22 (1.2703 s / it)
[18:52:01.152228] Syncing meters...
[18:52:01.995961] Averaged stats: lr: 0.0000 (0.0000)  loss: 1012.5652 (1036.3999)  grad_norm: 658.9276 (685.9726)  amp_scale: 1.0000 (1.0000)
[18:52:10.207177] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 97)
[18:52:12.149978] [Train][Ep-97/100]  [   0/1435]  eta: 0:46:15  lr: 0.0000 (0.0000)  time: 1.9342  data: 1.4511  max mem: 13835
[18:54:09.148418] [Train][Ep-97/100]  [ 100/1435]  eta: 0:26:11  lr: 0.0000 (0.0000)  loss: 1043.0754 (1045.4496)  grad_norm: 675.2723 (684.8699)  amp_scale: 1.0000 (1.0000)  time: 1.1217  data: 0.5469  max mem: 13835
[18:56:09.495793] [Train][Ep-97/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0000 (0.0000)  loss: 1042.1721 (1041.9486)  grad_norm: 682.6307 (691.7432)  amp_scale: 1.0000 (1.0000)  time: 1.1765  data: 0.6086  max mem: 13835
[18:58:08.636804] [Train][Ep-97/100]  [ 300/1435]  eta: 0:22:31  lr: 0.0000 (0.0000)  loss: 991.0350 (1036.6803)  grad_norm: 683.3366 (690.4965)  amp_scale: 1.0000 (1.0000)  time: 1.1999  data: 0.5041  max mem: 13835
[19:00:05.010523] [Train][Ep-97/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0000 (0.0000)  loss: 1038.5500 (1042.0074)  grad_norm: 668.2566 (688.0119)  amp_scale: 1.0000 (1.0000)  time: 1.2468  data: 0.3194  max mem: 13835
[19:02:12.994768] [Train][Ep-97/100]  [ 500/1435]  eta: 0:18:44  lr: 0.0000 (0.0000)  loss: 1031.4011 (1045.2177)  grad_norm: 676.6089 (687.3863)  amp_scale: 1.0000 (1.0000)  time: 1.2442  data: 0.2425  max mem: 13835
[19:04:16.154508] [Train][Ep-97/100]  [ 600/1435]  eta: 0:16:48  lr: 0.0000 (0.0000)  loss: 1043.2208 (1045.6318)  grad_norm: 687.7733 (686.8673)  amp_scale: 1.0000 (1.0000)  time: 1.1945  data: 0.0016  max mem: 13835
[19:06:22.903695] [Train][Ep-97/100]  [ 700/1435]  eta: 0:14:53  lr: 0.0000 (0.0000)  loss: 1022.9901 (1043.7469)  grad_norm: 679.6957 (686.7352)  amp_scale: 1.0000 (1.0000)  time: 1.3224  data: 0.1343  max mem: 13835
[19:08:31.586263] [Train][Ep-97/100]  [ 800/1435]  eta: 0:12:57  lr: 0.0000 (0.0000)  loss: 990.8336 (1040.9626)  grad_norm: 671.3866 (685.9539)  amp_scale: 1.0000 (1.0000)  time: 1.2707  data: 0.0797  max mem: 13835
[19:10:37.491825] [Train][Ep-97/100]  [ 900/1435]  eta: 0:10:57  lr: 0.0000 (0.0000)  loss: 1030.9816 (1041.3145)  grad_norm: 678.1453 (686.1097)  amp_scale: 1.0000 (1.0000)  time: 1.2405  data: 0.0276  max mem: 13835
[19:12:40.832470] [Train][Ep-97/100]  [1000/1435]  eta: 0:08:54  lr: 0.0000 (0.0000)  loss: 1037.7919 (1042.1860)  grad_norm: 686.5427 (686.4502)  amp_scale: 1.0000 (1.0000)  time: 1.2147  data: 0.3900  max mem: 13835
[19:14:44.649015] [Train][Ep-97/100]  [1100/1435]  eta: 0:06:52  lr: 0.0000 (0.0000)  loss: 1036.8619 (1043.4302)  grad_norm: 686.2928 (686.4355)  amp_scale: 1.0000 (1.0000)  time: 1.1606  data: 0.5923  max mem: 13835
[19:16:43.943744] [Train][Ep-97/100]  [1200/1435]  eta: 0:04:48  lr: 0.0000 (0.0000)  loss: 1033.9520 (1042.5373)  grad_norm: 675.2556 (685.4618)  amp_scale: 1.0000 (1.0000)  time: 1.1807  data: 0.6133  max mem: 13835
[19:18:51.984606] [Train][Ep-97/100]  [1300/1435]  eta: 0:02:46  lr: 0.0000 (0.0000)  loss: 1046.6976 (1042.4294)  grad_norm: 686.2825 (685.3928)  amp_scale: 1.0000 (1.0000)  time: 1.3706  data: 0.7995  max mem: 13835
[19:21:02.170851] [Train][Ep-97/100]  [1400/1435]  eta: 0:00:43  lr: 0.0000 (0.0000)  loss: 1038.8553 (1042.3885)  grad_norm: 677.8887 (685.1624)  amp_scale: 1.0000 (1.0000)  time: 1.3495  data: 0.7815  max mem: 13835
[19:21:41.972229] [Train][Ep-97/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1037.3129 (1041.7564)  grad_norm: 682.8631 (685.1714)  amp_scale: 1.0000 (1.0000)  time: 1.1552  data: 0.5859  max mem: 13835
[19:21:41.973097] [Train][Ep-97/100] Total time: 0:29:31 (1.2347 s / it)
[19:21:41.973580] Syncing meters...
[19:21:41.974892] Averaged stats: lr: 0.0000 (0.0000)  loss: 1037.3129 (1039.3085)  grad_norm: 682.8631 (685.1714)  amp_scale: 1.0000 (1.0000)
[19:21:50.416515] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 98)
[19:21:52.514873] [Train][Ep-98/100]  [   0/1435]  eta: 0:49:58  lr: 0.0000 (0.0000)  time: 2.0897  data: 1.6071  max mem: 13835
[19:23:52.980607] [Train][Ep-98/100]  [ 100/1435]  eta: 0:26:59  lr: 0.0000 (0.0000)  loss: 1072.8240 (1066.4066)  grad_norm: 675.8929 (688.0382)  amp_scale: 1.0000 (1.0000)  time: 1.2151  data: 0.6307  max mem: 13835
[19:25:51.836825] [Train][Ep-98/100]  [ 200/1435]  eta: 0:24:43  lr: 0.0000 (0.0000)  loss: 1024.0569 (1048.9029)  grad_norm: 672.4583 (681.8668)  amp_scale: 1.0000 (1.0000)  time: 1.3109  data: 0.7426  max mem: 13835
[19:28:00.392868] [Train][Ep-98/100]  [ 300/1435]  eta: 0:23:14  lr: 0.0000 (0.0000)  loss: 1047.0247 (1050.9766)  grad_norm: 679.6808 (688.1953)  amp_scale: 1.0000 (1.0000)  time: 1.2144  data: 0.6395  max mem: 13835
[19:29:57.238282] [Train][Ep-98/100]  [ 400/1435]  eta: 0:20:56  lr: 0.0000 (0.0000)  loss: 1029.8462 (1047.0750)  grad_norm: 686.1617 (686.7869)  amp_scale: 1.0000 (1.0000)  time: 1.1279  data: 0.3267  max mem: 13835
[19:31:56.266529] [Train][Ep-98/100]  [ 500/1435]  eta: 0:18:50  lr: 0.0000 (0.0000)  loss: 1036.1414 (1043.8747)  grad_norm: 677.3120 (685.5364)  amp_scale: 1.0000 (1.0000)  time: 1.2393  data: 0.2516  max mem: 13835
[19:33:54.523638] [Train][Ep-98/100]  [ 600/1435]  eta: 0:16:45  lr: 0.0000 (0.0000)  loss: 1033.9413 (1043.3343)  grad_norm: 683.3601 (684.3486)  amp_scale: 1.0000 (1.0000)  time: 1.1944  data: 0.0056  max mem: 13835
[19:35:52.163916] [Train][Ep-98/100]  [ 700/1435]  eta: 0:14:42  lr: 0.0000 (0.0000)  loss: 1005.5228 (1040.6049)  grad_norm: 697.6207 (684.9446)  amp_scale: 1.0000 (1.0000)  time: 1.2189  data: 0.0683  max mem: 13835
[19:37:54.663432] [Train][Ep-98/100]  [ 800/1435]  eta: 0:12:44  lr: 0.0000 (0.0000)  loss: 1051.3408 (1043.0943)  grad_norm: 677.2247 (684.3570)  amp_scale: 1.0000 (1.0000)  time: 1.2160  data: 0.1009  max mem: 13835
[19:39:59.775697] [Train][Ep-98/100]  [ 900/1435]  eta: 0:10:46  lr: 0.0000 (0.0000)  loss: 1010.6405 (1041.9655)  grad_norm: 679.3730 (684.2870)  amp_scale: 1.0000 (1.0000)  time: 1.1909  data: 0.0385  max mem: 13835
[19:42:03.119127] [Train][Ep-98/100]  [1000/1435]  eta: 0:08:46  lr: 0.0000 (0.0000)  loss: 1080.0840 (1047.1494)  grad_norm: 681.9633 (684.0353)  amp_scale: 1.0000 (1.0000)  time: 1.2298  data: 0.2677  max mem: 13835
[19:44:05.712062] [Train][Ep-98/100]  [1100/1435]  eta: 0:06:46  lr: 0.0000 (0.0000)  loss: 1028.3696 (1044.7370)  grad_norm: 676.4935 (683.2848)  amp_scale: 1.0000 (1.0000)  time: 1.2845  data: 0.0005  max mem: 13835
[19:46:09.242003] [Train][Ep-98/100]  [1200/1435]  eta: 0:04:45  lr: 0.0000 (0.0000)  loss: 1033.8467 (1044.9705)  grad_norm: 659.5255 (682.0900)  amp_scale: 1.0000 (1.0000)  time: 1.2651  data: 0.2329  max mem: 13835
[19:48:21.284921] [Train][Ep-98/100]  [1300/1435]  eta: 0:02:45  lr: 0.0000 (0.0000)  loss: 1024.6398 (1043.9345)  grad_norm: 670.5190 (682.6167)  amp_scale: 1.0000 (1.0000)  time: 1.3344  data: 0.2152  max mem: 13835
[19:50:27.670790] [Train][Ep-98/100]  [1400/1435]  eta: 0:00:42  lr: 0.0000 (0.0000)  loss: 1019.1564 (1042.0352)  grad_norm: 686.7501 (682.6189)  amp_scale: 1.0000 (1.0000)  time: 1.2814  data: 0.0196  max mem: 13835
[19:51:10.852877] [Train][Ep-98/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1019.1564 (1041.9587)  grad_norm: 672.5867 (682.1043)  amp_scale: 1.0000 (1.0000)  time: 1.3003  data: 0.0031  max mem: 13835
[19:51:10.853766] [Train][Ep-98/100] Total time: 0:29:20 (1.2268 s / it)
[19:51:10.854214] Syncing meters...
[19:51:11.846688] Averaged stats: lr: 0.0000 (0.0000)  loss: 1019.1564 (1038.6337)  grad_norm: 672.5867 (682.1043)  amp_scale: 1.0000 (1.0000)
[19:51:21.596750] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 99)
[19:51:23.381609] [Train][Ep-99/100]  [   0/1435]  eta: 0:42:29  lr: 0.0000 (0.0000)  time: 1.7763  data: 1.2938  max mem: 13835
[19:53:26.693297] [Train][Ep-99/100]  [ 100/1435]  eta: 0:27:33  lr: 0.0000 (0.0000)  loss: 998.8411 (1013.1837)  grad_norm: 682.0072 (693.3116)  amp_scale: 1.0000 (1.0000)  time: 1.1834  data: 0.6159  max mem: 13835
[19:55:25.803299] [Train][Ep-99/100]  [ 200/1435]  eta: 0:25:00  lr: 0.0000 (0.0000)  loss: 1004.1400 (1016.0721)  grad_norm: 668.1910 (688.9023)  amp_scale: 1.0000 (1.0000)  time: 1.2501  data: 0.6806  max mem: 13835
[19:57:23.553555] [Train][Ep-99/100]  [ 300/1435]  eta: 0:22:44  lr: 0.0000 (0.0000)  loss: 1035.4377 (1020.3944)  grad_norm: 677.9246 (688.5036)  amp_scale: 1.0000 (1.0000)  time: 1.1954  data: 0.6256  max mem: 13835
[19:59:18.421893] [Train][Ep-99/100]  [ 400/1435]  eta: 0:20:30  lr: 0.0000 (0.0000)  loss: 1028.3732 (1028.4482)  grad_norm: 686.5548 (687.6766)  amp_scale: 1.0000 (1.0000)  time: 1.1208  data: 0.3386  max mem: 13835
[20:01:12.647781] [Train][Ep-99/100]  [ 500/1435]  eta: 0:18:22  lr: 0.0000 (0.0000)  loss: 1059.8236 (1033.4837)  grad_norm: 694.3716 (687.8057)  amp_scale: 1.0000 (1.0000)  time: 1.1463  data: 0.4639  max mem: 13835
[20:03:12.585021] [Train][Ep-99/100]  [ 600/1435]  eta: 0:16:27  lr: 0.0000 (0.0000)  loss: 1062.0073 (1039.7310)  grad_norm: 669.3903 (687.6446)  amp_scale: 1.0000 (1.0000)  time: 1.2043  data: 0.4745  max mem: 13835
[20:05:11.849283] [Train][Ep-99/100]  [ 700/1435]  eta: 0:14:30  lr: 0.0000 (0.0000)  loss: 1016.9601 (1037.3094)  grad_norm: 687.9910 (689.3964)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.1691  max mem: 13835
[20:07:11.402261] [Train][Ep-99/100]  [ 800/1435]  eta: 0:12:32  lr: 0.0000 (0.0000)  loss: 994.5083 (1035.0985)  grad_norm: 680.4097 (689.0960)  amp_scale: 1.0000 (1.0000)  time: 1.2138  data: 0.5342  max mem: 13835
[20:09:13.934821] [Train][Ep-99/100]  [ 900/1435]  eta: 0:10:36  lr: 0.0000 (0.0000)  loss: 1030.4436 (1035.0159)  grad_norm: 678.2458 (687.9043)  amp_scale: 1.0000 (1.0000)  time: 1.2076  data: 0.4909  max mem: 13835
[20:11:17.698735] [Train][Ep-99/100]  [1000/1435]  eta: 0:08:39  lr: 0.0000 (0.0000)  loss: 1024.3618 (1035.0349)  grad_norm: 693.9371 (688.1320)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.1160  max mem: 13835
[20:13:18.919416] [Train][Ep-99/100]  [1100/1435]  eta: 0:06:40  lr: 0.0000 (0.0000)  loss: 1026.1464 (1033.7567)  grad_norm: 673.3134 (686.4514)  amp_scale: 1.0000 (1.0000)  time: 1.1904  data: 0.1204  max mem: 13835
[20:15:17.194127] [Train][Ep-99/100]  [1200/1435]  eta: 0:04:40  lr: 0.0000 (0.0000)  loss: 1014.4022 (1032.8259)  grad_norm: 677.5936 (685.8520)  amp_scale: 1.0000 (1.0000)  time: 1.2466  data: 0.3368  max mem: 13835
[20:17:18.250328] [Train][Ep-99/100]  [1300/1435]  eta: 0:02:41  lr: 0.0000 (0.0000)  loss: 1029.8381 (1031.7430)  grad_norm: 695.9566 (686.7507)  amp_scale: 1.0000 (1.0000)  time: 1.2346  data: 0.6647  max mem: 13835
[20:19:20.942455] [Train][Ep-99/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1012.2141 (1031.5002)  grad_norm: 677.6138 (686.3739)  amp_scale: 1.0000 (1.0000)  time: 1.2525  data: 0.0142  max mem: 13835
[20:20:01.259692] [Train][Ep-99/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1044.0859 (1032.0946)  grad_norm: 676.6177 (686.4783)  amp_scale: 1.0000 (1.0000)  time: 1.2479  data: 0.0003  max mem: 13835
[20:20:01.260529] [Train][Ep-99/100] Total time: 0:28:39 (1.1984 s / it)
[20:20:01.260943] Syncing meters...
[20:20:02.656428] Averaged stats: lr: 0.0000 (0.0000)  loss: 1044.0859 (1036.2798)  grad_norm: 676.6177 (686.4783)  amp_scale: 1.0000 (1.0000)
[20:20:04.712343] [Eval][Ep-99/100]  [  0/121]  eta: 0:04:07  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0476  data: 1.8855  max mem: 13835
[20:21:48.095401] [Eval][Ep-99/100]  [100/121]  eta: 0:00:21  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9080  data: 0.7486  max mem: 13835
[20:22:05.779160] [Eval][Ep-99/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8841  data: 0.7281  max mem: 13835
[20:22:05.780112] [Eval][Ep-99/100] Total time: 0:02:03 (1.0175 s / it)
[20:22:06.194589] [Eval][Ep-99/100] val_acc1_image=26.79 | val_acc1_audio=42.61 | val_acc1_fusion=39.19 | val_acc1_all=52.98
[20:22:14.610824] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 100)
