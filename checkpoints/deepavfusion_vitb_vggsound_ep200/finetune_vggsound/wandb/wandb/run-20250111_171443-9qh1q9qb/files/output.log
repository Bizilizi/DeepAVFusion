
[17:14:44.169545] Start training for 100 epochs
/home/wiss/zverev/miniconda3/envs/efav/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[17:14:52.216964] [Train][Ep-45/100]  [   0/1435]  eta: 3:12:21  lr: 0.0005 (0.0005)  time: 8.0429  data: 7.1625  max mem: 12727
[17:17:10.652843] [Train][Ep-45/100]  [ 100/1435]  eta: 0:32:16  lr: 0.0005 (0.0005)  loss: 1193.6354 (1187.2840)  grad_norm: 658.1409 (669.4034)  amp_scale: 1.0000 (1.0000)  time: 1.3046  data: 0.7404  max mem: 13835
[17:19:24.533929] [Train][Ep-45/100]  [ 200/1435]  eta: 0:28:42  lr: 0.0005 (0.0005)  loss: 1204.7122 (1198.7998)  grad_norm: 684.5579 (673.5301)  amp_scale: 1.0000 (1.0000)  time: 1.3741  data: 0.8090  max mem: 13835
[17:21:31.968737] [Train][Ep-45/100]  [ 300/1435]  eta: 0:25:37  lr: 0.0005 (0.0005)  loss: 1214.2588 (1203.6741)  grad_norm: 661.3173 (674.1818)  amp_scale: 1.0000 (1.0000)  time: 1.2970  data: 0.6650  max mem: 13835
[17:23:41.226384] [Train][Ep-45/100]  [ 400/1435]  eta: 0:23:06  lr: 0.0005 (0.0005)  loss: 1179.2662 (1195.1735)  grad_norm: 671.1118 (677.6930)  amp_scale: 1.0000 (1.0000)  time: 1.2792  data: 0.6670  max mem: 13835
[17:25:53.123821] [Train][Ep-45/100]  [ 500/1435]  eta: 0:20:48  lr: 0.0005 (0.0005)  loss: 1206.0934 (1194.9324)  grad_norm: 679.9670 (678.9826)  amp_scale: 1.0000 (1.0000)  time: 1.2589  data: 0.6650  max mem: 13835
[17:27:58.000845] [Train][Ep-45/100]  [ 600/1435]  eta: 0:18:22  lr: 0.0005 (0.0005)  loss: 1177.4435 (1193.1645)  grad_norm: 695.0467 (682.1518)  amp_scale: 1.0000 (1.0000)  time: 1.3016  data: 0.7106  max mem: 13835
[17:30:07.482838] [Train][Ep-45/100]  [ 700/1435]  eta: 0:16:08  lr: 0.0005 (0.0005)  loss: 1179.2419 (1195.1976)  grad_norm: 661.5372 (680.1956)  amp_scale: 1.0000 (1.0000)  time: 1.3344  data: 0.2968  max mem: 13835
[17:32:13.533878] [Train][Ep-45/100]  [ 800/1435]  eta: 0:13:51  lr: 0.0005 (0.0005)  loss: 1213.9032 (1198.8449)  grad_norm: 678.9845 (681.1276)  amp_scale: 1.0000 (1.0000)  time: 1.3229  data: 0.5469  max mem: 13835
[17:34:28.645819] [Train][Ep-45/100]  [ 900/1435]  eta: 0:11:43  lr: 0.0005 (0.0005)  loss: 1226.3505 (1201.1887)  grad_norm: 683.4824 (681.5688)  amp_scale: 1.0000 (1.0000)  time: 1.4579  data: 0.1340  max mem: 13835
[17:36:37.144732] [Train][Ep-45/100]  [1000/1435]  eta: 0:09:30  lr: 0.0005 (0.0005)  loss: 1154.4946 (1200.6666)  grad_norm: 693.7083 (683.0487)  amp_scale: 1.0000 (1.0000)  time: 1.2652  data: 0.2507  max mem: 13835
[17:38:45.851980] [Train][Ep-45/100]  [1100/1435]  eta: 0:07:18  lr: 0.0005 (0.0005)  loss: 1208.2081 (1201.8795)  grad_norm: 692.9937 (683.7569)  amp_scale: 1.0000 (1.0000)  time: 1.3270  data: 0.1577  max mem: 13835
[17:40:53.767450] [Train][Ep-45/100]  [1200/1435]  eta: 0:05:07  lr: 0.0005 (0.0005)  loss: 1164.4685 (1200.1707)  grad_norm: 677.6287 (684.5088)  amp_scale: 1.0000 (1.0000)  time: 1.2120  data: 0.2945  max mem: 13835
[17:43:08.752279] [Train][Ep-45/100]  [1300/1435]  eta: 0:02:56  lr: 0.0005 (0.0005)  loss: 1197.6144 (1202.2862)  grad_norm: 687.8901 (684.7002)  amp_scale: 1.0000 (1.0000)  time: 1.3394  data: 0.0002  max mem: 13835
[17:45:12.723137] [Train][Ep-45/100]  [1400/1435]  eta: 0:00:45  lr: 0.0005 (0.0005)  loss: 1174.5548 (1199.5309)  grad_norm: 698.9792 (685.8236)  amp_scale: 1.0000 (1.0000)  time: 1.2542  data: 0.4884  max mem: 13835
[17:45:54.431709] [Train][Ep-45/100]  [1434/1435]  eta: 0:00:01  lr: 0.0005 (0.0005)  loss: 1174.5548 (1199.7695)  grad_norm: 698.9792 (686.1711)  amp_scale: 1.0000 (1.0000)  time: 1.2468  data: 0.2260  max mem: 13835
[17:45:54.432759] [Train][Ep-45/100] Total time: 0:31:10 (1.3033 s / it)
[17:45:54.433218] Syncing meters...
[17:45:55.142134] Averaged stats: lr: 0.0005 (0.0005)  loss: 1174.5548 (1201.6689)  grad_norm: 698.9792 (686.1711)  amp_scale: 1.0000 (1.0000)
[17:46:00.870151] [Eval][Ep-45/100]  [  0/121]  eta: 0:11:31  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 5.7189  data: 5.5571  max mem: 13835
[17:47:45.504725] [Eval][Ep-45/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9350  data: 0.7748  max mem: 13835
[17:48:03.068782] [Eval][Ep-45/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8780  data: 0.7203  max mem: 13835
[17:48:03.069800] [Eval][Ep-45/100] Total time: 0:02:07 (1.0572 s / it)
[17:48:03.645430] [Eval][Ep-45/100] val_acc1_image=26.36 | val_acc1_audio=41.12 | val_acc1_fusion=37.69 | val_acc1_all=51.77
[17:48:12.174247] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 46)
[17:48:14.119110] [Train][Ep-46/100]  [   0/1435]  eta: 0:46:17  lr: 0.0005 (0.0005)  time: 1.9358  data: 1.4542  max mem: 13835
[17:50:15.629140] [Train][Ep-46/100]  [ 100/1435]  eta: 0:27:11  lr: 0.0005 (0.0005)  loss: 1228.4828 (1225.5088)  grad_norm: 686.0037 (681.8229)  amp_scale: 1.0000 (1.0000)  time: 1.1847  data: 0.4554  max mem: 13835
[17:52:19.116142] [Train][Ep-46/100]  [ 200/1435]  eta: 0:25:17  lr: 0.0005 (0.0005)  loss: 1167.1895 (1210.5546)  grad_norm: 690.2693 (688.5104)  amp_scale: 1.0000 (1.0000)  time: 1.1980  data: 0.2381  max mem: 13835
[17:54:29.422712] [Train][Ep-46/100]  [ 300/1435]  eta: 0:23:42  lr: 0.0005 (0.0005)  loss: 1175.6465 (1202.5605)  grad_norm: 697.3036 (689.4743)  amp_scale: 1.0000 (1.0000)  time: 1.2467  data: 0.0713  max mem: 13835
[17:56:36.063688] [Train][Ep-46/100]  [ 400/1435]  eta: 0:21:40  lr: 0.0005 (0.0005)  loss: 1169.6073 (1195.6215)  grad_norm: 673.1312 (685.6529)  amp_scale: 1.0000 (1.0000)  time: 1.2954  data: 0.0006  max mem: 13835
[17:58:40.847439] [Train][Ep-46/100]  [ 500/1435]  eta: 0:19:33  lr: 0.0005 (0.0005)  loss: 1199.3201 (1200.6374)  grad_norm: 667.7916 (684.8019)  amp_scale: 1.0000 (1.0000)  time: 1.2884  data: 0.2711  max mem: 13835
[18:00:48.979819] [Train][Ep-46/100]  [ 600/1435]  eta: 0:17:31  lr: 0.0005 (0.0005)  loss: 1160.8180 (1195.8982)  grad_norm: 680.9992 (685.2981)  amp_scale: 1.0000 (1.0000)  time: 1.2665  data: 0.6089  max mem: 13835
[18:02:55.726815] [Train][Ep-46/100]  [ 700/1435]  eta: 0:15:26  lr: 0.0005 (0.0005)  loss: 1186.8472 (1194.1244)  grad_norm: 684.0165 (685.2403)  amp_scale: 1.0000 (1.0000)  time: 1.2960  data: 0.6213  max mem: 13835
[18:05:10.648259] [Train][Ep-46/100]  [ 800/1435]  eta: 0:13:27  lr: 0.0005 (0.0005)  loss: 1177.4691 (1192.5703)  grad_norm: 670.5023 (684.6091)  amp_scale: 1.0000 (1.0000)  time: 1.3067  data: 0.1062  max mem: 13835
[18:07:25.739676] [Train][Ep-46/100]  [ 900/1435]  eta: 0:11:24  lr: 0.0005 (0.0005)  loss: 1164.7924 (1190.7980)  grad_norm: 662.6602 (682.3754)  amp_scale: 1.0000 (1.0000)  time: 1.2470  data: 0.1592  max mem: 13835
[18:09:41.596507] [Train][Ep-46/100]  [1000/1435]  eta: 0:09:20  lr: 0.0004 (0.0005)  loss: 1194.7854 (1195.7267)  grad_norm: 679.1208 (682.4532)  amp_scale: 1.0000 (1.0000)  time: 1.3434  data: 0.0126  max mem: 13835
[18:11:56.300592] [Train][Ep-46/100]  [1100/1435]  eta: 0:07:13  lr: 0.0004 (0.0005)  loss: 1193.1550 (1197.2954)  grad_norm: 665.2235 (681.6718)  amp_scale: 1.0000 (1.0000)  time: 1.3689  data: 0.0464  max mem: 13835
[18:14:08.155064] [Train][Ep-46/100]  [1200/1435]  eta: 0:05:04  lr: 0.0004 (0.0005)  loss: 1215.6127 (1198.7055)  grad_norm: 679.8477 (682.1389)  amp_scale: 1.0000 (1.0000)  time: 1.3024  data: 0.1671  max mem: 13835
[18:16:14.763681] [Train][Ep-46/100]  [1300/1435]  eta: 0:02:54  lr: 0.0004 (0.0005)  loss: 1210.4060 (1199.1519)  grad_norm: 695.9003 (683.1966)  amp_scale: 1.0000 (1.0000)  time: 1.1952  data: 0.5978  max mem: 13835
[18:18:18.246862] [Train][Ep-46/100]  [1400/1435]  eta: 0:00:45  lr: 0.0004 (0.0005)  loss: 1171.9106 (1198.5852)  grad_norm: 683.6665 (683.5477)  amp_scale: 1.0000 (1.0000)  time: 1.2088  data: 0.6204  max mem: 13835
[18:18:57.413622] [Train][Ep-46/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0005)  loss: 1182.8973 (1198.3674)  grad_norm: 695.7463 (684.0891)  amp_scale: 1.0000 (1.0000)  time: 1.1778  data: 0.3615  max mem: 13835
[18:18:57.414341] [Train][Ep-46/100] Total time: 0:30:45 (1.2859 s / it)
[18:18:57.414743] Syncing meters...
[18:18:58.418314] Averaged stats: lr: 0.0004 (0.0005)  loss: 1182.8973 (1195.1189)  grad_norm: 695.7463 (684.0891)  amp_scale: 1.0000 (1.0000)
[18:19:07.406023] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 47)
[18:19:09.535665] [Train][Ep-47/100]  [   0/1435]  eta: 0:50:42  lr: 0.0004 (0.0004)  time: 2.1203  data: 1.6362  max mem: 13835
[18:21:06.218833] [Train][Ep-47/100]  [ 100/1435]  eta: 0:26:10  lr: 0.0004 (0.0004)  loss: 1168.0980 (1197.6655)  grad_norm: 677.0809 (683.1252)  amp_scale: 1.0000 (1.0000)  time: 1.1182  data: 0.4984  max mem: 13835
[18:23:09.719262] [Train][Ep-47/100]  [ 200/1435]  eta: 0:24:48  lr: 0.0004 (0.0004)  loss: 1135.7834 (1176.1638)  grad_norm: 649.3897 (675.2427)  amp_scale: 1.0000 (1.0000)  time: 1.1420  data: 0.1594  max mem: 13835
[18:25:18.677994] [Train][Ep-47/100]  [ 300/1435]  eta: 0:23:19  lr: 0.0004 (0.0004)  loss: 1163.0836 (1172.5687)  grad_norm: 675.0401 (674.5762)  amp_scale: 1.0000 (1.0000)  time: 1.2294  data: 0.0535  max mem: 13835
[18:27:15.760679] [Train][Ep-47/100]  [ 400/1435]  eta: 0:21:00  lr: 0.0004 (0.0004)  loss: 1122.6229 (1168.6086)  grad_norm: 684.1578 (677.1367)  amp_scale: 1.0000 (1.0000)  time: 1.1008  data: 0.4303  max mem: 13835
[18:29:18.417818] [Train][Ep-47/100]  [ 500/1435]  eta: 0:19:00  lr: 0.0004 (0.0004)  loss: 1182.4274 (1171.6448)  grad_norm: 684.4832 (680.9246)  amp_scale: 1.0000 (1.0000)  time: 1.2887  data: 0.0004  max mem: 13835
[18:31:24.981743] [Train][Ep-47/100]  [ 600/1435]  eta: 0:17:04  lr: 0.0004 (0.0004)  loss: 1150.9177 (1172.6227)  grad_norm: 668.1814 (680.5731)  amp_scale: 1.0000 (1.0000)  time: 1.3119  data: 0.5595  max mem: 13835
[18:33:39.085279] [Train][Ep-47/100]  [ 700/1435]  eta: 0:15:13  lr: 0.0004 (0.0004)  loss: 1214.1384 (1177.3174)  grad_norm: 682.6824 (680.0678)  amp_scale: 1.0000 (1.0000)  time: 1.3438  data: 0.0401  max mem: 13835
[18:35:48.631323] [Train][Ep-47/100]  [ 800/1435]  eta: 0:13:13  lr: 0.0004 (0.0004)  loss: 1216.3441 (1180.1874)  grad_norm: 675.4855 (681.6013)  amp_scale: 1.0000 (1.0000)  time: 1.2030  data: 0.6341  max mem: 13835
[18:37:50.970374] [Train][Ep-47/100]  [ 900/1435]  eta: 0:11:07  lr: 0.0004 (0.0004)  loss: 1196.3054 (1183.1886)  grad_norm: 682.6716 (682.4931)  amp_scale: 1.0000 (1.0000)  time: 1.2509  data: 0.4869  max mem: 13835
[18:40:02.406149] [Train][Ep-47/100]  [1000/1435]  eta: 0:09:05  lr: 0.0004 (0.0004)  loss: 1215.6447 (1184.1263)  grad_norm: 686.7766 (682.2897)  amp_scale: 1.0000 (1.0000)  time: 1.3349  data: 0.7658  max mem: 13835
[18:42:18.569710] [Train][Ep-47/100]  [1100/1435]  eta: 0:07:03  lr: 0.0004 (0.0004)  loss: 1178.0541 (1182.7075)  grad_norm: 664.7325 (682.2073)  amp_scale: 1.0000 (1.0000)  time: 1.4051  data: 0.0180  max mem: 13835
[18:44:30.292941] [Train][Ep-47/100]  [1200/1435]  eta: 0:04:57  lr: 0.0004 (0.0004)  loss: 1164.9772 (1184.3382)  grad_norm: 688.3535 (682.7499)  amp_scale: 1.0000 (1.0000)  time: 1.3473  data: 0.0265  max mem: 13835
[18:46:35.917173] [Train][Ep-47/100]  [1300/1435]  eta: 0:02:51  lr: 0.0004 (0.0004)  loss: 1197.6962 (1185.6099)  grad_norm: 672.4777 (682.4052)  amp_scale: 1.0000 (1.0000)  time: 1.2088  data: 0.2025  max mem: 13835
[18:48:41.357545] [Train][Ep-47/100]  [1400/1435]  eta: 0:00:44  lr: 0.0004 (0.0004)  loss: 1157.4022 (1184.6074)  grad_norm: 678.5596 (683.1336)  amp_scale: 1.0000 (1.0000)  time: 1.1546  data: 0.1898  max mem: 13835
[18:49:19.761873] [Train][Ep-47/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1135.3761 (1183.4797)  grad_norm: 682.9915 (683.1641)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.2448  max mem: 13835
[18:49:19.762815] [Train][Ep-47/100] Total time: 0:30:12 (1.2630 s / it)
[18:49:19.763291] Syncing meters...
[18:49:20.523326] Averaged stats: lr: 0.0004 (0.0004)  loss: 1135.3761 (1190.4785)  grad_norm: 682.9915 (683.1641)  amp_scale: 1.0000 (1.0000)
[18:49:29.189334] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 48)
[18:49:31.312217] [Train][Ep-48/100]  [   0/1435]  eta: 0:50:34  lr: 0.0004 (0.0004)  time: 2.1147  data: 1.6311  max mem: 13835
[18:51:32.919347] [Train][Ep-48/100]  [ 100/1435]  eta: 0:27:15  lr: 0.0004 (0.0004)  loss: 1148.2114 (1174.8337)  grad_norm: 681.6232 (682.7982)  amp_scale: 1.0000 (1.0000)  time: 1.1498  data: 0.3979  max mem: 13835
[18:53:40.386582] [Train][Ep-48/100]  [ 200/1435]  eta: 0:25:43  lr: 0.0004 (0.0004)  loss: 1119.9181 (1154.6968)  grad_norm: 667.8810 (678.1535)  amp_scale: 1.0000 (1.0000)  time: 1.2586  data: 0.1133  max mem: 13835
[18:55:49.734918] [Train][Ep-48/100]  [ 300/1435]  eta: 0:23:54  lr: 0.0004 (0.0004)  loss: 1176.6222 (1165.5200)  grad_norm: 676.3246 (679.6463)  amp_scale: 1.0000 (1.0000)  time: 1.2932  data: 0.0830  max mem: 13835
[18:57:54.391581] [Train][Ep-48/100]  [ 400/1435]  eta: 0:21:43  lr: 0.0004 (0.0004)  loss: 1173.8595 (1164.7076)  grad_norm: 668.7905 (679.6379)  amp_scale: 1.0000 (1.0000)  time: 1.2611  data: 0.0381  max mem: 13835
[18:59:55.219770] [Train][Ep-48/100]  [ 500/1435]  eta: 0:19:28  lr: 0.0004 (0.0004)  loss: 1167.7915 (1165.0962)  grad_norm: 676.4262 (678.3191)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.5133  max mem: 13835
[19:01:54.665037] [Train][Ep-48/100]  [ 600/1435]  eta: 0:17:15  lr: 0.0004 (0.0004)  loss: 1137.9055 (1166.8286)  grad_norm: 686.8761 (682.1758)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.5864  max mem: 13835
[19:04:01.837063] [Train][Ep-48/100]  [ 700/1435]  eta: 0:15:14  lr: 0.0004 (0.0004)  loss: 1197.6586 (1170.6683)  grad_norm: 680.6995 (682.6021)  amp_scale: 1.0000 (1.0000)  time: 1.2543  data: 0.0034  max mem: 13835
[19:06:08.552559] [Train][Ep-48/100]  [ 800/1435]  eta: 0:13:12  lr: 0.0004 (0.0004)  loss: 1181.7994 (1171.0178)  grad_norm: 680.4111 (682.2411)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.0681  max mem: 13835
[19:08:19.062055] [Train][Ep-48/100]  [ 900/1435]  eta: 0:11:10  lr: 0.0004 (0.0004)  loss: 1184.8750 (1175.0137)  grad_norm: 688.0631 (684.3034)  amp_scale: 1.0000 (1.0000)  time: 1.2974  data: 0.0726  max mem: 13835
[19:10:21.775284] [Train][Ep-48/100]  [1000/1435]  eta: 0:09:04  lr: 0.0004 (0.0004)  loss: 1184.3890 (1177.8286)  grad_norm: 663.7953 (684.1836)  amp_scale: 1.0000 (1.0000)  time: 1.1544  data: 0.5017  max mem: 13835
[19:12:26.833058] [Train][Ep-48/100]  [1100/1435]  eta: 0:06:59  lr: 0.0004 (0.0004)  loss: 1218.4907 (1179.2224)  grad_norm: 709.3503 (685.8983)  amp_scale: 1.0000 (1.0000)  time: 1.2135  data: 0.0844  max mem: 13835
[19:14:30.714115] [Train][Ep-48/100]  [1200/1435]  eta: 0:04:53  lr: 0.0004 (0.0004)  loss: 1158.7905 (1179.1218)  grad_norm: 696.9403 (686.6340)  amp_scale: 1.0000 (1.0000)  time: 1.2615  data: 0.0129  max mem: 13835
[19:16:34.232187] [Train][Ep-48/100]  [1300/1435]  eta: 0:02:48  lr: 0.0004 (0.0004)  loss: 1160.1398 (1180.6960)  grad_norm: 700.9007 (687.1563)  amp_scale: 1.0000 (1.0000)  time: 1.2439  data: 0.0610  max mem: 13835
[19:18:40.483469] [Train][Ep-48/100]  [1400/1435]  eta: 0:00:43  lr: 0.0004 (0.0004)  loss: 1215.5453 (1181.7919)  grad_norm: 680.2610 (686.6707)  amp_scale: 1.0000 (1.0000)  time: 1.2168  data: 0.5978  max mem: 13835
[19:19:20.220286] [Train][Ep-48/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1182.5345 (1181.4351)  grad_norm: 666.8382 (686.3996)  amp_scale: 1.0000 (1.0000)  time: 1.1417  data: 0.5714  max mem: 13835
[19:19:20.221253] [Train][Ep-48/100] Total time: 0:29:51 (1.2481 s / it)
[19:19:20.221829] Syncing meters...
[19:19:20.223230] Averaged stats: lr: 0.0004 (0.0004)  loss: 1182.5345 (1184.4790)  grad_norm: 666.8382 (686.3996)  amp_scale: 1.0000 (1.0000)
[19:19:28.490038] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 49)
[19:19:30.458875] [Train][Ep-49/100]  [   0/1435]  eta: 0:46:53  lr: 0.0004 (0.0004)  time: 1.9603  data: 1.4753  max mem: 13835
[19:21:33.335084] [Train][Ep-49/100]  [ 100/1435]  eta: 0:27:30  lr: 0.0004 (0.0004)  loss: 1149.3081 (1161.1749)  grad_norm: 684.8180 (698.9444)  amp_scale: 1.0000 (1.0000)  time: 1.2143  data: 0.6429  max mem: 13835
[19:23:36.382531] [Train][Ep-49/100]  [ 200/1435]  eta: 0:25:23  lr: 0.0004 (0.0004)  loss: 1223.0603 (1185.6793)  grad_norm: 682.3945 (690.8449)  amp_scale: 1.0000 (1.0000)  time: 1.1967  data: 0.6026  max mem: 13835
[19:25:36.582522] [Train][Ep-49/100]  [ 300/1435]  eta: 0:23:07  lr: 0.0004 (0.0004)  loss: 1207.0515 (1185.1419)  grad_norm: 672.7352 (691.6273)  amp_scale: 1.0000 (1.0000)  time: 1.2267  data: 0.4981  max mem: 13835
[19:27:31.522554] [Train][Ep-49/100]  [ 400/1435]  eta: 0:20:46  lr: 0.0004 (0.0004)  loss: 1149.0736 (1181.0626)  grad_norm: 637.8390 (682.1273)  amp_scale: 1.0000 (1.0000)  time: 1.1370  data: 0.5184  max mem: 13835
[19:29:35.486710] [Train][Ep-49/100]  [ 500/1435]  eta: 0:18:52  lr: 0.0004 (0.0004)  loss: 1184.4620 (1179.6595)  grad_norm: 681.7974 (683.5098)  amp_scale: 1.0000 (1.0000)  time: 1.2344  data: 0.2493  max mem: 13835
[19:31:39.301713] [Train][Ep-49/100]  [ 600/1435]  eta: 0:16:55  lr: 0.0004 (0.0004)  loss: 1202.9749 (1181.4998)  grad_norm: 694.2524 (685.3262)  amp_scale: 1.0000 (1.0000)  time: 1.2449  data: 0.0023  max mem: 13835
[19:33:41.809435] [Train][Ep-49/100]  [ 700/1435]  eta: 0:14:54  lr: 0.0004 (0.0004)  loss: 1163.5671 (1179.3153)  grad_norm: 665.1235 (684.2489)  amp_scale: 1.0000 (1.0000)  time: 1.1833  data: 0.0615  max mem: 13835
[19:35:43.913092] [Train][Ep-49/100]  [ 800/1435]  eta: 0:12:53  lr: 0.0004 (0.0004)  loss: 1159.5569 (1179.9609)  grad_norm: 671.5950 (683.8558)  amp_scale: 1.0000 (1.0000)  time: 1.2200  data: 0.0017  max mem: 13835
[19:37:42.364360] [Train][Ep-49/100]  [ 900/1435]  eta: 0:10:49  lr: 0.0004 (0.0004)  loss: 1142.2201 (1178.4143)  grad_norm: 671.6208 (683.7845)  amp_scale: 1.0000 (1.0000)  time: 1.1390  data: 0.5265  max mem: 13835
[19:39:41.891398] [Train][Ep-49/100]  [1000/1435]  eta: 0:08:47  lr: 0.0004 (0.0004)  loss: 1179.4935 (1178.8945)  grad_norm: 673.2661 (683.8745)  amp_scale: 1.0000 (1.0000)  time: 1.1465  data: 0.3518  max mem: 13835
[19:41:38.973381] [Train][Ep-49/100]  [1100/1435]  eta: 0:06:44  lr: 0.0004 (0.0004)  loss: 1177.7185 (1179.2924)  grad_norm: 680.8226 (684.1746)  amp_scale: 1.0000 (1.0000)  time: 1.2510  data: 0.2583  max mem: 13835
[19:43:44.280052] [Train][Ep-49/100]  [1200/1435]  eta: 0:04:44  lr: 0.0004 (0.0004)  loss: 1122.5042 (1176.7715)  grad_norm: 681.1579 (684.7952)  amp_scale: 1.0000 (1.0000)  time: 1.1813  data: 0.1587  max mem: 13835
[19:45:42.163009] [Train][Ep-49/100]  [1300/1435]  eta: 0:02:43  lr: 0.0004 (0.0004)  loss: 1185.9069 (1176.2241)  grad_norm: 654.4052 (682.8214)  amp_scale: 1.0000 (1.0000)  time: 1.1611  data: 0.4213  max mem: 13835
[19:47:40.898585] [Train][Ep-49/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1190.2761 (1177.3174)  grad_norm: 670.1458 (682.1413)  amp_scale: 1.0000 (1.0000)  time: 1.2227  data: 0.1506  max mem: 13835
[19:48:22.754567] [Train][Ep-49/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1193.3210 (1177.6627)  grad_norm: 681.8937 (682.4397)  amp_scale: 1.0000 (1.0000)  time: 1.2332  data: 0.1138  max mem: 13835
[19:48:22.755482] [Train][Ep-49/100] Total time: 0:28:54 (1.2085 s / it)
[19:48:22.755910] Syncing meters...
[19:48:23.798440] Averaged stats: lr: 0.0004 (0.0004)  loss: 1193.3210 (1174.3626)  grad_norm: 681.8937 (682.4397)  amp_scale: 1.0000 (1.0000)
[19:48:33.595860] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 50)
[19:48:40.102778] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_0050.pth' (epoch 50)
[19:48:42.457647] [Train][Ep-50/100]  [   0/1435]  eta: 0:56:06  lr: 0.0004 (0.0004)  time: 2.3461  data: 1.8629  max mem: 13835
[19:50:45.212620] [Train][Ep-50/100]  [ 100/1435]  eta: 0:27:33  lr: 0.0004 (0.0004)  loss: 1187.1279 (1180.0168)  grad_norm: 660.6844 (669.0704)  amp_scale: 1.0000 (1.0000)  time: 1.2564  data: 0.6844  max mem: 13835
[19:52:44.675043] [Train][Ep-50/100]  [ 200/1435]  eta: 0:25:02  lr: 0.0004 (0.0004)  loss: 1166.8054 (1178.4417)  grad_norm: 687.6393 (679.1617)  amp_scale: 1.0000 (1.0000)  time: 1.2536  data: 0.2730  max mem: 13835
[19:54:40.970652] [Train][Ep-50/100]  [ 300/1435]  eta: 0:22:40  lr: 0.0004 (0.0004)  loss: 1137.2451 (1173.7198)  grad_norm: 676.4185 (682.4870)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.4805  max mem: 13835
[19:56:40.178444] [Train][Ep-50/100]  [ 400/1435]  eta: 0:20:39  lr: 0.0004 (0.0004)  loss: 1174.6663 (1174.0650)  grad_norm: 685.2906 (682.6724)  amp_scale: 1.0000 (1.0000)  time: 1.2153  data: 0.1364  max mem: 13835
[19:58:43.929222] [Train][Ep-50/100]  [ 500/1435]  eta: 0:18:46  lr: 0.0004 (0.0004)  loss: 1159.1042 (1175.2310)  grad_norm: 665.9003 (680.3498)  amp_scale: 1.0000 (1.0000)  time: 1.1453  data: 0.1970  max mem: 13835
[20:00:42.757959] [Train][Ep-50/100]  [ 600/1435]  eta: 0:16:43  lr: 0.0004 (0.0004)  loss: 1193.3032 (1179.1971)  grad_norm: 675.0751 (680.6011)  amp_scale: 1.0000 (1.0000)  time: 1.1570  data: 0.0228  max mem: 13835
[20:02:38.131932] [Train][Ep-50/100]  [ 700/1435]  eta: 0:14:38  lr: 0.0004 (0.0004)  loss: 1152.5186 (1176.6647)  grad_norm: 654.9960 (679.1157)  amp_scale: 1.0000 (1.0000)  time: 1.1663  data: 0.5878  max mem: 13835
[20:04:34.272479] [Train][Ep-50/100]  [ 800/1435]  eta: 0:12:36  lr: 0.0004 (0.0004)  loss: 1155.0063 (1177.1933)  grad_norm: 687.1877 (679.4013)  amp_scale: 1.0000 (1.0000)  time: 1.1985  data: 0.3752  max mem: 13835
[20:06:35.681340] [Train][Ep-50/100]  [ 900/1435]  eta: 0:10:38  lr: 0.0004 (0.0004)  loss: 1169.6212 (1176.9441)  grad_norm: 682.7834 (681.0992)  amp_scale: 1.0000 (1.0000)  time: 1.1967  data: 0.1150  max mem: 13835
[20:08:37.360575] [Train][Ep-50/100]  [1000/1435]  eta: 0:08:40  lr: 0.0004 (0.0004)  loss: 1172.9928 (1179.0716)  grad_norm: 662.7921 (680.7605)  amp_scale: 1.0000 (1.0000)  time: 1.2023  data: 0.0364  max mem: 13835
[20:10:35.993340] [Train][Ep-50/100]  [1100/1435]  eta: 0:06:40  lr: 0.0004 (0.0004)  loss: 1161.0522 (1179.6336)  grad_norm: 673.3253 (680.0117)  amp_scale: 1.0000 (1.0000)  time: 1.1433  data: 0.2826  max mem: 13835
[20:12:34.262550] [Train][Ep-50/100]  [1200/1435]  eta: 0:04:40  lr: 0.0004 (0.0004)  loss: 1170.9114 (1179.6588)  grad_norm: 676.0299 (680.6478)  amp_scale: 1.0000 (1.0000)  time: 1.2306  data: 0.2899  max mem: 13835
[20:14:33.842475] [Train][Ep-50/100]  [1300/1435]  eta: 0:02:41  lr: 0.0004 (0.0004)  loss: 1205.1625 (1181.5940)  grad_norm: 706.6396 (682.2604)  amp_scale: 1.0000 (1.0000)  time: 1.1937  data: 0.0091  max mem: 13835
[20:16:40.571984] [Train][Ep-50/100]  [1400/1435]  eta: 0:00:41  lr: 0.0004 (0.0004)  loss: 1200.5743 (1182.2697)  grad_norm: 686.4050 (682.5701)  amp_scale: 1.0000 (1.0000)  time: 1.2587  data: 0.0004  max mem: 13835
[20:17:20.768057] [Train][Ep-50/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1200.5743 (1182.1094)  grad_norm: 693.4883 (682.8494)  amp_scale: 1.0000 (1.0000)  time: 1.2006  data: 0.0350  max mem: 13835
[20:17:20.768931] [Train][Ep-50/100] Total time: 0:28:40 (1.1991 s / it)
[20:17:20.769367] Syncing meters...
[20:17:21.833489] Averaged stats: lr: 0.0004 (0.0004)  loss: 1200.5743 (1177.5390)  grad_norm: 693.4883 (682.8494)  amp_scale: 1.0000 (1.0000)
[20:17:23.941239] [Eval][Ep-50/100]  [  0/121]  eta: 0:04:13  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0991  data: 1.9386  max mem: 13835
[20:19:11.962962] [Eval][Ep-50/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9073  data: 0.7469  max mem: 13835
[20:19:29.351808] [Eval][Ep-50/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8694  data: 0.7134  max mem: 13835
[20:19:29.352653] [Eval][Ep-50/100] Total time: 0:02:07 (1.0538 s / it)
[20:19:30.023953] [Eval][Ep-50/100] val_acc1_image=27.79 | val_acc1_audio=41.14 | val_acc1_fusion=38.00 | val_acc1_all=52.44
[20:19:38.447692] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 51)
[20:19:40.209004] [Train][Ep-51/100]  [   0/1435]  eta: 0:41:55  lr: 0.0004 (0.0004)  time: 1.7530  data: 1.2720  max mem: 13835
[20:21:45.657512] [Train][Ep-51/100]  [ 100/1435]  eta: 0:28:01  lr: 0.0004 (0.0004)  loss: 1160.1096 (1154.6354)  grad_norm: 651.8353 (667.1221)  amp_scale: 1.0000 (1.0000)  time: 1.2936  data: 0.7242  max mem: 13835
[20:23:55.231822] [Train][Ep-51/100]  [ 200/1435]  eta: 0:26:17  lr: 0.0004 (0.0004)  loss: 1126.8334 (1163.1568)  grad_norm: 699.5887 (677.3779)  amp_scale: 1.0000 (1.0000)  time: 1.3402  data: 0.7701  max mem: 13835
[20:26:07.802967] [Train][Ep-51/100]  [ 300/1435]  eta: 0:24:28  lr: 0.0004 (0.0004)  loss: 1173.2518 (1167.4176)  grad_norm: 672.9262 (676.6734)  amp_scale: 1.0000 (1.0000)  time: 1.3567  data: 0.7841  max mem: 13835
[20:28:17.476226] [Train][Ep-51/100]  [ 400/1435]  eta: 0:22:19  lr: 0.0004 (0.0004)  loss: 1186.2111 (1170.0748)  grad_norm: 678.1544 (678.8718)  amp_scale: 1.0000 (1.0000)  time: 1.2376  data: 0.6675  max mem: 13835
[20:30:23.266033] [Train][Ep-51/100]  [ 500/1435]  eta: 0:20:03  lr: 0.0004 (0.0004)  loss: 1167.9438 (1166.3070)  grad_norm: 662.0781 (678.5566)  amp_scale: 1.0000 (1.0000)  time: 1.3093  data: 0.7094  max mem: 13835
[20:32:32.580997] [Train][Ep-51/100]  [ 600/1435]  eta: 0:17:55  lr: 0.0004 (0.0004)  loss: 1158.8729 (1169.9409)  grad_norm: 687.5097 (679.9652)  amp_scale: 1.0000 (1.0000)  time: 1.3368  data: 0.0010  max mem: 13835
[20:34:43.267365] [Train][Ep-51/100]  [ 700/1435]  eta: 0:15:48  lr: 0.0004 (0.0004)  loss: 1146.6760 (1168.3751)  grad_norm: 662.2072 (678.0262)  amp_scale: 1.0000 (1.0000)  time: 1.3108  data: 0.0010  max mem: 13835
[20:36:43.788746] [Train][Ep-51/100]  [ 800/1435]  eta: 0:13:32  lr: 0.0004 (0.0004)  loss: 1118.2788 (1165.1214)  grad_norm: 679.5869 (678.8936)  amp_scale: 1.0000 (1.0000)  time: 1.1773  data: 0.5544  max mem: 13835
[20:38:52.479983] [Train][Ep-51/100]  [ 900/1435]  eta: 0:11:25  lr: 0.0004 (0.0004)  loss: 1139.4882 (1166.7260)  grad_norm: 696.1478 (680.9787)  amp_scale: 1.0000 (1.0000)  time: 1.2808  data: 0.6853  max mem: 13835
[20:40:54.378183] [Train][Ep-51/100]  [1000/1435]  eta: 0:09:14  lr: 0.0004 (0.0004)  loss: 1166.8057 (1167.5265)  grad_norm: 670.9370 (680.1961)  amp_scale: 1.0000 (1.0000)  time: 1.1438  data: 0.5137  max mem: 13835
[20:42:58.663103] [Train][Ep-51/100]  [1100/1435]  eta: 0:07:06  lr: 0.0004 (0.0004)  loss: 1157.1603 (1167.0832)  grad_norm: 688.8856 (680.4089)  amp_scale: 1.0000 (1.0000)  time: 1.1789  data: 0.3270  max mem: 13835
[20:45:01.386744] [Train][Ep-51/100]  [1200/1435]  eta: 0:04:57  lr: 0.0004 (0.0004)  loss: 1187.1031 (1169.6906)  grad_norm: 675.2863 (680.6207)  amp_scale: 1.0000 (1.0000)  time: 1.2782  data: 0.1275  max mem: 13835
[20:47:04.956293] [Train][Ep-51/100]  [1300/1435]  eta: 0:02:50  lr: 0.0004 (0.0004)  loss: 1184.6532 (1171.5487)  grad_norm: 681.9580 (681.4439)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.2360  max mem: 13835
[20:49:03.757333] [Train][Ep-51/100]  [1400/1435]  eta: 0:00:44  lr: 0.0004 (0.0004)  loss: 1165.2773 (1171.6494)  grad_norm: 688.0062 (682.2077)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.1777  max mem: 13835
[20:49:44.334943] [Train][Ep-51/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1169.8188 (1172.1366)  grad_norm: 691.5917 (682.5419)  amp_scale: 1.0000 (1.0000)  time: 1.2216  data: 0.0031  max mem: 13835
[20:49:44.336541] [Train][Ep-51/100] Total time: 0:30:05 (1.2585 s / it)
[20:49:44.337882] Syncing meters...
[20:49:45.702764] Averaged stats: lr: 0.0004 (0.0004)  loss: 1169.8188 (1169.7191)  grad_norm: 691.5917 (682.5419)  amp_scale: 1.0000 (1.0000)
[20:49:55.390359] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 52)
[20:49:57.377052] [Train][Ep-52/100]  [   0/1435]  eta: 0:47:18  lr: 0.0004 (0.0004)  time: 1.9781  data: 1.4941  max mem: 13835
[20:51:54.701327] [Train][Ep-52/100]  [ 100/1435]  eta: 0:26:16  lr: 0.0004 (0.0004)  loss: 1107.7419 (1118.5965)  grad_norm: 659.2654 (666.1088)  amp_scale: 1.0000 (1.0000)  time: 1.2624  data: 0.6923  max mem: 13835
[20:53:59.868381] [Train][Ep-52/100]  [ 200/1435]  eta: 0:25:02  lr: 0.0004 (0.0004)  loss: 1126.7299 (1123.9784)  grad_norm: 661.0802 (666.4032)  amp_scale: 1.0000 (1.0000)  time: 1.2242  data: 0.6561  max mem: 13835
[20:56:07.649000] [Train][Ep-52/100]  [ 300/1435]  eta: 0:23:23  lr: 0.0004 (0.0004)  loss: 1139.4469 (1135.2258)  grad_norm: 657.8967 (672.0599)  amp_scale: 1.0000 (1.0000)  time: 1.2683  data: 0.6989  max mem: 13835
[20:58:12.543299] [Train][Ep-52/100]  [ 400/1435]  eta: 0:21:23  lr: 0.0004 (0.0004)  loss: 1172.5138 (1142.7165)  grad_norm: 679.2292 (674.0124)  amp_scale: 1.0000 (1.0000)  time: 1.2007  data: 0.6306  max mem: 13835
[21:00:08.472171] [Train][Ep-52/100]  [ 500/1435]  eta: 0:19:04  lr: 0.0004 (0.0004)  loss: 1136.3303 (1145.6976)  grad_norm: 663.1819 (676.6764)  amp_scale: 1.0000 (1.0000)  time: 1.1400  data: 0.1164  max mem: 13835
[21:02:12.183577] [Train][Ep-52/100]  [ 600/1435]  eta: 0:17:03  lr: 0.0004 (0.0004)  loss: 1136.0046 (1143.4286)  grad_norm: 692.4525 (677.8608)  amp_scale: 1.0000 (1.0000)  time: 1.2181  data: 0.2826  max mem: 13835
[21:04:19.652853] [Train][Ep-52/100]  [ 700/1435]  eta: 0:15:06  lr: 0.0004 (0.0004)  loss: 1127.6693 (1146.3078)  grad_norm: 683.1713 (678.7798)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.2638  max mem: 13835
[21:06:28.042728] [Train][Ep-52/100]  [ 800/1435]  eta: 0:13:06  lr: 0.0004 (0.0004)  loss: 1149.3053 (1147.4358)  grad_norm: 674.7038 (678.5810)  amp_scale: 1.0000 (1.0000)  time: 1.2378  data: 0.0205  max mem: 13835
[21:08:36.135729] [Train][Ep-52/100]  [ 900/1435]  eta: 0:11:05  lr: 0.0004 (0.0004)  loss: 1143.5776 (1151.0597)  grad_norm: 689.1245 (679.3071)  amp_scale: 1.0000 (1.0000)  time: 1.2148  data: 0.5124  max mem: 13835
[21:10:39.934812] [Train][Ep-52/100]  [1000/1435]  eta: 0:09:00  lr: 0.0004 (0.0004)  loss: 1208.1492 (1155.1862)  grad_norm: 678.2142 (679.8127)  amp_scale: 1.0000 (1.0000)  time: 1.2714  data: 0.7037  max mem: 13835
[21:12:43.442861] [Train][Ep-52/100]  [1100/1435]  eta: 0:06:56  lr: 0.0004 (0.0004)  loss: 1130.3726 (1154.6158)  grad_norm: 683.4285 (679.7857)  amp_scale: 1.0000 (1.0000)  time: 1.2243  data: 0.6557  max mem: 13835
[21:14:46.267891] [Train][Ep-52/100]  [1200/1435]  eta: 0:04:51  lr: 0.0004 (0.0004)  loss: 1166.5533 (1155.8309)  grad_norm: 679.1396 (680.2709)  amp_scale: 1.0000 (1.0000)  time: 1.2420  data: 0.6463  max mem: 13835
[21:16:52.213092] [Train][Ep-52/100]  [1300/1435]  eta: 0:02:47  lr: 0.0004 (0.0004)  loss: 1148.2936 (1154.9863)  grad_norm: 692.4160 (680.2100)  amp_scale: 1.0000 (1.0000)  time: 1.2863  data: 0.6899  max mem: 13835
[21:19:02.355438] [Train][Ep-52/100]  [1400/1435]  eta: 0:00:43  lr: 0.0004 (0.0004)  loss: 1113.2988 (1153.6660)  grad_norm: 672.3287 (679.8911)  amp_scale: 1.0000 (1.0000)  time: 1.3118  data: 0.0075  max mem: 13835
[21:19:43.274026] [Train][Ep-52/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1136.1289 (1152.7455)  grad_norm: 678.2998 (679.9703)  amp_scale: 1.0000 (1.0000)  time: 1.2193  data: 0.2791  max mem: 13835
[21:19:43.274945] [Train][Ep-52/100] Total time: 0:29:47 (1.2459 s / it)
[21:19:43.275489] Syncing meters...
[21:19:44.537842] Averaged stats: lr: 0.0004 (0.0004)  loss: 1136.1289 (1161.9342)  grad_norm: 678.2998 (679.9703)  amp_scale: 1.0000 (1.0000)
[21:19:52.707053] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 53)
[21:19:54.825550] [Train][Ep-53/100]  [   0/1435]  eta: 0:50:28  lr: 0.0004 (0.0004)  time: 2.1102  data: 1.6277  max mem: 13835
[21:21:57.442444] [Train][Ep-53/100]  [ 100/1435]  eta: 0:27:28  lr: 0.0004 (0.0004)  loss: 1134.8201 (1154.1893)  grad_norm: 690.2366 (685.1521)  amp_scale: 1.0000 (1.0000)  time: 1.2350  data: 0.6672  max mem: 13835
[21:23:59.296523] [Train][Ep-53/100]  [ 200/1435]  eta: 0:25:14  lr: 0.0004 (0.0004)  loss: 1178.8030 (1172.2062)  grad_norm: 679.5380 (685.6291)  amp_scale: 1.0000 (1.0000)  time: 1.2425  data: 0.6733  max mem: 13835
[21:26:01.082131] [Train][Ep-53/100]  [ 300/1435]  eta: 0:23:08  lr: 0.0004 (0.0004)  loss: 1135.3782 (1162.5010)  grad_norm: 678.0667 (684.5019)  amp_scale: 1.0000 (1.0000)  time: 1.1990  data: 0.6186  max mem: 13835
[21:28:02.592518] [Train][Ep-53/100]  [ 400/1435]  eta: 0:21:04  lr: 0.0004 (0.0004)  loss: 1112.9592 (1157.4765)  grad_norm: 702.2956 (685.4055)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.5325  max mem: 13835
[21:30:04.070092] [Train][Ep-53/100]  [ 500/1435]  eta: 0:19:00  lr: 0.0004 (0.0004)  loss: 1151.8879 (1160.8706)  grad_norm: 686.0920 (686.5419)  amp_scale: 1.0000 (1.0000)  time: 1.1604  data: 0.5878  max mem: 13835
[21:32:02.751995] [Train][Ep-53/100]  [ 600/1435]  eta: 0:16:54  lr: 0.0004 (0.0004)  loss: 1150.7517 (1162.2341)  grad_norm: 674.8680 (683.5119)  amp_scale: 1.0000 (1.0000)  time: 1.2031  data: 0.2225  max mem: 13835
[21:34:08.585935] [Train][Ep-53/100]  [ 700/1435]  eta: 0:14:57  lr: 0.0004 (0.0004)  loss: 1149.7500 (1159.0917)  grad_norm: 685.1614 (684.9637)  amp_scale: 1.0000 (1.0000)  time: 1.1999  data: 0.0003  max mem: 13835
[21:36:09.620478] [Train][Ep-53/100]  [ 800/1435]  eta: 0:12:54  lr: 0.0004 (0.0004)  loss: 1119.4658 (1158.5856)  grad_norm: 699.2654 (686.4266)  amp_scale: 1.0000 (1.0000)  time: 1.1307  data: 0.5467  max mem: 13835
[21:38:10.549860] [Train][Ep-53/100]  [ 900/1435]  eta: 0:10:51  lr: 0.0004 (0.0004)  loss: 1180.4330 (1162.2053)  grad_norm: 665.1484 (685.3169)  amp_scale: 1.0000 (1.0000)  time: 1.2355  data: 0.3338  max mem: 13835
[21:40:11.173953] [Train][Ep-53/100]  [1000/1435]  eta: 0:08:49  lr: 0.0004 (0.0004)  loss: 1169.5328 (1161.4442)  grad_norm: 669.7692 (684.1266)  amp_scale: 1.0000 (1.0000)  time: 1.2345  data: 0.3139  max mem: 13835
[21:42:19.801947] [Train][Ep-53/100]  [1100/1435]  eta: 0:06:49  lr: 0.0004 (0.0004)  loss: 1155.9897 (1159.8850)  grad_norm: 692.4535 (684.0544)  amp_scale: 1.0000 (1.0000)  time: 1.2163  data: 0.0171  max mem: 13835
[21:44:25.839809] [Train][Ep-53/100]  [1200/1435]  eta: 0:04:48  lr: 0.0004 (0.0004)  loss: 1167.5990 (1159.4288)  grad_norm: 682.4481 (684.4117)  amp_scale: 1.0000 (1.0000)  time: 1.2014  data: 0.3956  max mem: 13835
[21:46:22.728352] [Train][Ep-53/100]  [1300/1435]  eta: 0:02:44  lr: 0.0004 (0.0004)  loss: 1171.1309 (1161.5489)  grad_norm: 679.0032 (684.9128)  amp_scale: 1.0000 (1.0000)  time: 1.1571  data: 0.4930  max mem: 13835
[21:48:23.967660] [Train][Ep-53/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.9398)  grad_norm: 667.9927 (684.8692)  amp_scale: 1.0000 (1.0000)  time: 1.2706  data: 0.0190  max mem: 13835
[21:49:06.170333] [Train][Ep-53/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.0707)  grad_norm: 684.9609 (685.1899)  amp_scale: 1.0000 (1.0000)  time: 1.2777  data: 0.0017  max mem: 13835
[21:49:06.171427] [Train][Ep-53/100] Total time: 0:29:13 (1.2219 s / it)
[21:49:06.171855] Syncing meters...
[21:49:06.971946] Averaged stats: lr: 0.0004 (0.0004)  loss: 1153.2454 (1162.9023)  grad_norm: 684.9609 (685.1899)  amp_scale: 1.0000 (1.0000)
[21:49:17.414904] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 54)
[21:49:19.214036] [Train][Ep-54/100]  [   0/1435]  eta: 0:42:49  lr: 0.0004 (0.0004)  time: 1.7909  data: 1.3080  max mem: 13835
[21:51:14.675173] [Train][Ep-54/100]  [ 100/1435]  eta: 0:25:49  lr: 0.0004 (0.0004)  loss: 1154.0098 (1141.5965)  grad_norm: 662.6190 (670.7087)  amp_scale: 1.0000 (1.0000)  time: 1.1383  data: 0.5375  max mem: 13835
[21:53:15.475592] [Train][Ep-54/100]  [ 200/1435]  eta: 0:24:22  lr: 0.0004 (0.0004)  loss: 1211.2192 (1167.7552)  grad_norm: 692.4968 (680.8701)  amp_scale: 1.0000 (1.0000)  time: 1.1664  data: 0.4729  max mem: 13835
[21:55:17.719596] [Train][Ep-54/100]  [ 300/1435]  eta: 0:22:38  lr: 0.0004 (0.0004)  loss: 1149.4923 (1163.0624)  grad_norm: 674.8604 (680.5169)  amp_scale: 1.0000 (1.0000)  time: 1.2144  data: 0.2253  max mem: 13835
[21:57:14.323421] [Train][Ep-54/100]  [ 400/1435]  eta: 0:20:30  lr: 0.0004 (0.0004)  loss: 1130.0554 (1154.6911)  grad_norm: 687.5696 (681.8224)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.3423  max mem: 13835
[21:59:16.797740] [Train][Ep-54/100]  [ 500/1435]  eta: 0:18:38  lr: 0.0004 (0.0004)  loss: 1135.2944 (1151.4310)  grad_norm: 688.3165 (679.7705)  amp_scale: 1.0000 (1.0000)  time: 1.1940  data: 0.0203  max mem: 13835
[22:01:19.472491] [Train][Ep-54/100]  [ 600/1435]  eta: 0:16:43  lr: 0.0004 (0.0004)  loss: 1133.1404 (1146.4122)  grad_norm: 683.3227 (681.4751)  amp_scale: 1.0000 (1.0000)  time: 1.2555  data: 0.0158  max mem: 13835
[22:03:19.767961] [Train][Ep-54/100]  [ 700/1435]  eta: 0:14:43  lr: 0.0004 (0.0004)  loss: 1151.0945 (1150.4387)  grad_norm: 669.1240 (682.1746)  amp_scale: 1.0000 (1.0000)  time: 1.2891  data: 0.0011  max mem: 13835
[22:05:18.692710] [Train][Ep-54/100]  [ 800/1435]  eta: 0:12:42  lr: 0.0004 (0.0004)  loss: 1110.8517 (1147.5824)  grad_norm: 671.4208 (681.2047)  amp_scale: 1.0000 (1.0000)  time: 1.1182  data: 0.4649  max mem: 13835
[22:07:20.539910] [Train][Ep-54/100]  [ 900/1435]  eta: 0:10:43  lr: 0.0004 (0.0004)  loss: 1150.1005 (1149.5950)  grad_norm: 684.2000 (682.6451)  amp_scale: 1.0000 (1.0000)  time: 1.3289  data: 0.1444  max mem: 13835
[22:09:24.097541] [Train][Ep-54/100]  [1000/1435]  eta: 0:08:44  lr: 0.0004 (0.0004)  loss: 1127.6464 (1150.1500)  grad_norm: 679.6629 (683.3499)  amp_scale: 1.0000 (1.0000)  time: 1.1577  data: 0.1691  max mem: 13835
[22:11:24.712485] [Train][Ep-54/100]  [1100/1435]  eta: 0:06:43  lr: 0.0004 (0.0004)  loss: 1158.1869 (1153.6645)  grad_norm: 691.6543 (684.9957)  amp_scale: 1.0000 (1.0000)  time: 1.2121  data: 0.0170  max mem: 13835
[22:13:28.000086] [Train][Ep-54/100]  [1200/1435]  eta: 0:04:43  lr: 0.0004 (0.0004)  loss: 1118.6244 (1151.6636)  grad_norm: 678.9918 (685.5771)  amp_scale: 1.0000 (1.0000)  time: 1.2895  data: 0.0333  max mem: 13835
[22:15:30.272207] [Train][Ep-54/100]  [1300/1435]  eta: 0:02:43  lr: 0.0004 (0.0004)  loss: 1160.2275 (1151.1234)  grad_norm: 673.7654 (685.3501)  amp_scale: 1.0000 (1.0000)  time: 1.2428  data: 0.1345  max mem: 13835
[22:17:31.284389] [Train][Ep-54/100]  [1400/1435]  eta: 0:00:42  lr: 0.0004 (0.0004)  loss: 1156.1467 (1151.8224)  grad_norm: 678.8384 (685.3433)  amp_scale: 1.0000 (1.0000)  time: 1.2291  data: 0.6150  max mem: 13835
[22:18:11.145633] [Train][Ep-54/100]  [1434/1435]  eta: 0:00:01  lr: 0.0004 (0.0004)  loss: 1153.1736 (1150.6489)  grad_norm: 689.2122 (685.8974)  amp_scale: 1.0000 (1.0000)  time: 1.1505  data: 0.4293  max mem: 13835
[22:18:11.146526] [Train][Ep-54/100] Total time: 0:28:53 (1.2082 s / it)
[22:18:11.146992] Syncing meters...
[22:18:11.148333] Averaged stats: lr: 0.0004 (0.0004)  loss: 1153.1736 (1154.3003)  grad_norm: 689.2122 (685.8974)  amp_scale: 1.0000 (1.0000)
[22:18:19.361978] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 55)
[22:18:21.147316] [Train][Ep-55/100]  [   0/1435]  eta: 0:42:29  lr: 0.0004 (0.0004)  time: 1.7768  data: 1.2926  max mem: 13835
[22:20:20.240002] [Train][Ep-55/100]  [ 100/1435]  eta: 0:26:37  lr: 0.0004 (0.0004)  loss: 1141.2808 (1136.2970)  grad_norm: 661.8936 (666.5656)  amp_scale: 1.0000 (1.0000)  time: 1.1934  data: 0.6237  max mem: 13835
[22:22:23.275811] [Train][Ep-55/100]  [ 200/1435]  eta: 0:24:58  lr: 0.0004 (0.0004)  loss: 1151.6367 (1143.5581)  grad_norm: 659.7584 (665.8780)  amp_scale: 1.0000 (1.0000)  time: 1.2496  data: 0.6773  max mem: 13835
[22:24:22.328316] [Train][Ep-55/100]  [ 300/1435]  eta: 0:22:48  lr: 0.0004 (0.0004)  loss: 1104.6223 (1143.8972)  grad_norm: 665.2432 (668.9612)  amp_scale: 1.0000 (1.0000)  time: 1.2374  data: 0.4207  max mem: 13835
[22:26:23.939753] [Train][Ep-55/100]  [ 400/1435]  eta: 0:20:50  lr: 0.0004 (0.0004)  loss: 1123.3938 (1145.6056)  grad_norm: 685.7966 (674.5473)  amp_scale: 1.0000 (1.0000)  time: 1.2995  data: 0.1377  max mem: 13835
[22:28:25.378082] [Train][Ep-55/100]  [ 500/1435]  eta: 0:18:50  lr: 0.0004 (0.0004)  loss: 1111.0420 (1146.5607)  grad_norm: 660.1848 (675.2542)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.2359  max mem: 13835
[22:30:25.769496] [Train][Ep-55/100]  [ 600/1435]  eta: 0:16:49  lr: 0.0004 (0.0004)  loss: 1141.8640 (1145.9449)  grad_norm: 670.3505 (676.2587)  amp_scale: 1.0000 (1.0000)  time: 1.2005  data: 0.6284  max mem: 13835
[22:32:23.507280] [Train][Ep-55/100]  [ 700/1435]  eta: 0:14:45  lr: 0.0004 (0.0004)  loss: 1090.7911 (1139.5533)  grad_norm: 702.6535 (679.8163)  amp_scale: 1.0000 (1.0000)  time: 1.1791  data: 0.6016  max mem: 13835
[22:34:20.185988] [Train][Ep-55/100]  [ 800/1435]  eta: 0:12:41  lr: 0.0004 (0.0004)  loss: 1157.9999 (1141.9294)  grad_norm: 684.4780 (680.6423)  amp_scale: 1.0000 (1.0000)  time: 1.1984  data: 0.6245  max mem: 13835
[22:36:16.633958] [Train][Ep-55/100]  [ 900/1435]  eta: 0:10:39  lr: 0.0004 (0.0004)  loss: 1114.5884 (1142.7498)  grad_norm: 673.4995 (681.4554)  amp_scale: 1.0000 (1.0000)  time: 1.1411  data: 0.5456  max mem: 13835
[22:38:17.132003] [Train][Ep-55/100]  [1000/1435]  eta: 0:08:40  lr: 0.0004 (0.0004)  loss: 1140.5420 (1144.7659)  grad_norm: 691.2094 (682.4989)  amp_scale: 1.0000 (1.0000)  time: 1.1405  data: 0.2221  max mem: 13835
[22:40:16.889119] [Train][Ep-55/100]  [1100/1435]  eta: 0:06:40  lr: 0.0003 (0.0004)  loss: 1148.3146 (1144.8140)  grad_norm: 681.6050 (682.7563)  amp_scale: 1.0000 (1.0000)  time: 1.2376  data: 0.0247  max mem: 13835
[22:42:19.584596] [Train][Ep-55/100]  [1200/1435]  eta: 0:04:41  lr: 0.0003 (0.0004)  loss: 1203.3051 (1146.4615)  grad_norm: 671.5699 (681.6957)  amp_scale: 1.0000 (1.0000)  time: 1.2135  data: 0.0013  max mem: 13835
[22:44:20.690580] [Train][Ep-55/100]  [1300/1435]  eta: 0:02:42  lr: 0.0003 (0.0004)  loss: 1140.7765 (1146.7267)  grad_norm: 695.2836 (682.0185)  amp_scale: 1.0000 (1.0000)  time: 1.1467  data: 0.4988  max mem: 13835
[22:46:18.684260] [Train][Ep-55/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0004)  loss: 1168.8640 (1148.5437)  grad_norm: 672.3698 (682.3125)  amp_scale: 1.0000 (1.0000)  time: 1.2398  data: 0.2570  max mem: 13835
[22:46:58.536568] [Train][Ep-55/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0004)  loss: 1155.6774 (1148.8123)  grad_norm: 696.6046 (682.9444)  amp_scale: 1.0000 (1.0000)  time: 1.1125  data: 0.3776  max mem: 13835
[22:46:58.537603] [Train][Ep-55/100] Total time: 0:28:39 (1.1980 s / it)
[22:46:58.538098] Syncing meters...
[22:46:58.824188] Averaged stats: lr: 0.0003 (0.0004)  loss: 1155.6774 (1147.5761)  grad_norm: 696.6046 (682.9444)  amp_scale: 1.0000 (1.0000)
[22:47:07.290260] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 56)
[22:47:09.639747] [Train][Ep-56/100]  [   0/1435]  eta: 0:55:59  lr: 0.0003 (0.0003)  time: 2.3410  data: 1.8561  max mem: 13835
[22:49:04.929467] [Train][Ep-56/100]  [ 100/1435]  eta: 0:25:54  lr: 0.0003 (0.0003)  loss: 1171.2557 (1149.3444)  grad_norm: 660.5222 (681.5519)  amp_scale: 1.0000 (1.0000)  time: 1.2122  data: 0.5795  max mem: 13835
[22:51:02.640486] [Train][Ep-56/100]  [ 200/1435]  eta: 0:24:05  lr: 0.0003 (0.0003)  loss: 1157.1643 (1149.4671)  grad_norm: 689.5543 (686.4041)  amp_scale: 1.0000 (1.0000)  time: 1.1572  data: 0.2369  max mem: 13835
[22:52:58.115495] [Train][Ep-56/100]  [ 300/1435]  eta: 0:22:02  lr: 0.0003 (0.0003)  loss: 1144.9154 (1148.0577)  grad_norm: 677.5391 (681.3846)  amp_scale: 1.0000 (1.0000)  time: 1.1468  data: 0.5664  max mem: 13835
[22:54:56.237079] [Train][Ep-56/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1125.3744 (1140.7114)  grad_norm: 681.6371 (683.9067)  amp_scale: 1.0000 (1.0000)  time: 1.1891  data: 0.6165  max mem: 13835
[22:56:52.486703] [Train][Ep-56/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0003 (0.0003)  loss: 1134.6815 (1141.7886)  grad_norm: 693.0525 (686.1660)  amp_scale: 1.0000 (1.0000)  time: 1.2068  data: 0.0280  max mem: 13835
[22:58:56.389669] [Train][Ep-56/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0003 (0.0003)  loss: 1157.5814 (1141.7275)  grad_norm: 675.6992 (685.4572)  amp_scale: 1.0000 (1.0000)  time: 1.2515  data: 0.0006  max mem: 13835
[23:00:55.195567] [Train][Ep-56/100]  [ 700/1435]  eta: 0:14:27  lr: 0.0003 (0.0003)  loss: 1124.1108 (1140.3758)  grad_norm: 683.4939 (685.5036)  amp_scale: 1.0000 (1.0000)  time: 1.0967  data: 0.3211  max mem: 13835
[23:02:57.673276] [Train][Ep-56/100]  [ 800/1435]  eta: 0:12:33  lr: 0.0003 (0.0003)  loss: 1132.4875 (1138.4059)  grad_norm: 668.8890 (684.1729)  amp_scale: 1.0000 (1.0000)  time: 1.2179  data: 0.1942  max mem: 13835
[23:04:52.444287] [Train][Ep-56/100]  [ 900/1435]  eta: 0:10:32  lr: 0.0003 (0.0003)  loss: 1180.1276 (1142.8817)  grad_norm: 671.0045 (683.9162)  amp_scale: 1.0000 (1.0000)  time: 1.1070  data: 0.0915  max mem: 13835
[23:06:45.159687] [Train][Ep-56/100]  [1000/1435]  eta: 0:08:31  lr: 0.0003 (0.0003)  loss: 1136.1702 (1141.5383)  grad_norm: 685.4603 (683.6448)  amp_scale: 1.0000 (1.0000)  time: 1.0949  data: 0.3471  max mem: 13835
[23:08:41.828217] [Train][Ep-56/100]  [1100/1435]  eta: 0:06:33  lr: 0.0003 (0.0003)  loss: 1152.7764 (1140.7475)  grad_norm: 697.9684 (685.6197)  amp_scale: 1.0000 (1.0000)  time: 1.1906  data: 0.0002  max mem: 13835
[23:10:38.031418] [Train][Ep-56/100]  [1200/1435]  eta: 0:04:36  lr: 0.0003 (0.0003)  loss: 1136.8176 (1140.4033)  grad_norm: 672.1470 (685.3969)  amp_scale: 1.0000 (1.0000)  time: 1.1804  data: 0.0009  max mem: 13835
[23:12:32.816248] [Train][Ep-56/100]  [1300/1435]  eta: 0:02:38  lr: 0.0003 (0.0003)  loss: 1124.1840 (1140.5575)  grad_norm: 666.4873 (684.3965)  amp_scale: 1.0000 (1.0000)  time: 1.1916  data: 0.4459  max mem: 13835
[23:14:34.604688] [Train][Ep-56/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1142.5516 (1142.1739)  grad_norm: 685.4128 (684.6443)  amp_scale: 1.0000 (1.0000)  time: 1.2900  data: 0.0201  max mem: 13835
[23:15:15.637894] [Train][Ep-56/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1118.6050 (1142.6452)  grad_norm: 685.4128 (684.6523)  amp_scale: 1.0000 (1.0000)  time: 1.2060  data: 0.0903  max mem: 13835
[23:15:15.638854] [Train][Ep-56/100] Total time: 0:28:08 (1.1765 s / it)
[23:15:15.639333] Syncing meters...
[23:15:16.126118] Averaged stats: lr: 0.0003 (0.0003)  loss: 1118.6050 (1142.6959)  grad_norm: 685.4128 (684.6523)  amp_scale: 1.0000 (1.0000)
[23:15:25.796749] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 57)
[23:15:27.976318] [Train][Ep-57/100]  [   0/1435]  eta: 0:51:55  lr: 0.0003 (0.0003)  time: 2.1712  data: 1.6876  max mem: 13835
[23:17:22.446020] [Train][Ep-57/100]  [ 100/1435]  eta: 0:25:41  lr: 0.0003 (0.0003)  loss: 1166.3811 (1164.5284)  grad_norm: 660.1027 (674.4946)  amp_scale: 1.0000 (1.0000)  time: 1.1325  data: 0.2718  max mem: 13835
[23:19:14.657367] [Train][Ep-57/100]  [ 200/1435]  eta: 0:23:26  lr: 0.0003 (0.0003)  loss: 1148.8130 (1148.1609)  grad_norm: 663.6804 (673.4579)  amp_scale: 1.0000 (1.0000)  time: 1.1313  data: 0.0910  max mem: 13835
[23:21:05.469947] [Train][Ep-57/100]  [ 300/1435]  eta: 0:21:20  lr: 0.0003 (0.0003)  loss: 1163.1217 (1151.8936)  grad_norm: 688.0098 (681.1293)  amp_scale: 1.0000 (1.0000)  time: 1.1270  data: 0.3344  max mem: 13835
[23:23:02.842688] [Train][Ep-57/100]  [ 400/1435]  eta: 0:19:39  lr: 0.0003 (0.0003)  loss: 1153.6860 (1149.7753)  grad_norm: 691.8072 (685.7582)  amp_scale: 1.0000 (1.0000)  time: 1.1846  data: 0.0007  max mem: 13835
[23:25:01.686498] [Train][Ep-57/100]  [ 500/1435]  eta: 0:17:54  lr: 0.0003 (0.0003)  loss: 1134.9458 (1147.4420)  grad_norm: 674.8597 (684.7729)  amp_scale: 1.0000 (1.0000)  time: 1.1780  data: 0.0439  max mem: 13835
[23:26:56.526743] [Train][Ep-57/100]  [ 600/1435]  eta: 0:15:59  lr: 0.0003 (0.0003)  loss: 1112.9624 (1142.1629)  grad_norm: 685.6636 (684.8090)  amp_scale: 1.0000 (1.0000)  time: 1.1137  data: 0.0827  max mem: 13835
[23:28:51.892590] [Train][Ep-57/100]  [ 700/1435]  eta: 0:14:05  lr: 0.0003 (0.0003)  loss: 1089.9392 (1136.0436)  grad_norm: 670.7135 (683.6975)  amp_scale: 1.0000 (1.0000)  time: 1.1527  data: 0.2234  max mem: 13835
[23:30:47.847802] [Train][Ep-57/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0003 (0.0003)  loss: 1158.0289 (1138.8569)  grad_norm: 682.6401 (684.0961)  amp_scale: 1.0000 (1.0000)  time: 1.1856  data: 0.1746  max mem: 13835
[23:32:41.270869] [Train][Ep-57/100]  [ 900/1435]  eta: 0:10:14  lr: 0.0003 (0.0003)  loss: 1153.8628 (1139.0711)  grad_norm: 680.1334 (683.2176)  amp_scale: 1.0000 (1.0000)  time: 1.1139  data: 0.1262  max mem: 13835
[23:34:32.871826] [Train][Ep-57/100]  [1000/1435]  eta: 0:08:18  lr: 0.0003 (0.0003)  loss: 1137.6764 (1140.5236)  grad_norm: 704.2466 (685.1673)  amp_scale: 1.0000 (1.0000)  time: 1.1038  data: 0.3665  max mem: 13835
[23:36:34.977751] [Train][Ep-57/100]  [1100/1435]  eta: 0:06:26  lr: 0.0003 (0.0003)  loss: 1111.9238 (1139.9551)  grad_norm: 679.0955 (683.9455)  amp_scale: 1.0000 (1.0000)  time: 1.1804  data: 0.4552  max mem: 13835
[23:38:33.124439] [Train][Ep-57/100]  [1200/1435]  eta: 0:04:31  lr: 0.0003 (0.0003)  loss: 1120.8940 (1139.7984)  grad_norm: 653.8906 (682.8337)  amp_scale: 1.0000 (1.0000)  time: 1.1785  data: 0.1015  max mem: 13835
[23:40:28.943984] [Train][Ep-57/100]  [1300/1435]  eta: 0:02:35  lr: 0.0003 (0.0003)  loss: 1103.7347 (1138.4977)  grad_norm: 695.6777 (683.7147)  amp_scale: 1.0000 (1.0000)  time: 1.1890  data: 0.1213  max mem: 13835
[23:42:26.589742] [Train][Ep-57/100]  [1400/1435]  eta: 0:00:40  lr: 0.0003 (0.0003)  loss: 1130.4768 (1139.2786)  grad_norm: 691.8406 (684.2135)  amp_scale: 1.0000 (1.0000)  time: 1.2423  data: 0.2411  max mem: 13835
[23:43:06.086102] [Train][Ep-57/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1130.4768 (1140.1219)  grad_norm: 682.9067 (684.2416)  amp_scale: 1.0000 (1.0000)  time: 1.1986  data: 0.2395  max mem: 13835
[23:43:06.086952] [Train][Ep-57/100] Total time: 0:27:40 (1.1570 s / it)
[23:43:06.087467] Syncing meters...
[23:43:06.540055] Averaged stats: lr: 0.0003 (0.0003)  loss: 1130.4768 (1137.6848)  grad_norm: 682.9067 (684.2416)  amp_scale: 1.0000 (1.0000)
[23:43:14.996928] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 58)
[23:43:17.012279] [Train][Ep-58/100]  [   0/1435]  eta: 0:48:00  lr: 0.0003 (0.0003)  time: 2.0070  data: 1.5237  max mem: 13835
[23:45:12.486476] [Train][Ep-58/100]  [ 100/1435]  eta: 0:25:52  lr: 0.0003 (0.0003)  loss: 1123.9567 (1136.3262)  grad_norm: 679.7964 (682.1668)  amp_scale: 1.0000 (1.0000)  time: 1.1698  data: 0.5968  max mem: 13835
[23:47:13.697270] [Train][Ep-58/100]  [ 200/1435]  eta: 0:24:26  lr: 0.0003 (0.0003)  loss: 1121.3147 (1130.5098)  grad_norm: 658.5123 (679.9005)  amp_scale: 1.0000 (1.0000)  time: 1.2438  data: 0.6754  max mem: 13835
[23:49:13.017759] [Train][Ep-58/100]  [ 300/1435]  eta: 0:22:29  lr: 0.0003 (0.0003)  loss: 1119.1896 (1132.8614)  grad_norm: 671.0079 (680.3062)  amp_scale: 1.0000 (1.0000)  time: 1.1570  data: 0.5844  max mem: 13835
[23:51:08.672401] [Train][Ep-58/100]  [ 400/1435]  eta: 0:20:22  lr: 0.0003 (0.0003)  loss: 1118.3818 (1129.6602)  grad_norm: 680.8212 (681.9142)  amp_scale: 1.0000 (1.0000)  time: 1.1978  data: 0.6275  max mem: 13835
[23:53:07.747398] [Train][Ep-58/100]  [ 500/1435]  eta: 0:18:26  lr: 0.0003 (0.0003)  loss: 1129.3602 (1134.2819)  grad_norm: 666.3541 (679.0897)  amp_scale: 1.0000 (1.0000)  time: 1.2545  data: 0.6834  max mem: 13835
[23:55:00.346386] [Train][Ep-58/100]  [ 600/1435]  eta: 0:16:19  lr: 0.0003 (0.0003)  loss: 1116.7169 (1134.6632)  grad_norm: 682.3671 (679.5563)  amp_scale: 1.0000 (1.0000)  time: 1.1484  data: 0.5798  max mem: 13835
[23:56:55.074598] [Train][Ep-58/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0003 (0.0003)  loss: 1105.2561 (1135.1032)  grad_norm: 677.1669 (678.9622)  amp_scale: 1.0000 (1.0000)  time: 1.1257  data: 0.1328  max mem: 13835
[23:58:48.982913] [Train][Ep-58/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0003 (0.0003)  loss: 1122.3890 (1132.4289)  grad_norm: 666.1663 (678.4088)  amp_scale: 1.0000 (1.0000)  time: 1.2112  data: 0.0773  max mem: 13835
[00:00:47.145657] [Train][Ep-58/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0003 (0.0003)  loss: 1147.1532 (1134.2992)  grad_norm: 661.0896 (677.4008)  amp_scale: 1.0000 (1.0000)  time: 1.1493  data: 0.1662  max mem: 13835
[00:02:50.006134] [Train][Ep-58/100]  [1000/1435]  eta: 0:08:30  lr: 0.0003 (0.0003)  loss: 1089.9968 (1131.6343)  grad_norm: 690.9485 (678.8664)  amp_scale: 1.0000 (1.0000)  time: 1.2388  data: 0.0028  max mem: 13835
[00:04:50.015858] [Train][Ep-58/100]  [1100/1435]  eta: 0:06:34  lr: 0.0003 (0.0003)  loss: 1144.5507 (1132.6220)  grad_norm: 684.4959 (679.5155)  amp_scale: 1.0000 (1.0000)  time: 1.2591  data: 0.0136  max mem: 13835
[00:06:52.165554] [Train][Ep-58/100]  [1200/1435]  eta: 0:04:37  lr: 0.0003 (0.0003)  loss: 1134.5656 (1132.8019)  grad_norm: 683.3121 (678.9261)  amp_scale: 1.0000 (1.0000)  time: 1.2159  data: 0.0020  max mem: 13835
[00:08:51.071763] [Train][Ep-58/100]  [1300/1435]  eta: 0:02:39  lr: 0.0003 (0.0003)  loss: 1150.5400 (1134.0634)  grad_norm: 681.1357 (679.6273)  amp_scale: 1.0000 (1.0000)  time: 1.2163  data: 0.6449  max mem: 13835
[00:10:49.737179] [Train][Ep-58/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1135.3921 (1134.7242)  grad_norm: 670.3719 (679.7641)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.1663  max mem: 13835
[00:11:30.581303] [Train][Ep-58/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1099.1763 (1133.5599)  grad_norm: 645.7031 (679.4032)  amp_scale: 1.0000 (1.0000)  time: 1.2172  data: 0.0010  max mem: 13835
[00:11:30.582170] [Train][Ep-58/100] Total time: 0:28:15 (1.1816 s / it)
[00:11:30.582663] Syncing meters...
[00:11:30.926941] Averaged stats: lr: 0.0003 (0.0003)  loss: 1099.1763 (1132.5552)  grad_norm: 645.7031 (679.4032)  amp_scale: 1.0000 (1.0000)
[00:11:40.739031] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 59)
[00:11:42.578797] [Train][Ep-59/100]  [   0/1435]  eta: 0:43:48  lr: 0.0003 (0.0003)  time: 1.8316  data: 1.3472  max mem: 13835
[00:13:37.871907] [Train][Ep-59/100]  [ 100/1435]  eta: 0:25:48  lr: 0.0003 (0.0003)  loss: 1121.9197 (1127.0439)  grad_norm: 669.4777 (671.0205)  amp_scale: 1.0000 (1.0000)  time: 1.1275  data: 0.5560  max mem: 13835
[00:15:30.704097] [Train][Ep-59/100]  [ 200/1435]  eta: 0:23:32  lr: 0.0003 (0.0003)  loss: 1122.8838 (1129.6525)  grad_norm: 667.5913 (674.4738)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.5471  max mem: 13835
[00:17:30.637969] [Train][Ep-59/100]  [ 300/1435]  eta: 0:21:59  lr: 0.0003 (0.0003)  loss: 1131.4346 (1130.4302)  grad_norm: 667.4335 (675.7158)  amp_scale: 1.0000 (1.0000)  time: 1.2679  data: 0.0696  max mem: 13835
[00:19:28.004494] [Train][Ep-59/100]  [ 400/1435]  eta: 0:20:05  lr: 0.0003 (0.0003)  loss: 1122.6986 (1131.1573)  grad_norm: 690.5157 (679.4574)  amp_scale: 1.0000 (1.0000)  time: 1.1541  data: 0.4166  max mem: 13835
[00:21:31.920934] [Train][Ep-59/100]  [ 500/1435]  eta: 0:18:23  lr: 0.0003 (0.0003)  loss: 1121.2173 (1134.8589)  grad_norm: 666.8882 (679.1541)  amp_scale: 1.0000 (1.0000)  time: 1.1930  data: 0.6240  max mem: 13835
[00:23:28.193161] [Train][Ep-59/100]  [ 600/1435]  eta: 0:16:22  lr: 0.0003 (0.0003)  loss: 1148.8973 (1137.8164)  grad_norm: 676.7567 (679.0790)  amp_scale: 1.0000 (1.0000)  time: 1.1692  data: 0.2424  max mem: 13835
[00:25:32.015972] [Train][Ep-59/100]  [ 700/1435]  eta: 0:14:31  lr: 0.0003 (0.0003)  loss: 1112.8109 (1135.4391)  grad_norm: 667.3809 (678.5201)  amp_scale: 1.0000 (1.0000)  time: 1.2194  data: 0.2517  max mem: 13835
[00:27:37.523877] [Train][Ep-59/100]  [ 800/1435]  eta: 0:12:38  lr: 0.0003 (0.0003)  loss: 1104.4401 (1132.4668)  grad_norm: 658.2122 (677.6956)  amp_scale: 1.0000 (1.0000)  time: 1.2705  data: 0.1050  max mem: 13835
[00:29:35.858597] [Train][Ep-59/100]  [ 900/1435]  eta: 0:10:38  lr: 0.0003 (0.0003)  loss: 1092.6887 (1130.4556)  grad_norm: 692.7753 (679.1812)  amp_scale: 1.0000 (1.0000)  time: 1.1475  data: 0.1170  max mem: 13835
[00:31:30.263964] [Train][Ep-59/100]  [1000/1435]  eta: 0:08:36  lr: 0.0003 (0.0003)  loss: 1126.9747 (1130.6399)  grad_norm: 690.8911 (680.1996)  amp_scale: 1.0000 (1.0000)  time: 1.2026  data: 0.0468  max mem: 13835
[00:33:32.194873] [Train][Ep-59/100]  [1100/1435]  eta: 0:06:39  lr: 0.0003 (0.0003)  loss: 1139.3704 (1130.7458)  grad_norm: 669.4537 (679.8428)  amp_scale: 1.0000 (1.0000)  time: 1.1849  data: 0.0088  max mem: 13835
[00:35:26.403834] [Train][Ep-59/100]  [1200/1435]  eta: 0:04:38  lr: 0.0003 (0.0003)  loss: 1094.7814 (1130.0162)  grad_norm: 662.0041 (679.8365)  amp_scale: 1.0000 (1.0000)  time: 1.1676  data: 0.2486  max mem: 13835
[00:37:31.174856] [Train][Ep-59/100]  [1300/1435]  eta: 0:02:40  lr: 0.0003 (0.0003)  loss: 1114.6605 (1130.3766)  grad_norm: 654.4799 (679.0791)  amp_scale: 1.0000 (1.0000)  time: 1.2487  data: 0.0679  max mem: 13835
[00:39:35.479840] [Train][Ep-59/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.1259)  grad_norm: 675.0012 (678.7607)  amp_scale: 1.0000 (1.0000)  time: 1.2115  data: 0.1826  max mem: 13835
[00:40:13.544532] [Train][Ep-59/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.0173)  grad_norm: 676.5873 (678.4562)  amp_scale: 1.0000 (1.0000)  time: 1.1195  data: 0.1690  max mem: 13835
[00:40:13.545321] [Train][Ep-59/100] Total time: 0:28:32 (1.1936 s / it)
[00:40:13.545703] Syncing meters...
[00:40:14.534879] Averaged stats: lr: 0.0003 (0.0003)  loss: 1122.9279 (1130.7311)  grad_norm: 676.5873 (678.4562)  amp_scale: 1.0000 (1.0000)
[00:40:23.072528] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 60)
[00:40:25.035474] [Train][Ep-60/100]  [   0/1435]  eta: 0:46:44  lr: 0.0003 (0.0003)  time: 1.9546  data: 1.4695  max mem: 13835
[00:42:21.111501] [Train][Ep-60/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0003 (0.0003)  loss: 1094.6414 (1116.9438)  grad_norm: 694.8044 (697.4207)  amp_scale: 1.0000 (1.0000)  time: 1.1487  data: 0.5433  max mem: 13835
[00:44:18.665005] [Train][Ep-60/100]  [ 200/1435]  eta: 0:24:07  lr: 0.0003 (0.0003)  loss: 1117.0344 (1113.3056)  grad_norm: 668.2070 (687.3980)  amp_scale: 1.0000 (1.0000)  time: 1.1791  data: 0.3261  max mem: 13835
[00:46:12.281303] [Train][Ep-60/100]  [ 300/1435]  eta: 0:21:56  lr: 0.0003 (0.0003)  loss: 1126.1998 (1120.8908)  grad_norm: 673.8222 (686.5567)  amp_scale: 1.0000 (1.0000)  time: 1.1295  data: 0.3297  max mem: 13835
[00:48:12.122154] [Train][Ep-60/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1130.1552 (1125.0227)  grad_norm: 672.2643 (683.5852)  amp_scale: 1.0000 (1.0000)  time: 1.2429  data: 0.0033  max mem: 13835
[00:50:11.196588] [Train][Ep-60/100]  [ 500/1435]  eta: 0:18:17  lr: 0.0003 (0.0003)  loss: 1133.7047 (1129.8838)  grad_norm: 660.3648 (681.4690)  amp_scale: 1.0000 (1.0000)  time: 1.1497  data: 0.4981  max mem: 13835
[00:52:14.155874] [Train][Ep-60/100]  [ 600/1435]  eta: 0:16:27  lr: 0.0003 (0.0003)  loss: 1167.6117 (1133.9240)  grad_norm: 683.9601 (683.5630)  amp_scale: 1.0000 (1.0000)  time: 1.2460  data: 0.6714  max mem: 13835
[00:54:14.641025] [Train][Ep-60/100]  [ 700/1435]  eta: 0:14:31  lr: 0.0003 (0.0003)  loss: 1099.5969 (1131.0482)  grad_norm: 676.0014 (682.6267)  amp_scale: 1.0000 (1.0000)  time: 1.1268  data: 0.5566  max mem: 13835
[00:56:08.470202] [Train][Ep-60/100]  [ 800/1435]  eta: 0:12:29  lr: 0.0003 (0.0003)  loss: 1075.4445 (1128.7306)  grad_norm: 674.8703 (681.7621)  amp_scale: 1.0000 (1.0000)  time: 1.1797  data: 0.2166  max mem: 13835
[00:58:05.535190] [Train][Ep-60/100]  [ 900/1435]  eta: 0:10:30  lr: 0.0003 (0.0003)  loss: 1121.8022 (1129.3915)  grad_norm: 660.2281 (681.4753)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.0156  max mem: 13835
[01:00:08.764445] [Train][Ep-60/100]  [1000/1435]  eta: 0:08:35  lr: 0.0003 (0.0003)  loss: 1093.3651 (1127.1926)  grad_norm: 686.1987 (682.5075)  amp_scale: 1.0000 (1.0000)  time: 1.2857  data: 0.0270  max mem: 13835
[01:02:04.984669] [Train][Ep-60/100]  [1100/1435]  eta: 0:06:36  lr: 0.0003 (0.0003)  loss: 1124.1228 (1126.2518)  grad_norm: 701.0429 (685.0812)  amp_scale: 1.0000 (1.0000)  time: 1.1309  data: 0.2347  max mem: 13835
[01:04:00.766599] [Train][Ep-60/100]  [1200/1435]  eta: 0:04:37  lr: 0.0003 (0.0003)  loss: 1105.4857 (1126.4349)  grad_norm: 685.3036 (685.6680)  amp_scale: 1.0000 (1.0000)  time: 1.1076  data: 0.4596  max mem: 13835
[01:05:58.012380] [Train][Ep-60/100]  [1300/1435]  eta: 0:02:39  lr: 0.0003 (0.0003)  loss: 1130.5465 (1127.4025)  grad_norm: 680.1862 (685.3178)  amp_scale: 1.0000 (1.0000)  time: 1.1188  data: 0.4483  max mem: 13835
[01:07:58.898693] [Train][Ep-60/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1112.8419 (1127.8726)  grad_norm: 668.3009 (684.4726)  amp_scale: 1.0000 (1.0000)  time: 1.2231  data: 0.0761  max mem: 13835
[01:08:39.671287] [Train][Ep-60/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1112.8419 (1127.4339)  grad_norm: 684.4253 (684.4345)  amp_scale: 1.0000 (1.0000)  time: 1.2247  data: 0.0513  max mem: 13835
[01:08:39.672158] [Train][Ep-60/100] Total time: 0:28:16 (1.1823 s / it)
[01:08:39.672549] Syncing meters...
[01:08:40.119617] Averaged stats: lr: 0.0003 (0.0003)  loss: 1112.8419 (1125.8958)  grad_norm: 684.4253 (684.4345)  amp_scale: 1.0000 (1.0000)
[01:08:42.257231] [Eval][Ep-60/100]  [  0/121]  eta: 0:04:17  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.1297  data: 1.9686  max mem: 13835
[01:10:26.333891] [Eval][Ep-60/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9328  data: 0.7732  max mem: 13835
[01:10:44.274018] [Eval][Ep-60/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8968  data: 0.7399  max mem: 13835
[01:10:44.274885] [Eval][Ep-60/100] Total time: 0:02:04 (1.0260 s / it)
[01:10:45.674491] [Eval][Ep-60/100] val_acc1_image=27.69 | val_acc1_audio=41.80 | val_acc1_fusion=38.59 | val_acc1_all=52.43
[01:10:55.227662] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 61)
[01:10:57.135092] [Train][Ep-61/100]  [   0/1435]  eta: 0:45:25  lr: 0.0003 (0.0003)  time: 1.8993  data: 1.4193  max mem: 13835
[01:12:56.818908] [Train][Ep-61/100]  [ 100/1435]  eta: 0:26:47  lr: 0.0003 (0.0003)  loss: 1133.1208 (1150.3961)  grad_norm: 680.7669 (688.3679)  amp_scale: 1.0000 (1.0000)  time: 1.1259  data: 0.5549  max mem: 13835
[01:14:51.322155] [Train][Ep-61/100]  [ 200/1435]  eta: 0:24:10  lr: 0.0003 (0.0003)  loss: 1103.6975 (1132.3689)  grad_norm: 676.0978 (682.5214)  amp_scale: 1.0000 (1.0000)  time: 1.1893  data: 0.0011  max mem: 13835
[01:16:49.796544] [Train][Ep-61/100]  [ 300/1435]  eta: 0:22:16  lr: 0.0003 (0.0003)  loss: 1105.2249 (1132.4882)  grad_norm: 674.3769 (682.4874)  amp_scale: 1.0000 (1.0000)  time: 1.1665  data: 0.1671  max mem: 13835
[01:18:44.162750] [Train][Ep-61/100]  [ 400/1435]  eta: 0:20:10  lr: 0.0003 (0.0003)  loss: 1103.2438 (1126.4060)  grad_norm: 679.1340 (683.4138)  amp_scale: 1.0000 (1.0000)  time: 1.1853  data: 0.3185  max mem: 13835
[01:20:40.999440] [Train][Ep-61/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0003 (0.0003)  loss: 1128.9828 (1128.8012)  grad_norm: 670.5722 (679.8823)  amp_scale: 1.0000 (1.0000)  time: 1.1249  data: 0.2621  max mem: 13835
[01:22:38.758955] [Train][Ep-61/100]  [ 600/1435]  eta: 0:16:17  lr: 0.0003 (0.0003)  loss: 1157.7257 (1131.5505)  grad_norm: 676.5739 (680.6669)  amp_scale: 1.0000 (1.0000)  time: 1.2091  data: 0.1047  max mem: 13835
[01:24:40.130726] [Train][Ep-61/100]  [ 700/1435]  eta: 0:14:24  lr: 0.0003 (0.0003)  loss: 1101.3042 (1127.8022)  grad_norm: 668.1329 (680.1843)  amp_scale: 1.0000 (1.0000)  time: 1.2184  data: 0.5903  max mem: 13835
[01:26:39.264249] [Train][Ep-61/100]  [ 800/1435]  eta: 0:12:28  lr: 0.0003 (0.0003)  loss: 1109.9492 (1128.2480)  grad_norm: 691.9919 (682.5843)  amp_scale: 1.0000 (1.0000)  time: 1.1760  data: 0.4900  max mem: 13835
[01:28:42.668740] [Train][Ep-61/100]  [ 900/1435]  eta: 0:10:33  lr: 0.0003 (0.0003)  loss: 1113.0869 (1127.8763)  grad_norm: 658.7227 (680.5302)  amp_scale: 1.0000 (1.0000)  time: 1.2412  data: 0.2014  max mem: 13835
[01:30:40.629807] [Train][Ep-61/100]  [1000/1435]  eta: 0:08:35  lr: 0.0003 (0.0003)  loss: 1111.9211 (1128.5359)  grad_norm: 679.6602 (681.1189)  amp_scale: 1.0000 (1.0000)  time: 1.2043  data: 0.6325  max mem: 13835
[01:32:41.612414] [Train][Ep-61/100]  [1100/1435]  eta: 0:06:37  lr: 0.0003 (0.0003)  loss: 1086.7367 (1127.5022)  grad_norm: 670.7905 (680.2675)  amp_scale: 1.0000 (1.0000)  time: 1.1821  data: 0.0370  max mem: 13835
[01:34:44.325658] [Train][Ep-61/100]  [1200/1435]  eta: 0:04:39  lr: 0.0003 (0.0003)  loss: 1107.8143 (1126.4177)  grad_norm: 692.2354 (681.6744)  amp_scale: 1.0000 (1.0000)  time: 1.2067  data: 0.0298  max mem: 13835
[01:36:39.682096] [Train][Ep-61/100]  [1300/1435]  eta: 0:02:40  lr: 0.0003 (0.0003)  loss: 1132.4368 (1126.2208)  grad_norm: 669.4879 (681.5297)  amp_scale: 1.0000 (1.0000)  time: 1.1118  data: 0.5053  max mem: 13835
[01:38:38.240783] [Train][Ep-61/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1103.1010 (1125.1523)  grad_norm: 683.2739 (682.2953)  amp_scale: 1.0000 (1.0000)  time: 1.2185  data: 0.0550  max mem: 13835
[01:39:16.782545] [Train][Ep-61/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1116.8125 (1125.9831)  grad_norm: 683.2739 (682.5439)  amp_scale: 1.0000 (1.0000)  time: 1.1380  data: 0.1861  max mem: 13835
[01:39:16.783495] [Train][Ep-61/100] Total time: 0:28:21 (1.1857 s / it)
[01:39:16.783909] Syncing meters...
[01:39:16.846804] Averaged stats: lr: 0.0003 (0.0003)  loss: 1116.8125 (1122.5772)  grad_norm: 683.2739 (682.5439)  amp_scale: 1.0000 (1.0000)
[01:39:27.725498] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 62)
[01:39:29.965107] [Train][Ep-62/100]  [   0/1435]  eta: 0:53:21  lr: 0.0003 (0.0003)  time: 2.2310  data: 1.7483  max mem: 13835
[01:41:26.449512] [Train][Ep-62/100]  [ 100/1435]  eta: 0:26:09  lr: 0.0003 (0.0003)  loss: 1084.2795 (1094.1282)  grad_norm: 665.6312 (669.8192)  amp_scale: 1.0000 (1.0000)  time: 1.1290  data: 0.5486  max mem: 13835
[01:43:22.948591] [Train][Ep-62/100]  [ 200/1435]  eta: 0:24:05  lr: 0.0003 (0.0003)  loss: 1113.1803 (1101.5155)  grad_norm: 673.4159 (674.8770)  amp_scale: 1.0000 (1.0000)  time: 1.0464  data: 0.4097  max mem: 13835
[01:45:17.282870] [Train][Ep-62/100]  [ 300/1435]  eta: 0:21:58  lr: 0.0003 (0.0003)  loss: 1087.4833 (1098.4277)  grad_norm: 683.3630 (678.7905)  amp_scale: 1.0000 (1.0000)  time: 1.1656  data: 0.3198  max mem: 13835
[01:47:12.218964] [Train][Ep-62/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0003 (0.0003)  loss: 1130.2765 (1103.6864)  grad_norm: 672.6752 (680.2137)  amp_scale: 1.0000 (1.0000)  time: 1.1103  data: 0.3664  max mem: 13835
[01:49:13.542805] [Train][Ep-62/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0003 (0.0003)  loss: 1100.8850 (1102.6706)  grad_norm: 697.6655 (683.3400)  amp_scale: 1.0000 (1.0000)  time: 1.1349  data: 0.3093  max mem: 13835
[01:51:06.610284] [Train][Ep-62/100]  [ 600/1435]  eta: 0:16:10  lr: 0.0003 (0.0003)  loss: 1107.1379 (1106.9146)  grad_norm: 696.6996 (686.1558)  amp_scale: 1.0000 (1.0000)  time: 1.1346  data: 0.5605  max mem: 13835
[01:53:07.262616] [Train][Ep-62/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0003 (0.0003)  loss: 1104.1501 (1106.5059)  grad_norm: 672.9807 (684.1393)  amp_scale: 1.0000 (1.0000)  time: 1.2382  data: 0.2465  max mem: 13835
[01:55:07.558483] [Train][Ep-62/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0003 (0.0003)  loss: 1094.3663 (1106.6942)  grad_norm: 663.7806 (682.8447)  amp_scale: 1.0000 (1.0000)  time: 1.1732  data: 0.0609  max mem: 13835
[01:57:06.780322] [Train][Ep-62/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0003 (0.0003)  loss: 1104.5276 (1107.3638)  grad_norm: 689.1499 (684.5376)  amp_scale: 1.0000 (1.0000)  time: 1.2481  data: 0.0224  max mem: 13835
[01:59:08.124566] [Train][Ep-62/100]  [1000/1435]  eta: 0:08:32  lr: 0.0003 (0.0003)  loss: 1095.9692 (1108.2892)  grad_norm: 665.7405 (683.7555)  amp_scale: 1.0000 (1.0000)  time: 1.2215  data: 0.2051  max mem: 13835
[02:01:00.084167] [Train][Ep-62/100]  [1100/1435]  eta: 0:06:33  lr: 0.0003 (0.0003)  loss: 1088.6865 (1108.0730)  grad_norm: 698.0090 (684.7496)  amp_scale: 1.0000 (1.0000)  time: 1.0784  data: 0.4437  max mem: 13835
[02:02:56.246050] [Train][Ep-62/100]  [1200/1435]  eta: 0:04:35  lr: 0.0003 (0.0003)  loss: 1079.3174 (1107.5765)  grad_norm: 675.9305 (684.1154)  amp_scale: 1.0000 (1.0000)  time: 1.1326  data: 0.1971  max mem: 13835
[02:04:55.685861] [Train][Ep-62/100]  [1300/1435]  eta: 0:02:38  lr: 0.0003 (0.0003)  loss: 1111.2739 (1107.8020)  grad_norm: 659.0616 (683.3970)  amp_scale: 1.0000 (1.0000)  time: 1.2600  data: 0.2093  max mem: 13835
[02:06:50.586903] [Train][Ep-62/100]  [1400/1435]  eta: 0:00:41  lr: 0.0003 (0.0003)  loss: 1122.8087 (1109.5764)  grad_norm: 697.5992 (684.3106)  amp_scale: 1.0000 (1.0000)  time: 1.1383  data: 0.4506  max mem: 13835
[02:07:29.785279] [Train][Ep-62/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1097.4268 (1109.2787)  grad_norm: 701.2114 (684.6932)  amp_scale: 1.0000 (1.0000)  time: 1.2113  data: 0.6254  max mem: 13835
[02:07:29.786236] [Train][Ep-62/100] Total time: 0:28:02 (1.1722 s / it)
[02:07:29.786791] Syncing meters...
[02:07:29.804258] Averaged stats: lr: 0.0003 (0.0003)  loss: 1097.4268 (1115.7999)  grad_norm: 701.2114 (684.6932)  amp_scale: 1.0000 (1.0000)
[02:07:38.290543] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 63)
[02:07:40.168811] [Train][Ep-63/100]  [   0/1435]  eta: 0:44:43  lr: 0.0003 (0.0003)  time: 1.8702  data: 1.3881  max mem: 13835
[02:09:35.406777] [Train][Ep-63/100]  [ 100/1435]  eta: 0:25:47  lr: 0.0003 (0.0003)  loss: 1127.2312 (1122.3156)  grad_norm: 683.0700 (691.6490)  amp_scale: 1.0000 (1.0000)  time: 1.1482  data: 0.5233  max mem: 13835
[02:11:33.862684] [Train][Ep-63/100]  [ 200/1435]  eta: 0:24:07  lr: 0.0003 (0.0003)  loss: 1077.9928 (1100.7412)  grad_norm: 655.8652 (676.3162)  amp_scale: 1.0000 (1.0000)  time: 1.1245  data: 0.1261  max mem: 13835
[02:13:28.451891] [Train][Ep-63/100]  [ 300/1435]  eta: 0:22:00  lr: 0.0003 (0.0003)  loss: 1058.6890 (1095.1634)  grad_norm: 696.1917 (681.2079)  amp_scale: 1.0000 (1.0000)  time: 1.1178  data: 0.2428  max mem: 13835
[02:15:22.115512] [Train][Ep-63/100]  [ 400/1435]  eta: 0:19:57  lr: 0.0003 (0.0003)  loss: 1099.1813 (1100.1712)  grad_norm: 655.0016 (680.3980)  amp_scale: 1.0000 (1.0000)  time: 1.1505  data: 0.2822  max mem: 13835
[02:17:19.936517] [Train][Ep-63/100]  [ 500/1435]  eta: 0:18:05  lr: 0.0003 (0.0003)  loss: 1094.4032 (1099.3642)  grad_norm: 684.9783 (682.6840)  amp_scale: 1.0000 (1.0000)  time: 1.2100  data: 0.0603  max mem: 13835
[02:19:17.572505] [Train][Ep-63/100]  [ 600/1435]  eta: 0:16:11  lr: 0.0003 (0.0003)  loss: 1113.1714 (1102.6037)  grad_norm: 678.0714 (682.9693)  amp_scale: 1.0000 (1.0000)  time: 1.1779  data: 0.2084  max mem: 13835
[02:21:14.081109] [Train][Ep-63/100]  [ 700/1435]  eta: 0:14:15  lr: 0.0003 (0.0003)  loss: 1082.6757 (1102.8616)  grad_norm: 683.0816 (683.1301)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.0285  max mem: 13835
[02:23:13.068540] [Train][Ep-63/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0003 (0.0003)  loss: 1097.2981 (1105.2892)  grad_norm: 661.6086 (682.3084)  amp_scale: 1.0000 (1.0000)  time: 1.1497  data: 0.2267  max mem: 13835
[02:25:07.848270] [Train][Ep-63/100]  [ 900/1435]  eta: 0:10:23  lr: 0.0003 (0.0003)  loss: 1078.7463 (1103.5108)  grad_norm: 699.9334 (684.3723)  amp_scale: 1.0000 (1.0000)  time: 1.0813  data: 0.2796  max mem: 13835
[02:27:04.804756] [Train][Ep-63/100]  [1000/1435]  eta: 0:08:26  lr: 0.0003 (0.0003)  loss: 1106.3481 (1105.3922)  grad_norm: 677.9419 (685.0056)  amp_scale: 1.0000 (1.0000)  time: 1.2175  data: 0.0114  max mem: 13835
[02:29:01.567675] [Train][Ep-63/100]  [1100/1435]  eta: 0:06:30  lr: 0.0003 (0.0003)  loss: 1071.8944 (1104.5392)  grad_norm: 668.4068 (684.4278)  amp_scale: 1.0000 (1.0000)  time: 1.2102  data: 0.2216  max mem: 13835
[02:30:57.050158] [Train][Ep-63/100]  [1200/1435]  eta: 0:04:33  lr: 0.0003 (0.0003)  loss: 1078.4426 (1104.4145)  grad_norm: 666.8069 (684.0612)  amp_scale: 1.0000 (1.0000)  time: 1.1480  data: 0.0136  max mem: 13835
[02:32:58.733987] [Train][Ep-63/100]  [1300/1435]  eta: 0:02:37  lr: 0.0003 (0.0003)  loss: 1102.5288 (1105.9231)  grad_norm: 678.6703 (684.1075)  amp_scale: 1.0000 (1.0000)  time: 1.2580  data: 0.6899  max mem: 13835
[02:34:56.630633] [Train][Ep-63/100]  [1400/1435]  eta: 0:00:40  lr: 0.0003 (0.0003)  loss: 1136.2181 (1107.2422)  grad_norm: 682.5721 (684.0297)  amp_scale: 1.0000 (1.0000)  time: 1.1664  data: 0.1718  max mem: 13835
[02:35:33.060757] [Train][Ep-63/100]  [1434/1435]  eta: 0:00:01  lr: 0.0003 (0.0003)  loss: 1122.5685 (1107.6587)  grad_norm: 678.9230 (683.6287)  amp_scale: 1.0000 (1.0000)  time: 1.0457  data: 0.1294  max mem: 13835
[02:35:33.061595] [Train][Ep-63/100] Total time: 0:27:54 (1.1671 s / it)
[02:35:33.062102] Syncing meters...
[02:35:33.928611] Averaged stats: lr: 0.0003 (0.0003)  loss: 1122.5685 (1109.8081)  grad_norm: 678.9230 (683.6287)  amp_scale: 1.0000 (1.0000)
[02:35:43.916426] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 64)
[02:35:45.821955] [Train][Ep-64/100]  [   0/1435]  eta: 0:45:22  lr: 0.0003 (0.0003)  time: 1.8973  data: 1.4142  max mem: 13835
[02:37:40.921010] [Train][Ep-64/100]  [ 100/1435]  eta: 0:25:46  lr: 0.0003 (0.0003)  loss: 1101.4612 (1101.4630)  grad_norm: 677.9417 (677.5683)  amp_scale: 1.0000 (1.0000)  time: 1.1490  data: 0.5851  max mem: 13835
[02:39:32.509352] [Train][Ep-64/100]  [ 200/1435]  eta: 0:23:24  lr: 0.0003 (0.0003)  loss: 1092.9055 (1104.5740)  grad_norm: 675.7102 (679.5866)  amp_scale: 1.0000 (1.0000)  time: 1.0717  data: 0.3280  max mem: 13835
[02:41:28.050845] [Train][Ep-64/100]  [ 300/1435]  eta: 0:21:37  lr: 0.0003 (0.0003)  loss: 1091.0892 (1098.8463)  grad_norm: 660.9969 (676.7011)  amp_scale: 1.0000 (1.0000)  time: 1.1089  data: 0.5388  max mem: 13835
[02:43:21.756491] [Train][Ep-64/100]  [ 400/1435]  eta: 0:19:41  lr: 0.0003 (0.0003)  loss: 1101.0468 (1103.2406)  grad_norm: 672.6188 (676.6913)  amp_scale: 1.0000 (1.0000)  time: 1.1117  data: 0.4937  max mem: 13835
[02:45:20.904145] [Train][Ep-64/100]  [ 500/1435]  eta: 0:17:56  lr: 0.0002 (0.0003)  loss: 1139.0300 (1106.0712)  grad_norm: 673.8331 (678.5012)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.5993  max mem: 13835
[02:47:15.419188] [Train][Ep-64/100]  [ 600/1435]  eta: 0:16:00  lr: 0.0002 (0.0003)  loss: 1087.7965 (1105.2497)  grad_norm: 667.5818 (677.5706)  amp_scale: 1.0000 (1.0000)  time: 1.1021  data: 0.4960  max mem: 13835
[02:49:12.237838] [Train][Ep-64/100]  [ 700/1435]  eta: 0:14:07  lr: 0.0002 (0.0003)  loss: 1102.7629 (1106.1022)  grad_norm: 676.6734 (679.8174)  amp_scale: 1.0000 (1.0000)  time: 1.1630  data: 0.2784  max mem: 13835
[02:51:06.527359] [Train][Ep-64/100]  [ 800/1435]  eta: 0:12:11  lr: 0.0002 (0.0002)  loss: 1078.8160 (1104.3323)  grad_norm: 695.5401 (681.3473)  amp_scale: 1.0000 (1.0000)  time: 1.1421  data: 0.2497  max mem: 13835
[02:53:05.188611] [Train][Ep-64/100]  [ 900/1435]  eta: 0:10:18  lr: 0.0002 (0.0002)  loss: 1103.0908 (1104.7416)  grad_norm: 692.9426 (682.9203)  amp_scale: 1.0000 (1.0000)  time: 1.2658  data: 0.1346  max mem: 13835
[02:55:04.815033] [Train][Ep-64/100]  [1000/1435]  eta: 0:08:24  lr: 0.0002 (0.0002)  loss: 1100.7015 (1105.0181)  grad_norm: 685.0334 (683.4471)  amp_scale: 1.0000 (1.0000)  time: 1.1618  data: 0.0480  max mem: 13835
[02:56:58.805869] [Train][Ep-64/100]  [1100/1435]  eta: 0:06:27  lr: 0.0002 (0.0002)  loss: 1095.5891 (1105.3658)  grad_norm: 689.3782 (683.8208)  amp_scale: 1.0000 (1.0000)  time: 1.1091  data: 0.3072  max mem: 13835
[02:58:56.690827] [Train][Ep-64/100]  [1200/1435]  eta: 0:04:32  lr: 0.0002 (0.0002)  loss: 1142.5267 (1106.3974)  grad_norm: 674.4221 (683.7561)  amp_scale: 1.0000 (1.0000)  time: 1.1712  data: 0.3720  max mem: 13835
[03:00:50.917142] [Train][Ep-64/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1070.9614 (1106.2943)  grad_norm: 684.4152 (683.0946)  amp_scale: 1.0000 (1.0000)  time: 1.1037  data: 0.4954  max mem: 13835
[03:02:52.510221] [Train][Ep-64/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1112.0111 (1107.6796)  grad_norm: 678.4622 (683.8443)  amp_scale: 1.0000 (1.0000)  time: 1.2032  data: 0.0879  max mem: 13835
[03:03:29.887730] [Train][Ep-64/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1107.6964 (1107.8227)  grad_norm: 670.8315 (683.4764)  amp_scale: 1.0000 (1.0000)  time: 1.0785  data: 0.0932  max mem: 13835
[03:03:29.888718] [Train][Ep-64/100] Total time: 0:27:45 (1.1610 s / it)
[03:03:29.889221] Syncing meters...
[03:03:31.198244] Averaged stats: lr: 0.0002 (0.0002)  loss: 1107.6964 (1109.3859)  grad_norm: 670.8315 (683.4764)  amp_scale: 1.0000 (1.0000)
[03:03:39.450163] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 65)
[03:03:41.372432] [Train][Ep-65/100]  [   0/1435]  eta: 0:45:46  lr: 0.0002 (0.0002)  time: 1.9142  data: 1.4315  max mem: 13835
[03:05:36.489584] [Train][Ep-65/100]  [ 100/1435]  eta: 0:25:46  lr: 0.0002 (0.0002)  loss: 1109.2045 (1112.2017)  grad_norm: 686.3566 (681.6535)  amp_scale: 1.0000 (1.0000)  time: 1.1260  data: 0.5554  max mem: 13835
[03:07:33.845456] [Train][Ep-65/100]  [ 200/1435]  eta: 0:24:00  lr: 0.0002 (0.0002)  loss: 1094.3278 (1107.6285)  grad_norm: 683.2503 (677.6120)  amp_scale: 1.0000 (1.0000)  time: 1.0906  data: 0.5236  max mem: 13835
[03:09:32.332467] [Train][Ep-65/100]  [ 300/1435]  eta: 0:22:10  lr: 0.0002 (0.0002)  loss: 1098.2448 (1104.7645)  grad_norm: 681.9003 (681.7689)  amp_scale: 1.0000 (1.0000)  time: 1.2668  data: 0.6986  max mem: 13835
[03:11:34.463659] [Train][Ep-65/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0002 (0.0002)  loss: 1105.8829 (1108.6062)  grad_norm: 671.9269 (680.4283)  amp_scale: 1.0000 (1.0000)  time: 1.2537  data: 0.6879  max mem: 13835
[03:13:35.984690] [Train][Ep-65/100]  [ 500/1435]  eta: 0:18:33  lr: 0.0002 (0.0002)  loss: 1066.3965 (1103.4711)  grad_norm: 649.6008 (679.6896)  amp_scale: 1.0000 (1.0000)  time: 1.1932  data: 0.6276  max mem: 13835
[03:15:36.240343] [Train][Ep-65/100]  [ 600/1435]  eta: 0:16:35  lr: 0.0002 (0.0002)  loss: 1135.2014 (1103.5813)  grad_norm: 679.2510 (680.0791)  amp_scale: 1.0000 (1.0000)  time: 1.2746  data: 0.0774  max mem: 13835
[03:17:34.976355] [Train][Ep-65/100]  [ 700/1435]  eta: 0:14:35  lr: 0.0002 (0.0002)  loss: 1051.2650 (1100.1684)  grad_norm: 669.6006 (679.5123)  amp_scale: 1.0000 (1.0000)  time: 1.2424  data: 0.0510  max mem: 13835
[03:19:37.264958] [Train][Ep-65/100]  [ 800/1435]  eta: 0:12:39  lr: 0.0002 (0.0002)  loss: 1115.1053 (1101.6656)  grad_norm: 694.0386 (681.0267)  amp_scale: 1.0000 (1.0000)  time: 1.2218  data: 0.1347  max mem: 13835
[03:21:39.452038] [Train][Ep-65/100]  [ 900/1435]  eta: 0:10:41  lr: 0.0002 (0.0002)  loss: 1067.2234 (1100.7894)  grad_norm: 671.5763 (681.2359)  amp_scale: 1.0000 (1.0000)  time: 1.1116  data: 0.0668  max mem: 13835
[03:23:31.320920] [Train][Ep-65/100]  [1000/1435]  eta: 0:08:37  lr: 0.0002 (0.0002)  loss: 1073.9741 (1100.9938)  grad_norm: 682.4803 (681.8171)  amp_scale: 1.0000 (1.0000)  time: 1.0876  data: 0.2802  max mem: 13835
[03:25:33.854659] [Train][Ep-65/100]  [1100/1435]  eta: 0:06:39  lr: 0.0002 (0.0002)  loss: 1109.5656 (1101.3160)  grad_norm: 677.1648 (681.7587)  amp_scale: 1.0000 (1.0000)  time: 1.2203  data: 0.0590  max mem: 13835
[03:27:29.229120] [Train][Ep-65/100]  [1200/1435]  eta: 0:04:39  lr: 0.0002 (0.0002)  loss: 1118.5225 (1101.2514)  grad_norm: 695.6002 (682.6361)  amp_scale: 1.0000 (1.0000)  time: 1.2365  data: 0.1730  max mem: 13835
[03:29:26.321394] [Train][Ep-65/100]  [1300/1435]  eta: 0:02:40  lr: 0.0002 (0.0002)  loss: 1081.1182 (1101.8574)  grad_norm: 686.2149 (683.7898)  amp_scale: 1.0000 (1.0000)  time: 1.0920  data: 0.5161  max mem: 13835
[03:31:17.745864] [Train][Ep-65/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1095.1335 (1101.4731)  grad_norm: 693.5547 (684.3287)  amp_scale: 1.0000 (1.0000)  time: 1.0709  data: 0.2927  max mem: 13835
[03:31:53.744366] [Train][Ep-65/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1096.9014 (1101.6374)  grad_norm: 693.8436 (684.2998)  amp_scale: 1.0000 (1.0000)  time: 1.0971  data: 0.5202  max mem: 13835
[03:31:53.745287] [Train][Ep-65/100] Total time: 0:28:14 (1.1807 s / it)
[03:31:53.745773] Syncing meters...
[03:31:53.839459] Averaged stats: lr: 0.0002 (0.0002)  loss: 1096.9014 (1102.7099)  grad_norm: 693.8436 (684.2998)  amp_scale: 1.0000 (1.0000)
[03:32:03.235698] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 66)
[03:32:05.002621] [Train][Ep-66/100]  [   0/1435]  eta: 0:42:04  lr: 0.0002 (0.0002)  time: 1.7593  data: 1.2767  max mem: 13835
[03:34:02.636473] [Train][Ep-66/100]  [ 100/1435]  eta: 0:26:18  lr: 0.0002 (0.0002)  loss: 1059.9023 (1088.5504)  grad_norm: 677.4982 (680.8672)  amp_scale: 1.0000 (1.0000)  time: 1.1618  data: 0.5881  max mem: 13835
[03:36:02.437148] [Train][Ep-66/100]  [ 200/1435]  eta: 0:24:29  lr: 0.0002 (0.0002)  loss: 1106.4691 (1101.9113)  grad_norm: 679.6228 (680.0791)  amp_scale: 1.0000 (1.0000)  time: 1.1434  data: 0.4087  max mem: 13835
[03:37:56.274510] [Train][Ep-66/100]  [ 300/1435]  eta: 0:22:11  lr: 0.0002 (0.0002)  loss: 1055.1689 (1096.3337)  grad_norm: 674.2780 (682.3312)  amp_scale: 1.0000 (1.0000)  time: 1.0870  data: 0.4341  max mem: 13835
[03:39:56.955686] [Train][Ep-66/100]  [ 400/1435]  eta: 0:20:22  lr: 0.0002 (0.0002)  loss: 1128.3257 (1102.4045)  grad_norm: 686.5491 (683.9784)  amp_scale: 1.0000 (1.0000)  time: 1.2080  data: 0.0523  max mem: 13835
[03:41:48.794459] [Train][Ep-66/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0002 (0.0002)  loss: 1132.7079 (1106.6242)  grad_norm: 667.4707 (682.5054)  amp_scale: 1.0000 (1.0000)  time: 1.0460  data: 0.3443  max mem: 13835
[03:43:44.837642] [Train][Ep-66/100]  [ 600/1435]  eta: 0:16:14  lr: 0.0002 (0.0002)  loss: 1103.4751 (1103.6558)  grad_norm: 679.8754 (680.6559)  amp_scale: 1.0000 (1.0000)  time: 1.1096  data: 0.2519  max mem: 13835
[03:45:38.976249] [Train][Ep-66/100]  [ 700/1435]  eta: 0:14:15  lr: 0.0002 (0.0002)  loss: 1115.1249 (1107.4030)  grad_norm: 695.6192 (682.8137)  amp_scale: 1.0000 (1.0000)  time: 1.2167  data: 0.0485  max mem: 13835
[03:47:35.485036] [Train][Ep-66/100]  [ 800/1435]  eta: 0:12:19  lr: 0.0002 (0.0002)  loss: 1069.8309 (1103.6193)  grad_norm: 676.7949 (682.7994)  amp_scale: 1.0000 (1.0000)  time: 1.1883  data: 0.6176  max mem: 13835
[03:49:36.630653] [Train][Ep-66/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0002 (0.0002)  loss: 1135.6047 (1105.9041)  grad_norm: 667.7224 (681.7527)  amp_scale: 1.0000 (1.0000)  time: 1.1452  data: 0.3812  max mem: 13835
[03:51:31.255555] [Train][Ep-66/100]  [1000/1435]  eta: 0:08:27  lr: 0.0002 (0.0002)  loss: 1091.1635 (1106.8351)  grad_norm: 686.5084 (682.0753)  amp_scale: 1.0000 (1.0000)  time: 1.1197  data: 0.5360  max mem: 13835
[03:53:21.998376] [Train][Ep-66/100]  [1100/1435]  eta: 0:06:29  lr: 0.0002 (0.0002)  loss: 1074.7687 (1104.2630)  grad_norm: 656.1772 (680.8862)  amp_scale: 1.0000 (1.0000)  time: 1.0948  data: 0.5104  max mem: 13835
[03:55:21.939030] [Train][Ep-66/100]  [1200/1435]  eta: 0:04:33  lr: 0.0002 (0.0002)  loss: 1106.1473 (1103.3188)  grad_norm: 676.8662 (681.6322)  amp_scale: 1.0000 (1.0000)  time: 1.2120  data: 0.6345  max mem: 13835
[03:57:24.352615] [Train][Ep-66/100]  [1300/1435]  eta: 0:02:37  lr: 0.0002 (0.0002)  loss: 1100.1365 (1102.2762)  grad_norm: 685.9773 (682.4594)  amp_scale: 1.0000 (1.0000)  time: 1.2375  data: 0.6705  max mem: 13835
[03:59:24.335002] [Train][Ep-66/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1090.3954 (1101.0188)  grad_norm: 680.2221 (682.6484)  amp_scale: 1.0000 (1.0000)  time: 1.1338  data: 0.5662  max mem: 13835
[04:00:00.607997] [Train][Ep-66/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1100.5184 (1101.4309)  grad_norm: 679.2040 (682.7858)  amp_scale: 1.0000 (1.0000)  time: 1.0688  data: 0.4806  max mem: 13835
[04:00:00.608963] [Train][Ep-66/100] Total time: 0:27:57 (1.1689 s / it)
[04:00:00.609481] Syncing meters...
[04:00:00.610770] Averaged stats: lr: 0.0002 (0.0002)  loss: 1100.5184 (1101.1403)  grad_norm: 679.2040 (682.7858)  amp_scale: 1.0000 (1.0000)
[04:00:07.959417] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 67)
[04:00:10.119579] [Train][Ep-67/100]  [   0/1435]  eta: 0:51:28  lr: 0.0002 (0.0002)  time: 2.1520  data: 1.6682  max mem: 13835
[04:01:59.339705] [Train][Ep-67/100]  [ 100/1435]  eta: 0:24:32  lr: 0.0002 (0.0002)  loss: 1072.9086 (1089.5971)  grad_norm: 679.5233 (683.7018)  amp_scale: 1.0000 (1.0000)  time: 1.1711  data: 0.6054  max mem: 13835
[04:03:54.813093] [Train][Ep-67/100]  [ 200/1435]  eta: 0:23:13  lr: 0.0002 (0.0002)  loss: 1091.3195 (1093.6871)  grad_norm: 693.7512 (687.6700)  amp_scale: 1.0000 (1.0000)  time: 1.0887  data: 0.5035  max mem: 13835
[04:05:46.820314] [Train][Ep-67/100]  [ 300/1435]  eta: 0:21:17  lr: 0.0002 (0.0002)  loss: 1091.4586 (1090.9573)  grad_norm: 677.9137 (683.6184)  amp_scale: 1.0000 (1.0000)  time: 1.1612  data: 0.3458  max mem: 13835
[04:07:44.128200] [Train][Ep-67/100]  [ 400/1435]  eta: 0:19:37  lr: 0.0002 (0.0002)  loss: 1068.5688 (1088.4630)  grad_norm: 679.9637 (683.6458)  amp_scale: 1.0000 (1.0000)  time: 1.1386  data: 0.2902  max mem: 13835
[04:09:38.660061] [Train][Ep-67/100]  [ 500/1435]  eta: 0:17:45  lr: 0.0002 (0.0002)  loss: 1035.4457 (1085.1394)  grad_norm: 682.1505 (683.1420)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.0155  max mem: 13835
[04:11:34.076726] [Train][Ep-67/100]  [ 600/1435]  eta: 0:15:53  lr: 0.0002 (0.0002)  loss: 1109.9656 (1086.0761)  grad_norm: 672.2065 (681.4920)  amp_scale: 1.0000 (1.0000)  time: 1.1551  data: 0.2190  max mem: 13835
[04:13:26.467141] [Train][Ep-67/100]  [ 700/1435]  eta: 0:13:57  lr: 0.0002 (0.0002)  loss: 1107.1565 (1089.6841)  grad_norm: 681.0076 (681.4560)  amp_scale: 1.0000 (1.0000)  time: 1.1000  data: 0.4199  max mem: 13835
[04:15:21.539607] [Train][Ep-67/100]  [ 800/1435]  eta: 0:12:04  lr: 0.0002 (0.0002)  loss: 1094.4917 (1091.2521)  grad_norm: 678.8322 (681.8028)  amp_scale: 1.0000 (1.0000)  time: 1.1487  data: 0.5825  max mem: 13835
[04:17:16.033032] [Train][Ep-67/100]  [ 900/1435]  eta: 0:10:10  lr: 0.0002 (0.0002)  loss: 1113.9863 (1094.1329)  grad_norm: 685.8356 (682.0778)  amp_scale: 1.0000 (1.0000)  time: 1.1574  data: 0.4166  max mem: 13835
[04:19:12.734615] [Train][Ep-67/100]  [1000/1435]  eta: 0:08:17  lr: 0.0002 (0.0002)  loss: 1085.2278 (1092.7622)  grad_norm: 672.2594 (681.3956)  amp_scale: 1.0000 (1.0000)  time: 1.1614  data: 0.0269  max mem: 13835
[04:21:09.563797] [Train][Ep-67/100]  [1100/1435]  eta: 0:06:23  lr: 0.0002 (0.0002)  loss: 1063.1935 (1092.2390)  grad_norm: 688.0092 (681.5092)  amp_scale: 1.0000 (1.0000)  time: 1.2232  data: 0.1037  max mem: 13835
[04:23:08.701882] [Train][Ep-67/100]  [1200/1435]  eta: 0:04:30  lr: 0.0002 (0.0002)  loss: 1092.0891 (1092.5005)  grad_norm: 674.6016 (681.1327)  amp_scale: 1.0000 (1.0000)  time: 1.1774  data: 0.0176  max mem: 13835
[04:25:08.762974] [Train][Ep-67/100]  [1300/1435]  eta: 0:02:35  lr: 0.0002 (0.0002)  loss: 1115.2871 (1093.0975)  grad_norm: 692.2967 (681.7390)  amp_scale: 1.0000 (1.0000)  time: 1.2647  data: 0.0017  max mem: 13835
[04:27:08.900667] [Train][Ep-67/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1112.1119 (1094.7762)  grad_norm: 677.2224 (682.8127)  amp_scale: 1.0000 (1.0000)  time: 1.2021  data: 0.6258  max mem: 13835
[04:27:48.893250] [Train][Ep-67/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1114.9122 (1096.1748)  grad_norm: 690.6852 (683.0232)  amp_scale: 1.0000 (1.0000)  time: 1.1316  data: 0.5610  max mem: 13835
[04:27:48.894255] [Train][Ep-67/100] Total time: 0:27:40 (1.1574 s / it)
[04:27:48.894722] Syncing meters...
[04:27:48.896032] Averaged stats: lr: 0.0002 (0.0002)  loss: 1114.9122 (1095.0748)  grad_norm: 690.6852 (683.0232)  amp_scale: 1.0000 (1.0000)
[04:27:58.708128] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 68)
[04:28:00.588163] [Train][Ep-68/100]  [   0/1435]  eta: 0:44:46  lr: 0.0002 (0.0002)  time: 1.8718  data: 1.3894  max mem: 13835
[04:29:59.847526] [Train][Ep-68/100]  [ 100/1435]  eta: 0:26:41  lr: 0.0002 (0.0002)  loss: 1118.8396 (1123.9177)  grad_norm: 664.4813 (666.9680)  amp_scale: 1.0000 (1.0000)  time: 1.1949  data: 0.6249  max mem: 13835
[04:31:58.115902] [Train][Ep-68/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0002 (0.0002)  loss: 1094.7129 (1115.7209)  grad_norm: 676.0359 (670.9213)  amp_scale: 1.0000 (1.0000)  time: 1.0858  data: 0.4765  max mem: 13835
[04:33:52.020255] [Train][Ep-68/100]  [ 300/1435]  eta: 0:22:12  lr: 0.0002 (0.0002)  loss: 1067.7169 (1103.7495)  grad_norm: 674.7136 (673.3183)  amp_scale: 1.0000 (1.0000)  time: 1.1392  data: 0.2650  max mem: 13835
[04:35:43.535017] [Train][Ep-68/100]  [ 400/1435]  eta: 0:19:59  lr: 0.0002 (0.0002)  loss: 1103.2462 (1100.5581)  grad_norm: 660.9841 (670.4336)  amp_scale: 1.0000 (1.0000)  time: 1.1737  data: 0.2953  max mem: 13835
[04:37:39.467615] [Train][Ep-68/100]  [ 500/1435]  eta: 0:18:03  lr: 0.0002 (0.0002)  loss: 1059.2369 (1099.3474)  grad_norm: 658.2383 (671.8725)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.0025  max mem: 13835
[04:39:39.214092] [Train][Ep-68/100]  [ 600/1435]  eta: 0:16:13  lr: 0.0002 (0.0002)  loss: 1096.9338 (1101.9678)  grad_norm: 705.8615 (675.5618)  amp_scale: 1.0000 (1.0000)  time: 1.1743  data: 0.0103  max mem: 13835
[04:41:33.961869] [Train][Ep-68/100]  [ 700/1435]  eta: 0:14:14  lr: 0.0002 (0.0002)  loss: 1064.2201 (1099.0156)  grad_norm: 658.5023 (676.1493)  amp_scale: 1.0000 (1.0000)  time: 1.1228  data: 0.2131  max mem: 13835
[04:43:28.748692] [Train][Ep-68/100]  [ 800/1435]  eta: 0:12:17  lr: 0.0002 (0.0002)  loss: 1061.2629 (1097.0735)  grad_norm: 685.6292 (678.0590)  amp_scale: 1.0000 (1.0000)  time: 1.1159  data: 0.1935  max mem: 13835
[04:45:27.960559] [Train][Ep-68/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0002 (0.0002)  loss: 1050.7628 (1095.4460)  grad_norm: 667.0256 (678.4898)  amp_scale: 1.0000 (1.0000)  time: 1.1811  data: 0.6104  max mem: 13835
[04:47:22.522939] [Train][Ep-68/100]  [1000/1435]  eta: 0:08:25  lr: 0.0002 (0.0002)  loss: 1090.8350 (1095.2872)  grad_norm: 681.9797 (679.3792)  amp_scale: 1.0000 (1.0000)  time: 1.0833  data: 0.5020  max mem: 13835
[04:49:15.642420] [Train][Ep-68/100]  [1100/1435]  eta: 0:06:28  lr: 0.0002 (0.0002)  loss: 1093.2507 (1097.7013)  grad_norm: 682.4905 (680.2599)  amp_scale: 1.0000 (1.0000)  time: 1.1641  data: 0.1162  max mem: 13835
[04:51:08.454428] [Train][Ep-68/100]  [1200/1435]  eta: 0:04:31  lr: 0.0002 (0.0002)  loss: 1070.4630 (1097.0970)  grad_norm: 675.8522 (680.5836)  amp_scale: 1.0000 (1.0000)  time: 1.1021  data: 0.3348  max mem: 13835
[04:53:06.287421] [Train][Ep-68/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1115.9135 (1100.1893)  grad_norm: 684.3597 (681.1529)  amp_scale: 1.0000 (1.0000)  time: 1.2696  data: 0.0577  max mem: 13835
[04:55:08.565539] [Train][Ep-68/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1106.8478 (1100.8027)  grad_norm: 693.8120 (682.2590)  amp_scale: 1.0000 (1.0000)  time: 1.2477  data: 0.0009  max mem: 13835
[04:55:46.040208] [Train][Ep-68/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1094.8826 (1100.0979)  grad_norm: 685.0228 (682.3862)  amp_scale: 1.0000 (1.0000)  time: 1.0973  data: 0.2173  max mem: 13835
[04:55:46.041171] [Train][Ep-68/100] Total time: 0:27:47 (1.1619 s / it)
[04:55:46.041639] Syncing meters...
[04:55:46.514610] Averaged stats: lr: 0.0002 (0.0002)  loss: 1094.8826 (1096.3728)  grad_norm: 685.0228 (682.3862)  amp_scale: 1.0000 (1.0000)
[04:55:54.992223] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 69)
[04:55:56.962696] [Train][Ep-69/100]  [   0/1435]  eta: 0:46:56  lr: 0.0002 (0.0002)  time: 1.9625  data: 1.4794  max mem: 13835
[04:57:57.179943] [Train][Ep-69/100]  [ 100/1435]  eta: 0:26:54  lr: 0.0002 (0.0002)  loss: 1088.0563 (1083.7626)  grad_norm: 668.4505 (676.5431)  amp_scale: 1.0000 (1.0000)  time: 1.1793  data: 0.6011  max mem: 13835
[04:59:53.620648] [Train][Ep-69/100]  [ 200/1435]  eta: 0:24:26  lr: 0.0002 (0.0002)  loss: 1140.6396 (1095.9250)  grad_norm: 674.6815 (678.5481)  amp_scale: 1.0000 (1.0000)  time: 1.1524  data: 0.5799  max mem: 13835
[05:01:47.645807] [Train][Ep-69/100]  [ 300/1435]  eta: 0:22:09  lr: 0.0002 (0.0002)  loss: 1094.9923 (1102.7425)  grad_norm: 685.6246 (681.7914)  amp_scale: 1.0000 (1.0000)  time: 1.0958  data: 0.5018  max mem: 13835
[05:03:50.439480] [Train][Ep-69/100]  [ 400/1435]  eta: 0:20:27  lr: 0.0002 (0.0002)  loss: 1102.9038 (1104.1252)  grad_norm: 681.5765 (681.7181)  amp_scale: 1.0000 (1.0000)  time: 1.2316  data: 0.6628  max mem: 13835
[05:05:44.633469] [Train][Ep-69/100]  [ 500/1435]  eta: 0:18:20  lr: 0.0002 (0.0002)  loss: 1064.0828 (1100.9027)  grad_norm: 679.6638 (682.8768)  amp_scale: 1.0000 (1.0000)  time: 1.1006  data: 0.5361  max mem: 13835
[05:07:37.962379] [Train][Ep-69/100]  [ 600/1435]  eta: 0:16:16  lr: 0.0002 (0.0002)  loss: 1101.3223 (1102.0887)  grad_norm: 683.3104 (684.0037)  amp_scale: 1.0000 (1.0000)  time: 1.1543  data: 0.5907  max mem: 13835
[05:09:34.804972] [Train][Ep-69/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0002 (0.0002)  loss: 1062.4313 (1095.8475)  grad_norm: 662.9075 (682.8867)  amp_scale: 1.0000 (1.0000)  time: 1.1447  data: 0.5652  max mem: 13835
[05:11:31.112070] [Train][Ep-69/100]  [ 800/1435]  eta: 0:12:21  lr: 0.0002 (0.0002)  loss: 1091.9888 (1094.9104)  grad_norm: 700.2058 (683.6592)  amp_scale: 1.0000 (1.0000)  time: 1.1530  data: 0.5784  max mem: 13835
[05:13:28.517473] [Train][Ep-69/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0002 (0.0002)  loss: 1067.5176 (1094.9314)  grad_norm: 675.7617 (682.7677)  amp_scale: 1.0000 (1.0000)  time: 1.1463  data: 0.5510  max mem: 13835
[05:15:25.694597] [Train][Ep-69/100]  [1000/1435]  eta: 0:08:28  lr: 0.0002 (0.0002)  loss: 1073.6025 (1092.6664)  grad_norm: 693.6541 (683.8241)  amp_scale: 1.0000 (1.0000)  time: 1.2106  data: 0.3857  max mem: 13835
[05:17:29.315490] [Train][Ep-69/100]  [1100/1435]  eta: 0:06:33  lr: 0.0002 (0.0002)  loss: 1034.1240 (1089.7900)  grad_norm: 667.7916 (683.5234)  amp_scale: 1.0000 (1.0000)  time: 1.2352  data: 0.0080  max mem: 13835
[05:19:30.642828] [Train][Ep-69/100]  [1200/1435]  eta: 0:04:36  lr: 0.0002 (0.0002)  loss: 1063.9962 (1091.4438)  grad_norm: 683.3068 (683.3875)  amp_scale: 1.0000 (1.0000)  time: 1.1385  data: 0.1562  max mem: 13835
[05:21:32.409802] [Train][Ep-69/100]  [1300/1435]  eta: 0:02:39  lr: 0.0002 (0.0002)  loss: 1064.0736 (1091.2101)  grad_norm: 684.0034 (683.2094)  amp_scale: 1.0000 (1.0000)  time: 1.1992  data: 0.0675  max mem: 13835
[05:23:26.020306] [Train][Ep-69/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1062.8790 (1090.4943)  grad_norm: 689.1555 (682.9065)  amp_scale: 1.0000 (1.0000)  time: 1.2271  data: 0.1091  max mem: 13835
[05:24:05.757155] [Train][Ep-69/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1071.6003 (1090.0303)  grad_norm: 691.9009 (683.0271)  amp_scale: 1.0000 (1.0000)  time: 1.1783  data: 0.0967  max mem: 13835
[05:24:05.758051] [Train][Ep-69/100] Total time: 0:28:10 (1.1782 s / it)
[05:24:05.758557] Syncing meters...
[05:24:06.340165] Averaged stats: lr: 0.0002 (0.0002)  loss: 1071.6003 (1089.3376)  grad_norm: 691.9009 (683.0271)  amp_scale: 1.0000 (1.0000)
[05:24:14.068963] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 70)
[05:24:16.060479] [Train][Ep-70/100]  [   0/1435]  eta: 0:47:26  lr: 0.0002 (0.0002)  time: 1.9835  data: 1.4995  max mem: 13835
[05:26:10.130040] [Train][Ep-70/100]  [ 100/1435]  eta: 0:25:33  lr: 0.0002 (0.0002)  loss: 1077.0544 (1095.9544)  grad_norm: 671.8488 (667.7545)  amp_scale: 1.0000 (1.0000)  time: 1.1434  data: 0.5731  max mem: 13835
[05:28:06.691056] [Train][Ep-70/100]  [ 200/1435]  eta: 0:23:49  lr: 0.0002 (0.0002)  loss: 1078.4296 (1095.2708)  grad_norm: 694.0197 (678.4237)  amp_scale: 1.0000 (1.0000)  time: 1.1057  data: 0.5035  max mem: 13835
[05:30:00.988806] [Train][Ep-70/100]  [ 300/1435]  eta: 0:21:47  lr: 0.0002 (0.0002)  loss: 1063.5306 (1094.0348)  grad_norm: 676.0362 (677.4067)  amp_scale: 1.0000 (1.0000)  time: 1.1202  data: 0.5485  max mem: 13835
[05:31:55.355723] [Train][Ep-70/100]  [ 400/1435]  eta: 0:19:49  lr: 0.0002 (0.0002)  loss: 1050.3354 (1087.3914)  grad_norm: 673.7719 (677.9641)  amp_scale: 1.0000 (1.0000)  time: 1.1542  data: 0.4982  max mem: 13835
[05:33:47.952510] [Train][Ep-70/100]  [ 500/1435]  eta: 0:17:50  lr: 0.0002 (0.0002)  loss: 1048.6636 (1084.9258)  grad_norm: 673.4979 (679.7681)  amp_scale: 1.0000 (1.0000)  time: 1.1654  data: 0.2250  max mem: 13835
[05:35:45.814917] [Train][Ep-70/100]  [ 600/1435]  eta: 0:16:00  lr: 0.0002 (0.0002)  loss: 1094.9518 (1087.1686)  grad_norm: 673.3486 (680.8437)  amp_scale: 1.0000 (1.0000)  time: 1.1291  data: 0.1834  max mem: 13835
[05:37:38.440442] [Train][Ep-70/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0002 (0.0002)  loss: 1062.3574 (1086.4592)  grad_norm: 650.4150 (679.3560)  amp_scale: 1.0000 (1.0000)  time: 1.1219  data: 0.5376  max mem: 13835
[05:39:28.968597] [Train][Ep-70/100]  [ 800/1435]  eta: 0:12:04  lr: 0.0002 (0.0002)  loss: 1061.5071 (1084.9545)  grad_norm: 670.7518 (679.7362)  amp_scale: 1.0000 (1.0000)  time: 1.1504  data: 0.3467  max mem: 13835
[05:41:28.836577] [Train][Ep-70/100]  [ 900/1435]  eta: 0:10:14  lr: 0.0002 (0.0002)  loss: 1113.2363 (1088.2089)  grad_norm: 685.5421 (680.9038)  amp_scale: 1.0000 (1.0000)  time: 1.1704  data: 0.2779  max mem: 13835
[05:43:28.090495] [Train][Ep-70/100]  [1000/1435]  eta: 0:08:21  lr: 0.0002 (0.0002)  loss: 1062.1868 (1086.8267)  grad_norm: 687.3784 (682.2901)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.1289  max mem: 13835
[05:45:31.312361] [Train][Ep-70/100]  [1100/1435]  eta: 0:06:28  lr: 0.0002 (0.0002)  loss: 1058.9504 (1086.5306)  grad_norm: 675.0972 (682.2914)  amp_scale: 1.0000 (1.0000)  time: 1.2190  data: 0.0521  max mem: 13835
[05:47:22.277051] [Train][Ep-70/100]  [1200/1435]  eta: 0:04:31  lr: 0.0002 (0.0002)  loss: 1089.7610 (1086.2633)  grad_norm: 674.2456 (682.1326)  amp_scale: 1.0000 (1.0000)  time: 1.1223  data: 0.3840  max mem: 13835
[05:49:15.350098] [Train][Ep-70/100]  [1300/1435]  eta: 0:02:35  lr: 0.0002 (0.0002)  loss: 1083.3414 (1086.5651)  grad_norm: 688.5170 (682.4044)  amp_scale: 1.0000 (1.0000)  time: 1.1676  data: 0.5978  max mem: 13835
[05:51:15.550845] [Train][Ep-70/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1057.8230 (1084.5921)  grad_norm: 677.6196 (682.2818)  amp_scale: 1.0000 (1.0000)  time: 1.2422  data: 0.6636  max mem: 13835
[05:51:54.354083] [Train][Ep-70/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1061.4662 (1084.1672)  grad_norm: 675.6593 (682.8242)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.5580  max mem: 13835
[05:51:54.355039] [Train][Ep-70/100] Total time: 0:27:40 (1.1570 s / it)
[05:51:54.355453] Syncing meters...
[05:51:54.612344] Averaged stats: lr: 0.0002 (0.0002)  loss: 1061.4662 (1089.7343)  grad_norm: 675.6593 (682.8242)  amp_scale: 1.0000 (1.0000)
[05:51:56.636522] [Eval][Ep-70/100]  [  0/121]  eta: 0:04:04  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0166  data: 1.8567  max mem: 13835
[05:53:38.527229] [Eval][Ep-70/100]  [100/121]  eta: 0:00:21  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9238  data: 0.7640  max mem: 13835
[05:53:56.993805] [Eval][Ep-70/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9231  data: 0.7658  max mem: 13835
[05:53:56.994693] [Eval][Ep-70/100] Total time: 0:02:02 (1.0114 s / it)
[05:53:57.325800] [Eval][Ep-70/100] val_acc1_image=27.19 | val_acc1_audio=41.62 | val_acc1_fusion=38.55 | val_acc1_all=51.99
[05:54:07.017838] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 71)
[05:54:09.023548] [Train][Ep-71/100]  [   0/1435]  eta: 0:47:46  lr: 0.0002 (0.0002)  time: 1.9977  data: 1.5186  max mem: 13835
[05:56:05.064679] [Train][Ep-71/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0002 (0.0002)  loss: 1049.5055 (1076.1068)  grad_norm: 657.9664 (660.5444)  amp_scale: 1.0000 (1.0000)  time: 1.1583  data: 0.3554  max mem: 13835
[05:57:59.296149] [Train][Ep-71/100]  [ 200/1435]  eta: 0:23:47  lr: 0.0002 (0.0002)  loss: 1063.7928 (1074.7675)  grad_norm: 669.9023 (671.9922)  amp_scale: 1.0000 (1.0000)  time: 1.1318  data: 0.3201  max mem: 13835
[05:59:58.793380] [Train][Ep-71/100]  [ 300/1435]  eta: 0:22:06  lr: 0.0002 (0.0002)  loss: 1076.3844 (1078.6467)  grad_norm: 686.5472 (677.7008)  amp_scale: 1.0000 (1.0000)  time: 1.2286  data: 0.0246  max mem: 13835
[06:01:56.997083] [Train][Ep-71/100]  [ 400/1435]  eta: 0:20:12  lr: 0.0002 (0.0002)  loss: 1090.8867 (1078.4463)  grad_norm: 681.7861 (682.2487)  amp_scale: 1.0000 (1.0000)  time: 1.1641  data: 0.3640  max mem: 13835
[06:03:56.721713] [Train][Ep-71/100]  [ 500/1435]  eta: 0:18:20  lr: 0.0002 (0.0002)  loss: 1085.5065 (1080.7822)  grad_norm: 675.7876 (685.3953)  amp_scale: 1.0000 (1.0000)  time: 1.2339  data: 0.2194  max mem: 13835
[06:05:56.245422] [Train][Ep-71/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0002 (0.0002)  loss: 1087.5605 (1083.5597)  grad_norm: 676.3846 (683.8122)  amp_scale: 1.0000 (1.0000)  time: 1.1686  data: 0.5763  max mem: 13835
[06:07:56.833968] [Train][Ep-71/100]  [ 700/1435]  eta: 0:14:30  lr: 0.0002 (0.0002)  loss: 1058.7747 (1081.1020)  grad_norm: 675.9284 (684.5143)  amp_scale: 1.0000 (1.0000)  time: 1.2012  data: 0.6353  max mem: 13835
[06:09:47.911617] [Train][Ep-71/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0002 (0.0002)  loss: 1097.4834 (1080.5155)  grad_norm: 669.9116 (683.5954)  amp_scale: 1.0000 (1.0000)  time: 1.1645  data: 0.5955  max mem: 13835
[06:11:47.022797] [Train][Ep-71/100]  [ 900/1435]  eta: 0:10:29  lr: 0.0002 (0.0002)  loss: 1065.9624 (1080.6687)  grad_norm: 688.4822 (684.6758)  amp_scale: 1.0000 (1.0000)  time: 1.2463  data: 0.6787  max mem: 13835
[06:13:44.007093] [Train][Ep-71/100]  [1000/1435]  eta: 0:08:31  lr: 0.0002 (0.0002)  loss: 1083.8052 (1081.2939)  grad_norm: 662.1290 (683.3605)  amp_scale: 1.0000 (1.0000)  time: 1.1091  data: 0.5411  max mem: 13835
[06:15:41.123957] [Train][Ep-71/100]  [1100/1435]  eta: 0:06:33  lr: 0.0002 (0.0002)  loss: 1074.3790 (1082.2048)  grad_norm: 685.1394 (684.3331)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.3570  max mem: 13835
[06:17:42.110241] [Train][Ep-71/100]  [1200/1435]  eta: 0:04:36  lr: 0.0002 (0.0002)  loss: 1057.9147 (1081.8418)  grad_norm: 683.6889 (684.5590)  amp_scale: 1.0000 (1.0000)  time: 1.1836  data: 0.0960  max mem: 13835
[06:19:39.738579] [Train][Ep-71/100]  [1300/1435]  eta: 0:02:39  lr: 0.0002 (0.0002)  loss: 1068.5939 (1081.6155)  grad_norm: 675.9238 (684.1527)  amp_scale: 1.0000 (1.0000)  time: 1.1152  data: 0.0718  max mem: 13835
[06:21:37.225799] [Train][Ep-71/100]  [1400/1435]  eta: 0:00:41  lr: 0.0002 (0.0002)  loss: 1079.3221 (1081.6765)  grad_norm: 682.8563 (684.4303)  amp_scale: 1.0000 (1.0000)  time: 1.1883  data: 0.3881  max mem: 13835
[06:22:16.582485] [Train][Ep-71/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1079.3221 (1081.6107)  grad_norm: 684.5042 (684.7871)  amp_scale: 1.0000 (1.0000)  time: 1.1839  data: 0.1437  max mem: 13835
[06:22:16.583491] [Train][Ep-71/100] Total time: 0:28:09 (1.1774 s / it)
[06:22:16.583942] Syncing meters...
[06:22:17.538392] Averaged stats: lr: 0.0002 (0.0002)  loss: 1079.3221 (1079.7782)  grad_norm: 684.5042 (684.7871)  amp_scale: 1.0000 (1.0000)
[06:22:26.076349] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 72)
[06:22:27.859670] [Train][Ep-72/100]  [   0/1435]  eta: 0:42:27  lr: 0.0002 (0.0002)  time: 1.7750  data: 1.2931  max mem: 13835
[06:24:24.702648] [Train][Ep-72/100]  [ 100/1435]  eta: 0:26:07  lr: 0.0002 (0.0002)  loss: 1099.5682 (1078.2363)  grad_norm: 684.7309 (682.8147)  amp_scale: 1.0000 (1.0000)  time: 1.2303  data: 0.6617  max mem: 13835
[06:26:22.417067] [Train][Ep-72/100]  [ 200/1435]  eta: 0:24:11  lr: 0.0002 (0.0002)  loss: 1093.9357 (1083.0566)  grad_norm: 677.9274 (684.5760)  amp_scale: 1.0000 (1.0000)  time: 1.1702  data: 0.2697  max mem: 13835
[06:28:20.158904] [Train][Ep-72/100]  [ 300/1435]  eta: 0:22:15  lr: 0.0002 (0.0002)  loss: 1098.2128 (1087.0005)  grad_norm: 668.7209 (679.7326)  amp_scale: 1.0000 (1.0000)  time: 1.1696  data: 0.6016  max mem: 13835
[06:30:13.212654] [Train][Ep-72/100]  [ 400/1435]  eta: 0:20:05  lr: 0.0002 (0.0002)  loss: 1091.1465 (1090.3908)  grad_norm: 675.7143 (679.9947)  amp_scale: 1.0000 (1.0000)  time: 1.1789  data: 0.0017  max mem: 13835
[06:32:08.596960] [Train][Ep-72/100]  [ 500/1435]  eta: 0:18:07  lr: 0.0002 (0.0002)  loss: 1064.5211 (1084.1288)  grad_norm: 674.7166 (678.8450)  amp_scale: 1.0000 (1.0000)  time: 1.2080  data: 0.0068  max mem: 13835
[06:34:06.495037] [Train][Ep-72/100]  [ 600/1435]  eta: 0:16:13  lr: 0.0002 (0.0002)  loss: 1038.2363 (1080.4402)  grad_norm: 671.0562 (679.1261)  amp_scale: 1.0000 (1.0000)  time: 1.1975  data: 0.0175  max mem: 13835
[06:36:03.431902] [Train][Ep-72/100]  [ 700/1435]  eta: 0:14:16  lr: 0.0002 (0.0002)  loss: 1070.2496 (1080.1065)  grad_norm: 672.2578 (681.3466)  amp_scale: 1.0000 (1.0000)  time: 1.0806  data: 0.2588  max mem: 13835
[06:37:56.154549] [Train][Ep-72/100]  [ 800/1435]  eta: 0:12:17  lr: 0.0002 (0.0002)  loss: 1064.1281 (1080.4272)  grad_norm: 686.0176 (681.6654)  amp_scale: 1.0000 (1.0000)  time: 1.1156  data: 0.0380  max mem: 13835
[06:39:49.489128] [Train][Ep-72/100]  [ 900/1435]  eta: 0:10:19  lr: 0.0002 (0.0002)  loss: 1083.7965 (1083.7009)  grad_norm: 675.9179 (682.2736)  amp_scale: 1.0000 (1.0000)  time: 1.1895  data: 0.1988  max mem: 13835
[06:41:46.205412] [Train][Ep-72/100]  [1000/1435]  eta: 0:08:24  lr: 0.0002 (0.0002)  loss: 1127.9565 (1086.3857)  grad_norm: 684.3365 (683.6791)  amp_scale: 1.0000 (1.0000)  time: 1.1533  data: 0.2066  max mem: 13835
[06:43:45.600161] [Train][Ep-72/100]  [1100/1435]  eta: 0:06:29  lr: 0.0002 (0.0002)  loss: 1089.1227 (1084.7494)  grad_norm: 675.3263 (682.7650)  amp_scale: 1.0000 (1.0000)  time: 1.1590  data: 0.0446  max mem: 13835
[06:45:45.039732] [Train][Ep-72/100]  [1200/1435]  eta: 0:04:33  lr: 0.0002 (0.0002)  loss: 1075.9695 (1083.8604)  grad_norm: 676.4095 (683.2046)  amp_scale: 1.0000 (1.0000)  time: 1.1738  data: 0.1229  max mem: 13835
[06:47:37.477582] [Train][Ep-72/100]  [1300/1435]  eta: 0:02:36  lr: 0.0002 (0.0002)  loss: 1082.3722 (1084.9945)  grad_norm: 693.1663 (684.7067)  amp_scale: 1.0000 (1.0000)  time: 1.2048  data: 0.5089  max mem: 13835
[06:49:32.459463] [Train][Ep-72/100]  [1400/1435]  eta: 0:00:40  lr: 0.0002 (0.0002)  loss: 1058.9021 (1082.9411)  grad_norm: 674.3469 (684.9082)  amp_scale: 1.0000 (1.0000)  time: 1.1748  data: 0.5991  max mem: 13835
[06:50:10.423420] [Train][Ep-72/100]  [1434/1435]  eta: 0:00:01  lr: 0.0002 (0.0002)  loss: 1056.8019 (1082.0343)  grad_norm: 681.5046 (685.4849)  amp_scale: 1.0000 (1.0000)  time: 1.0657  data: 0.4958  max mem: 13835
[06:50:10.424297] [Train][Ep-72/100] Total time: 0:27:44 (1.1598 s / it)
[06:50:10.424679] Syncing meters...
[06:50:10.776943] Averaged stats: lr: 0.0002 (0.0002)  loss: 1056.8019 (1077.9958)  grad_norm: 681.5046 (685.4849)  amp_scale: 1.0000 (1.0000)
[06:50:21.021384] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 73)
[06:50:22.812085] [Train][Ep-73/100]  [   0/1435]  eta: 0:42:38  lr: 0.0002 (0.0002)  time: 1.7829  data: 1.2994  max mem: 13835
[06:52:16.079188] [Train][Ep-73/100]  [ 100/1435]  eta: 0:25:20  lr: 0.0002 (0.0002)  loss: 1086.2841 (1078.0553)  grad_norm: 694.2014 (697.3688)  amp_scale: 1.0000 (1.0000)  time: 1.1437  data: 0.1786  max mem: 13835
[06:54:08.912791] [Train][Ep-73/100]  [ 200/1435]  eta: 0:23:20  lr: 0.0002 (0.0002)  loss: 1066.6812 (1077.0850)  grad_norm: 671.4891 (685.8042)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.1280  max mem: 13835
[06:56:08.899869] [Train][Ep-73/100]  [ 300/1435]  eta: 0:21:51  lr: 0.0002 (0.0002)  loss: 1080.1315 (1081.2214)  grad_norm: 658.7127 (681.6421)  amp_scale: 1.0000 (1.0000)  time: 1.2100  data: 0.1721  max mem: 13835
[06:58:05.282531] [Train][Ep-73/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0002 (0.0002)  loss: 1086.1301 (1079.9101)  grad_norm: 683.6428 (685.2408)  amp_scale: 1.0000 (1.0000)  time: 1.1329  data: 0.3348  max mem: 13835
[07:00:01.494345] [Train][Ep-73/100]  [ 500/1435]  eta: 0:18:03  lr: 0.0002 (0.0002)  loss: 1074.5607 (1076.4949)  grad_norm: 680.4181 (685.1868)  amp_scale: 1.0000 (1.0000)  time: 1.1564  data: 0.0020  max mem: 13835
[07:01:59.979124] [Train][Ep-73/100]  [ 600/1435]  eta: 0:16:11  lr: 0.0001 (0.0002)  loss: 1062.9371 (1074.8938)  grad_norm: 683.3768 (685.4002)  amp_scale: 1.0000 (1.0000)  time: 1.3183  data: 0.7475  max mem: 13835
[07:03:58.981023] [Train][Ep-73/100]  [ 700/1435]  eta: 0:14:17  lr: 0.0001 (0.0002)  loss: 1044.1375 (1074.2994)  grad_norm: 688.1326 (685.9319)  amp_scale: 1.0000 (1.0000)  time: 1.1458  data: 0.5264  max mem: 13835
[07:05:52.263277] [Train][Ep-73/100]  [ 800/1435]  eta: 0:12:18  lr: 0.0001 (0.0002)  loss: 1082.1804 (1074.7955)  grad_norm: 682.3200 (685.5310)  amp_scale: 1.0000 (1.0000)  time: 1.1356  data: 0.5694  max mem: 13835
[07:07:48.997685] [Train][Ep-73/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0001 (0.0002)  loss: 1080.1049 (1076.5829)  grad_norm: 667.8289 (685.4181)  amp_scale: 1.0000 (1.0000)  time: 1.1346  data: 0.5624  max mem: 13835
[07:09:43.879426] [Train][Ep-73/100]  [1000/1435]  eta: 0:08:25  lr: 0.0001 (0.0001)  loss: 1074.7957 (1079.0962)  grad_norm: 679.9933 (686.5803)  amp_scale: 1.0000 (1.0000)  time: 1.1703  data: 0.5993  max mem: 13835
[07:11:43.782237] [Train][Ep-73/100]  [1100/1435]  eta: 0:06:30  lr: 0.0001 (0.0001)  loss: 1049.2102 (1079.4067)  grad_norm: 681.1381 (686.9355)  amp_scale: 1.0000 (1.0000)  time: 1.2175  data: 0.6454  max mem: 13835
[07:13:40.031876] [Train][Ep-73/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1095.3303 (1080.1626)  grad_norm: 679.0228 (686.1661)  amp_scale: 1.0000 (1.0000)  time: 1.1847  data: 0.5039  max mem: 13835
[07:15:38.263166] [Train][Ep-73/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1085.7682 (1080.3223)  grad_norm: 699.8123 (687.0234)  amp_scale: 1.0000 (1.0000)  time: 1.1896  data: 0.6245  max mem: 13835
[07:17:38.110789] [Train][Ep-73/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1064.8607 (1079.1044)  grad_norm: 665.8484 (686.4249)  amp_scale: 1.0000 (1.0000)  time: 1.1539  data: 0.5846  max mem: 13835
[07:18:15.841963] [Train][Ep-73/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1095.8291 (1080.1046)  grad_norm: 672.7346 (686.9940)  amp_scale: 1.0000 (1.0000)  time: 1.0882  data: 0.5227  max mem: 13835
[07:18:15.842979] [Train][Ep-73/100] Total time: 0:27:54 (1.1671 s / it)
[07:18:15.843400] Syncing meters...
[07:18:15.844677] Averaged stats: lr: 0.0001 (0.0001)  loss: 1095.8291 (1075.4335)  grad_norm: 672.7346 (686.9940)  amp_scale: 1.0000 (1.0000)
[07:18:24.414808] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 74)
[07:18:26.135831] [Train][Ep-74/100]  [   0/1435]  eta: 0:40:57  lr: 0.0001 (0.0001)  time: 1.7128  data: 1.2298  max mem: 13835
[07:20:25.767766] [Train][Ep-74/100]  [ 100/1435]  eta: 0:26:43  lr: 0.0001 (0.0001)  loss: 1036.7373 (1065.9071)  grad_norm: 670.7004 (669.2362)  amp_scale: 1.0000 (1.0000)  time: 1.1857  data: 0.6133  max mem: 13835
[07:22:21.520231] [Train][Ep-74/100]  [ 200/1435]  eta: 0:24:16  lr: 0.0001 (0.0001)  loss: 1070.8716 (1073.1802)  grad_norm: 669.0848 (678.2122)  amp_scale: 1.0000 (1.0000)  time: 1.1211  data: 0.5551  max mem: 13835
[07:24:17.098055] [Train][Ep-74/100]  [ 300/1435]  eta: 0:22:09  lr: 0.0001 (0.0001)  loss: 1074.9073 (1075.8318)  grad_norm: 687.0739 (681.0485)  amp_scale: 1.0000 (1.0000)  time: 1.0713  data: 0.3896  max mem: 13835
[07:26:12.321179] [Train][Ep-74/100]  [ 400/1435]  eta: 0:20:07  lr: 0.0001 (0.0001)  loss: 1045.8702 (1074.0270)  grad_norm: 683.0305 (683.6352)  amp_scale: 1.0000 (1.0000)  time: 1.1421  data: 0.5588  max mem: 13835
[07:28:09.063102] [Train][Ep-74/100]  [ 500/1435]  eta: 0:18:11  lr: 0.0001 (0.0001)  loss: 1067.8639 (1077.0041)  grad_norm: 694.8993 (686.9467)  amp_scale: 1.0000 (1.0000)  time: 1.2185  data: 0.0762  max mem: 13835
[07:30:06.341363] [Train][Ep-74/100]  [ 600/1435]  eta: 0:16:15  lr: 0.0001 (0.0001)  loss: 1081.4321 (1077.3382)  grad_norm: 671.0911 (687.5400)  amp_scale: 1.0000 (1.0000)  time: 1.1553  data: 0.5842  max mem: 13835
[07:31:58.848243] [Train][Ep-74/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0001 (0.0001)  loss: 1061.1025 (1074.9000)  grad_norm: 694.8861 (686.7827)  amp_scale: 1.0000 (1.0000)  time: 1.1936  data: 0.2379  max mem: 13835
[07:33:49.994352] [Train][Ep-74/100]  [ 800/1435]  eta: 0:12:13  lr: 0.0001 (0.0001)  loss: 1061.0123 (1075.6069)  grad_norm: 676.8716 (689.5304)  amp_scale: 1.0000 (1.0000)  time: 1.1328  data: 0.2273  max mem: 13835
[07:35:45.821686] [Train][Ep-74/100]  [ 900/1435]  eta: 0:10:18  lr: 0.0001 (0.0001)  loss: 1103.2327 (1078.9921)  grad_norm: 682.9180 (689.0679)  amp_scale: 1.0000 (1.0000)  time: 1.0868  data: 0.1272  max mem: 13835
[07:37:45.722666] [Train][Ep-74/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1063.7031 (1077.2325)  grad_norm: 709.1299 (690.2114)  amp_scale: 1.0000 (1.0000)  time: 1.2060  data: 0.0023  max mem: 13835
[07:39:43.791936] [Train][Ep-74/100]  [1100/1435]  eta: 0:06:29  lr: 0.0001 (0.0001)  loss: 1056.3773 (1077.1293)  grad_norm: 700.1324 (690.0765)  amp_scale: 1.0000 (1.0000)  time: 1.2011  data: 0.0181  max mem: 13835
[07:41:42.728151] [Train][Ep-74/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1028.9495 (1073.3480)  grad_norm: 683.9789 (690.5498)  amp_scale: 1.0000 (1.0000)  time: 1.1714  data: 0.0575  max mem: 13835
[07:43:38.907242] [Train][Ep-74/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1051.3071 (1071.9897)  grad_norm: 692.0512 (690.7990)  amp_scale: 1.0000 (1.0000)  time: 1.1516  data: 0.0872  max mem: 13835
[07:45:35.089968] [Train][Ep-74/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1060.4261 (1071.4670)  grad_norm: 693.3913 (691.2905)  amp_scale: 1.0000 (1.0000)  time: 1.1819  data: 0.3002  max mem: 13835
[07:46:13.304373] [Train][Ep-74/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1066.5817 (1071.2485)  grad_norm: 682.1238 (690.7891)  amp_scale: 1.0000 (1.0000)  time: 1.0767  data: 0.4742  max mem: 13835
[07:46:13.305300] [Train][Ep-74/100] Total time: 0:27:48 (1.1630 s / it)
[07:46:13.305728] Syncing meters...
[07:46:13.658616] Averaged stats: lr: 0.0001 (0.0001)  loss: 1066.5817 (1073.5991)  grad_norm: 682.1238 (690.7891)  amp_scale: 1.0000 (1.0000)
[07:46:23.428499] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 75)
[07:46:25.343770] [Train][Ep-75/100]  [   0/1435]  eta: 0:45:36  lr: 0.0001 (0.0001)  time: 1.9072  data: 1.4245  max mem: 13835
[07:48:18.006159] [Train][Ep-75/100]  [ 100/1435]  eta: 0:25:14  lr: 0.0001 (0.0001)  loss: 1046.4498 (1043.9744)  grad_norm: 667.6732 (674.9066)  amp_scale: 1.0000 (1.0000)  time: 1.1319  data: 0.4651  max mem: 13835
[07:50:09.924281] [Train][Ep-75/100]  [ 200/1435]  eta: 0:23:11  lr: 0.0001 (0.0001)  loss: 1066.3706 (1053.5916)  grad_norm: 683.1339 (674.8390)  amp_scale: 1.0000 (1.0000)  time: 1.0771  data: 0.5111  max mem: 13835
[07:51:59.402728] [Train][Ep-75/100]  [ 300/1435]  eta: 0:21:06  lr: 0.0001 (0.0001)  loss: 1059.3702 (1058.0731)  grad_norm: 677.9271 (675.5331)  amp_scale: 1.0000 (1.0000)  time: 1.1213  data: 0.3104  max mem: 13835
[07:53:53.067838] [Train][Ep-75/100]  [ 400/1435]  eta: 0:19:20  lr: 0.0001 (0.0001)  loss: 1080.2322 (1063.1684)  grad_norm: 674.2017 (679.7929)  amp_scale: 1.0000 (1.0000)  time: 1.1373  data: 0.5377  max mem: 13835
[07:55:49.422396] [Train][Ep-75/100]  [ 500/1435]  eta: 0:17:36  lr: 0.0001 (0.0001)  loss: 1055.3497 (1063.8818)  grad_norm: 694.1585 (682.5491)  amp_scale: 1.0000 (1.0000)  time: 1.1579  data: 0.5918  max mem: 13835
[07:57:43.150983] [Train][Ep-75/100]  [ 600/1435]  eta: 0:15:44  lr: 0.0001 (0.0001)  loss: 1061.0547 (1063.3839)  grad_norm: 683.4650 (683.7365)  amp_scale: 1.0000 (1.0000)  time: 1.0990  data: 0.3705  max mem: 13835
[07:59:39.791817] [Train][Ep-75/100]  [ 700/1435]  eta: 0:13:54  lr: 0.0001 (0.0001)  loss: 1072.8368 (1065.4839)  grad_norm: 681.8165 (684.2436)  amp_scale: 1.0000 (1.0000)  time: 1.0899  data: 0.3565  max mem: 13835
[08:01:32.068415] [Train][Ep-75/100]  [ 800/1435]  eta: 0:12:00  lr: 0.0001 (0.0001)  loss: 1074.7268 (1066.3376)  grad_norm: 670.8585 (683.4874)  amp_scale: 1.0000 (1.0000)  time: 1.1617  data: 0.3164  max mem: 13835
[08:03:24.096207] [Train][Ep-75/100]  [ 900/1435]  eta: 0:10:06  lr: 0.0001 (0.0001)  loss: 1076.2246 (1065.8088)  grad_norm: 683.0988 (684.4614)  amp_scale: 1.0000 (1.0000)  time: 1.1901  data: 0.0025  max mem: 13835
[08:05:24.802261] [Train][Ep-75/100]  [1000/1435]  eta: 0:08:15  lr: 0.0001 (0.0001)  loss: 1120.7286 (1068.9109)  grad_norm: 684.4984 (685.6166)  amp_scale: 1.0000 (1.0000)  time: 1.2192  data: 0.0035  max mem: 13835
[08:07:19.414929] [Train][Ep-75/100]  [1100/1435]  eta: 0:06:22  lr: 0.0001 (0.0001)  loss: 1063.4381 (1069.6031)  grad_norm: 667.4498 (684.6036)  amp_scale: 1.0000 (1.0000)  time: 1.1324  data: 0.4802  max mem: 13835
[08:09:15.050265] [Train][Ep-75/100]  [1200/1435]  eta: 0:04:28  lr: 0.0001 (0.0001)  loss: 1096.8340 (1071.5775)  grad_norm: 683.6251 (685.1487)  amp_scale: 1.0000 (1.0000)  time: 1.1658  data: 0.5943  max mem: 13835
[08:11:09.857371] [Train][Ep-75/100]  [1300/1435]  eta: 0:02:34  lr: 0.0001 (0.0001)  loss: 1097.4412 (1073.7097)  grad_norm: 686.9404 (685.5008)  amp_scale: 1.0000 (1.0000)  time: 1.1724  data: 0.3911  max mem: 13835
[08:13:03.452917] [Train][Ep-75/100]  [1400/1435]  eta: 0:00:39  lr: 0.0001 (0.0001)  loss: 1117.6398 (1075.6567)  grad_norm: 682.8393 (685.1455)  amp_scale: 1.0000 (1.0000)  time: 1.1357  data: 0.3697  max mem: 13835
[08:13:41.228100] [Train][Ep-75/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1109.3944 (1076.5059)  grad_norm: 688.3077 (685.4750)  amp_scale: 1.0000 (1.0000)  time: 1.1214  data: 0.3722  max mem: 13835
[08:13:41.229008] [Train][Ep-75/100] Total time: 0:27:17 (1.1413 s / it)
[08:13:41.229646] Syncing meters...
[08:13:41.584203] Averaged stats: lr: 0.0001 (0.0001)  loss: 1109.3944 (1071.1714)  grad_norm: 688.3077 (685.4750)  amp_scale: 1.0000 (1.0000)
[08:13:49.361335] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 76)
[08:13:51.277506] [Train][Ep-76/100]  [   0/1435]  eta: 0:45:37  lr: 0.0001 (0.0001)  time: 1.9078  data: 1.4244  max mem: 13835
[08:15:41.443072] [Train][Ep-76/100]  [ 100/1435]  eta: 0:24:41  lr: 0.0001 (0.0001)  loss: 1075.5526 (1076.5925)  grad_norm: 690.6432 (692.2083)  amp_scale: 1.0000 (1.0000)  time: 1.0680  data: 0.3124  max mem: 13835
[08:17:36.728479] [Train][Ep-76/100]  [ 200/1435]  eta: 0:23:16  lr: 0.0001 (0.0001)  loss: 1036.7903 (1059.2211)  grad_norm: 673.6120 (685.7418)  amp_scale: 1.0000 (1.0000)  time: 1.1257  data: 0.0014  max mem: 13835
[08:19:31.937253] [Train][Ep-76/100]  [ 300/1435]  eta: 0:21:31  lr: 0.0001 (0.0001)  loss: 1033.3906 (1059.7455)  grad_norm: 681.8786 (684.3047)  amp_scale: 1.0000 (1.0000)  time: 1.1349  data: 0.0757  max mem: 13835
[08:21:25.102727] [Train][Ep-76/100]  [ 400/1435]  eta: 0:19:36  lr: 0.0001 (0.0001)  loss: 1063.2777 (1061.6396)  grad_norm: 690.0909 (684.8301)  amp_scale: 1.0000 (1.0000)  time: 1.1347  data: 0.0851  max mem: 13835
[08:23:24.761721] [Train][Ep-76/100]  [ 500/1435]  eta: 0:17:53  lr: 0.0001 (0.0001)  loss: 1045.0208 (1064.0031)  grad_norm: 688.0105 (685.4727)  amp_scale: 1.0000 (1.0000)  time: 1.2721  data: 0.1566  max mem: 13835
[08:25:18.088072] [Train][Ep-76/100]  [ 600/1435]  eta: 0:15:56  lr: 0.0001 (0.0001)  loss: 1047.3710 (1065.0529)  grad_norm: 693.7499 (686.4145)  amp_scale: 1.0000 (1.0000)  time: 1.1986  data: 0.0043  max mem: 13835
[08:27:17.722766] [Train][Ep-76/100]  [ 700/1435]  eta: 0:14:07  lr: 0.0001 (0.0001)  loss: 1048.8401 (1064.5290)  grad_norm: 717.8699 (688.7629)  amp_scale: 1.0000 (1.0000)  time: 1.1632  data: 0.1843  max mem: 13835
[08:29:15.012727] [Train][Ep-76/100]  [ 800/1435]  eta: 0:12:13  lr: 0.0001 (0.0001)  loss: 1048.8690 (1064.0744)  grad_norm: 686.4244 (688.2874)  amp_scale: 1.0000 (1.0000)  time: 1.1619  data: 0.3575  max mem: 13835
[08:31:09.805082] [Train][Ep-76/100]  [ 900/1435]  eta: 0:10:17  lr: 0.0001 (0.0001)  loss: 1048.7939 (1064.0709)  grad_norm: 678.8179 (688.7629)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.1906  max mem: 13835
[08:33:09.373881] [Train][Ep-76/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1059.1290 (1064.5715)  grad_norm: 673.7136 (687.6546)  amp_scale: 1.0000 (1.0000)  time: 1.1747  data: 0.0091  max mem: 13835
[08:35:05.072565] [Train][Ep-76/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1053.4165 (1065.0104)  grad_norm: 674.8752 (686.3879)  amp_scale: 1.0000 (1.0000)  time: 1.1003  data: 0.3153  max mem: 13835
[08:36:57.577650] [Train][Ep-76/100]  [1200/1435]  eta: 0:04:31  lr: 0.0001 (0.0001)  loss: 1023.2015 (1062.0724)  grad_norm: 670.8608 (685.1495)  amp_scale: 1.0000 (1.0000)  time: 1.2062  data: 0.3749  max mem: 13835
[08:38:59.715656] [Train][Ep-76/100]  [1300/1435]  eta: 0:02:36  lr: 0.0001 (0.0001)  loss: 1032.9252 (1061.0178)  grad_norm: 685.4137 (684.9195)  amp_scale: 1.0000 (1.0000)  time: 1.2326  data: 0.1523  max mem: 13835
[08:40:59.185545] [Train][Ep-76/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1077.9121 (1062.9864)  grad_norm: 670.8624 (684.4715)  amp_scale: 1.0000 (1.0000)  time: 1.2485  data: 0.0002  max mem: 13835
[08:41:37.339690] [Train][Ep-76/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1077.9121 (1062.4243)  grad_norm: 669.5952 (684.2973)  amp_scale: 1.0000 (1.0000)  time: 1.1479  data: 0.0063  max mem: 13835
[08:41:37.340599] [Train][Ep-76/100] Total time: 0:27:47 (1.1623 s / it)
[08:41:37.341022] Syncing meters...
[08:41:39.430602] Averaged stats: lr: 0.0001 (0.0001)  loss: 1077.9121 (1068.6717)  grad_norm: 669.5952 (684.2973)  amp_scale: 1.0000 (1.0000)
[08:41:47.230405] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 77)
[08:41:49.220071] [Train][Ep-77/100]  [   0/1435]  eta: 0:47:23  lr: 0.0001 (0.0001)  time: 1.9817  data: 1.4983  max mem: 13835
[08:43:41.779726] [Train][Ep-77/100]  [ 100/1435]  eta: 0:25:13  lr: 0.0001 (0.0001)  loss: 1088.3265 (1080.0224)  grad_norm: 667.3278 (686.6802)  amp_scale: 1.0000 (1.0000)  time: 1.0892  data: 0.5058  max mem: 13835
[08:45:34.383002] [Train][Ep-77/100]  [ 200/1435]  eta: 0:23:15  lr: 0.0001 (0.0001)  loss: 1062.7740 (1079.4053)  grad_norm: 673.9470 (680.1814)  amp_scale: 1.0000 (1.0000)  time: 1.1101  data: 0.4928  max mem: 13835
[08:47:28.199376] [Train][Ep-77/100]  [ 300/1435]  eta: 0:21:25  lr: 0.0001 (0.0001)  loss: 1055.8749 (1070.5655)  grad_norm: 672.4773 (677.6013)  amp_scale: 1.0000 (1.0000)  time: 1.2109  data: 0.6391  max mem: 13835
[08:49:28.730823] [Train][Ep-77/100]  [ 400/1435]  eta: 0:19:51  lr: 0.0001 (0.0001)  loss: 1068.8732 (1073.0231)  grad_norm: 668.7872 (677.7632)  amp_scale: 1.0000 (1.0000)  time: 1.2585  data: 0.6907  max mem: 13835
[08:51:30.980982] [Train][Ep-77/100]  [ 500/1435]  eta: 0:18:09  lr: 0.0001 (0.0001)  loss: 1088.0411 (1075.7451)  grad_norm: 671.5984 (677.3454)  amp_scale: 1.0000 (1.0000)  time: 1.1861  data: 0.6134  max mem: 13835
[08:53:27.049378] [Train][Ep-77/100]  [ 600/1435]  eta: 0:16:12  lr: 0.0001 (0.0001)  loss: 1044.6100 (1072.2416)  grad_norm: 690.3158 (679.9422)  amp_scale: 1.0000 (1.0000)  time: 1.1263  data: 0.5593  max mem: 13835
[08:55:16.622333] [Train][Ep-77/100]  [ 700/1435]  eta: 0:14:08  lr: 0.0001 (0.0001)  loss: 1028.0948 (1070.0445)  grad_norm: 662.4212 (679.3180)  amp_scale: 1.0000 (1.0000)  time: 1.1010  data: 0.2392  max mem: 13835
[08:57:14.036480] [Train][Ep-77/100]  [ 800/1435]  eta: 0:12:14  lr: 0.0001 (0.0001)  loss: 1052.8165 (1069.2817)  grad_norm: 682.7202 (678.6403)  amp_scale: 1.0000 (1.0000)  time: 1.2267  data: 0.0188  max mem: 13835
[08:59:15.319464] [Train][Ep-77/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0001 (0.0001)  loss: 1068.3997 (1069.6503)  grad_norm: 685.9423 (678.9809)  amp_scale: 1.0000 (1.0000)  time: 1.1817  data: 0.2957  max mem: 13835
[09:01:07.326468] [Train][Ep-77/100]  [1000/1435]  eta: 0:08:24  lr: 0.0001 (0.0001)  loss: 1055.5310 (1068.4554)  grad_norm: 685.5444 (679.9004)  amp_scale: 1.0000 (1.0000)  time: 1.1721  data: 0.1304  max mem: 13835
[09:03:04.983835] [Train][Ep-77/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1050.0424 (1068.6927)  grad_norm: 680.8638 (680.3783)  amp_scale: 1.0000 (1.0000)  time: 1.1948  data: 0.0498  max mem: 13835
[09:05:00.564697] [Train][Ep-77/100]  [1200/1435]  eta: 0:04:32  lr: 0.0001 (0.0001)  loss: 1040.0863 (1067.2801)  grad_norm: 676.2872 (680.4338)  amp_scale: 1.0000 (1.0000)  time: 1.2188  data: 0.0076  max mem: 13835
[09:07:04.646941] [Train][Ep-77/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1063.9473 (1068.5135)  grad_norm: 690.6628 (680.7903)  amp_scale: 1.0000 (1.0000)  time: 1.2578  data: 0.0698  max mem: 13835
[09:08:57.312856] [Train][Ep-77/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1042.4799 (1068.4735)  grad_norm: 671.2648 (680.5644)  amp_scale: 1.0000 (1.0000)  time: 1.0917  data: 0.1968  max mem: 13835
[09:09:34.025815] [Train][Ep-77/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1106.0312 (1069.4987)  grad_norm: 671.2648 (680.7780)  amp_scale: 1.0000 (1.0000)  time: 1.1053  data: 0.2705  max mem: 13835
[09:09:34.026645] [Train][Ep-77/100] Total time: 0:27:46 (1.1615 s / it)
[09:09:34.027097] Syncing meters...
[09:09:35.271368] Averaged stats: lr: 0.0001 (0.0001)  loss: 1106.0312 (1066.0572)  grad_norm: 671.2648 (680.7780)  amp_scale: 1.0000 (1.0000)
[09:09:45.086185] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 78)
[09:09:47.173146] [Train][Ep-78/100]  [   0/1435]  eta: 0:49:42  lr: 0.0001 (0.0001)  time: 2.0784  data: 1.5910  max mem: 13835
[09:11:47.640307] [Train][Ep-78/100]  [ 100/1435]  eta: 0:26:59  lr: 0.0001 (0.0001)  loss: 1073.5684 (1081.5237)  grad_norm: 678.2129 (680.7870)  amp_scale: 1.0000 (1.0000)  time: 1.1954  data: 0.6225  max mem: 13835
[09:13:43.307236] [Train][Ep-78/100]  [ 200/1435]  eta: 0:24:23  lr: 0.0001 (0.0001)  loss: 1045.7581 (1074.4192)  grad_norm: 671.4186 (681.7467)  amp_scale: 1.0000 (1.0000)  time: 1.2428  data: 0.1784  max mem: 13835
[09:15:42.788284] [Train][Ep-78/100]  [ 300/1435]  eta: 0:22:28  lr: 0.0001 (0.0001)  loss: 1062.1049 (1071.1116)  grad_norm: 676.7352 (681.4530)  amp_scale: 1.0000 (1.0000)  time: 1.1741  data: 0.5286  max mem: 13835
[09:17:39.758006] [Train][Ep-78/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0001 (0.0001)  loss: 1049.1520 (1067.1341)  grad_norm: 677.6065 (681.9397)  amp_scale: 1.0000 (1.0000)  time: 1.1788  data: 0.6108  max mem: 13835
[09:19:35.396510] [Train][Ep-78/100]  [ 500/1435]  eta: 0:18:21  lr: 0.0001 (0.0001)  loss: 1061.0901 (1068.3377)  grad_norm: 706.9278 (685.3450)  amp_scale: 1.0000 (1.0000)  time: 1.1725  data: 0.2974  max mem: 13835
[09:21:34.762579] [Train][Ep-78/100]  [ 600/1435]  eta: 0:16:25  lr: 0.0001 (0.0001)  loss: 1033.9883 (1063.4679)  grad_norm: 690.1887 (685.7502)  amp_scale: 1.0000 (1.0000)  time: 1.2030  data: 0.0268  max mem: 13835
[09:23:33.686387] [Train][Ep-78/100]  [ 700/1435]  eta: 0:14:28  lr: 0.0001 (0.0001)  loss: 1070.0062 (1064.8673)  grad_norm: 676.2644 (685.8639)  amp_scale: 1.0000 (1.0000)  time: 1.1442  data: 0.3306  max mem: 13835
[09:25:31.486502] [Train][Ep-78/100]  [ 800/1435]  eta: 0:12:30  lr: 0.0001 (0.0001)  loss: 1055.8590 (1066.7918)  grad_norm: 698.7422 (687.7652)  amp_scale: 1.0000 (1.0000)  time: 1.1300  data: 0.1105  max mem: 13835
[09:27:23.909375] [Train][Ep-78/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0001 (0.0001)  loss: 1058.4919 (1067.1394)  grad_norm: 692.6091 (687.2334)  amp_scale: 1.0000 (1.0000)  time: 1.1054  data: 0.0346  max mem: 13835
[09:29:16.883554] [Train][Ep-78/100]  [1000/1435]  eta: 0:08:29  lr: 0.0001 (0.0001)  loss: 1080.2261 (1067.7624)  grad_norm: 689.1031 (687.9189)  amp_scale: 1.0000 (1.0000)  time: 1.1285  data: 0.3634  max mem: 13835
[09:31:13.377300] [Train][Ep-78/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1053.7640 (1066.8509)  grad_norm: 676.1855 (687.1998)  amp_scale: 1.0000 (1.0000)  time: 1.1427  data: 0.5750  max mem: 13835
[09:33:04.625955] [Train][Ep-78/100]  [1200/1435]  eta: 0:04:33  lr: 0.0001 (0.0001)  loss: 1004.3859 (1063.4362)  grad_norm: 681.3525 (686.8003)  amp_scale: 1.0000 (1.0000)  time: 1.1036  data: 0.5283  max mem: 13835
[09:35:03.585823] [Train][Ep-78/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1042.9982 (1063.1865)  grad_norm: 688.8528 (686.9233)  amp_scale: 1.0000 (1.0000)  time: 1.1690  data: 0.6003  max mem: 13835
[09:36:54.609409] [Train][Ep-78/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1037.4054 (1062.2467)  grad_norm: 658.2577 (686.5463)  amp_scale: 1.0000 (1.0000)  time: 1.0861  data: 0.4899  max mem: 13835
[09:37:32.714789] [Train][Ep-78/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1095.9562 (1062.9366)  grad_norm: 669.9415 (686.4729)  amp_scale: 1.0000 (1.0000)  time: 1.1134  data: 0.5469  max mem: 13835
[09:37:32.715741] [Train][Ep-78/100] Total time: 0:27:47 (1.1621 s / it)
[09:37:32.716203] Syncing meters...
[09:37:32.717470] Averaged stats: lr: 0.0001 (0.0001)  loss: 1095.9562 (1061.8564)  grad_norm: 669.9415 (686.4729)  amp_scale: 1.0000 (1.0000)
[09:37:41.390434] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 79)
[09:37:43.319330] [Train][Ep-79/100]  [   0/1435]  eta: 0:45:56  lr: 0.0001 (0.0001)  time: 1.9206  data: 1.4379  max mem: 13835
[09:39:35.412589] [Train][Ep-79/100]  [ 100/1435]  eta: 0:25:06  lr: 0.0001 (0.0001)  loss: 1075.6271 (1077.9155)  grad_norm: 691.8361 (691.5871)  amp_scale: 1.0000 (1.0000)  time: 1.1271  data: 0.5256  max mem: 13835
[09:41:30.214994] [Train][Ep-79/100]  [ 200/1435]  eta: 0:23:25  lr: 0.0001 (0.0001)  loss: 1056.9100 (1057.3663)  grad_norm: 691.3990 (688.8930)  amp_scale: 1.0000 (1.0000)  time: 1.1431  data: 0.5047  max mem: 13835
[09:43:25.893980] [Train][Ep-79/100]  [ 300/1435]  eta: 0:21:38  lr: 0.0001 (0.0001)  loss: 1066.0895 (1056.4752)  grad_norm: 668.0392 (686.2576)  amp_scale: 1.0000 (1.0000)  time: 1.1193  data: 0.5019  max mem: 13835
[09:45:16.905373] [Train][Ep-79/100]  [ 400/1435]  eta: 0:19:35  lr: 0.0001 (0.0001)  loss: 1074.5405 (1059.5953)  grad_norm: 672.1364 (684.5847)  amp_scale: 1.0000 (1.0000)  time: 1.1070  data: 0.3735  max mem: 13835
[09:47:09.781767] [Train][Ep-79/100]  [ 500/1435]  eta: 0:17:40  lr: 0.0001 (0.0001)  loss: 1036.1951 (1060.2562)  grad_norm: 685.3734 (685.9475)  amp_scale: 1.0000 (1.0000)  time: 1.0880  data: 0.4221  max mem: 13835
[09:49:05.895290] [Train][Ep-79/100]  [ 600/1435]  eta: 0:15:50  lr: 0.0001 (0.0001)  loss: 1049.6255 (1061.3403)  grad_norm: 672.3503 (685.8824)  amp_scale: 1.0000 (1.0000)  time: 1.1866  data: 0.0601  max mem: 13835
[09:51:07.755222] [Train][Ep-79/100]  [ 700/1435]  eta: 0:14:05  lr: 0.0001 (0.0001)  loss: 1007.2571 (1058.0352)  grad_norm: 683.5502 (687.2428)  amp_scale: 1.0000 (1.0000)  time: 1.2548  data: 0.0062  max mem: 13835
[09:53:02.800258] [Train][Ep-79/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0001 (0.0001)  loss: 1020.8529 (1054.1573)  grad_norm: 698.4409 (687.6023)  amp_scale: 1.0000 (1.0000)  time: 1.0797  data: 0.4584  max mem: 13835
[09:54:57.333333] [Train][Ep-79/100]  [ 900/1435]  eta: 0:10:15  lr: 0.0001 (0.0001)  loss: 1085.8020 (1055.7350)  grad_norm: 690.1385 (688.7566)  amp_scale: 1.0000 (1.0000)  time: 1.1598  data: 0.5921  max mem: 13835
[09:56:55.258431] [Train][Ep-79/100]  [1000/1435]  eta: 0:08:21  lr: 0.0001 (0.0001)  loss: 1058.5009 (1057.8414)  grad_norm: 684.8213 (688.1478)  amp_scale: 1.0000 (1.0000)  time: 1.2177  data: 0.6333  max mem: 13835
[09:58:57.100705] [Train][Ep-79/100]  [1100/1435]  eta: 0:06:28  lr: 0.0001 (0.0001)  loss: 1028.6218 (1055.5678)  grad_norm: 681.9256 (688.2843)  amp_scale: 1.0000 (1.0000)  time: 1.2079  data: 0.6314  max mem: 13835
[10:00:53.256606] [Train][Ep-79/100]  [1200/1435]  eta: 0:04:32  lr: 0.0001 (0.0001)  loss: 1062.0582 (1055.9893)  grad_norm: 689.3698 (688.5946)  amp_scale: 1.0000 (1.0000)  time: 1.1534  data: 0.5777  max mem: 13835
[10:02:46.651889] [Train][Ep-79/100]  [1300/1435]  eta: 0:02:36  lr: 0.0001 (0.0001)  loss: 1033.4692 (1056.5706)  grad_norm: 658.1625 (687.1655)  amp_scale: 1.0000 (1.0000)  time: 1.1527  data: 0.5840  max mem: 13835
[10:04:43.073004] [Train][Ep-79/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1038.5288 (1055.6458)  grad_norm: 677.1042 (687.0914)  amp_scale: 1.0000 (1.0000)  time: 1.1412  data: 0.5715  max mem: 13835
[10:05:21.973486] [Train][Ep-79/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1043.9067 (1055.3142)  grad_norm: 677.8640 (687.2157)  amp_scale: 1.0000 (1.0000)  time: 1.1148  data: 0.5488  max mem: 13835
[10:05:21.974338] [Train][Ep-79/100] Total time: 0:27:40 (1.1572 s / it)
[10:05:21.974822] Syncing meters...
[10:05:21.976139] Averaged stats: lr: 0.0001 (0.0001)  loss: 1043.9067 (1060.7140)  grad_norm: 677.8640 (687.2157)  amp_scale: 1.0000 (1.0000)
[10:05:31.624297] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 80)
[10:05:33.610398] [Train][Ep-80/100]  [   0/1435]  eta: 0:47:18  lr: 0.0001 (0.0001)  time: 1.9778  data: 1.4948  max mem: 13835
[10:07:32.570720] [Train][Ep-80/100]  [ 100/1435]  eta: 0:26:38  lr: 0.0001 (0.0001)  loss: 1013.6853 (1020.0942)  grad_norm: 676.9301 (680.9753)  amp_scale: 1.0000 (1.0000)  time: 1.2807  data: 0.7061  max mem: 13835
[10:09:28.028407] [Train][Ep-80/100]  [ 200/1435]  eta: 0:24:12  lr: 0.0001 (0.0001)  loss: 1042.6077 (1033.4379)  grad_norm: 683.8710 (678.6549)  amp_scale: 1.0000 (1.0000)  time: 1.0954  data: 0.5310  max mem: 13835
[10:11:23.852108] [Train][Ep-80/100]  [ 300/1435]  eta: 0:22:08  lr: 0.0001 (0.0001)  loss: 1032.0151 (1038.6062)  grad_norm: 694.4518 (680.4232)  amp_scale: 1.0000 (1.0000)  time: 1.2110  data: 0.6102  max mem: 13835
[10:13:22.980480] [Train][Ep-80/100]  [ 400/1435]  eta: 0:20:16  lr: 0.0001 (0.0001)  loss: 1042.3802 (1043.9232)  grad_norm: 684.9913 (683.6532)  amp_scale: 1.0000 (1.0000)  time: 1.1452  data: 0.5723  max mem: 13835
[10:15:17.760977] [Train][Ep-80/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0001 (0.0001)  loss: 1042.9098 (1047.0901)  grad_norm: 701.4368 (687.4443)  amp_scale: 1.0000 (1.0000)  time: 1.1880  data: 0.2156  max mem: 13835
[10:17:13.314663] [Train][Ep-80/100]  [ 600/1435]  eta: 0:16:14  lr: 0.0001 (0.0001)  loss: 1040.3986 (1047.8682)  grad_norm: 671.9547 (686.7670)  amp_scale: 1.0000 (1.0000)  time: 1.1779  data: 0.6108  max mem: 13835
[10:19:09.815316] [Train][Ep-80/100]  [ 700/1435]  eta: 0:14:17  lr: 0.0001 (0.0001)  loss: 1061.6506 (1051.3265)  grad_norm: 692.4233 (685.5701)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.2998  max mem: 13835
[10:21:08.984131] [Train][Ep-80/100]  [ 800/1435]  eta: 0:12:23  lr: 0.0001 (0.0001)  loss: 1059.7665 (1054.0582)  grad_norm: 674.9196 (685.4845)  amp_scale: 1.0000 (1.0000)  time: 1.1511  data: 0.3267  max mem: 13835
[10:23:05.083972] [Train][Ep-80/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0001 (0.0001)  loss: 1034.3602 (1053.1918)  grad_norm: 676.2615 (684.5965)  amp_scale: 1.0000 (1.0000)  time: 1.1285  data: 0.2698  max mem: 13835
[10:25:03.532513] [Train][Ep-80/100]  [1000/1435]  eta: 0:08:29  lr: 0.0001 (0.0001)  loss: 1055.5342 (1054.3581)  grad_norm: 693.8558 (685.5413)  amp_scale: 1.0000 (1.0000)  time: 1.3181  data: 0.7443  max mem: 13835
[10:27:08.729747] [Train][Ep-80/100]  [1100/1435]  eta: 0:06:34  lr: 0.0001 (0.0001)  loss: 1047.1354 (1055.1103)  grad_norm: 667.6718 (684.3177)  amp_scale: 1.0000 (1.0000)  time: 1.2210  data: 0.6525  max mem: 13835
[10:29:03.753567] [Train][Ep-80/100]  [1200/1435]  eta: 0:04:36  lr: 0.0001 (0.0001)  loss: 1065.5798 (1056.5909)  grad_norm: 682.6656 (684.6242)  amp_scale: 1.0000 (1.0000)  time: 1.1468  data: 0.5656  max mem: 13835
[10:31:02.695022] [Train][Ep-80/100]  [1300/1435]  eta: 0:02:38  lr: 0.0001 (0.0001)  loss: 1089.2003 (1057.9319)  grad_norm: 683.8950 (684.4241)  amp_scale: 1.0000 (1.0000)  time: 1.2099  data: 0.6401  max mem: 13835
[10:32:57.484896] [Train][Ep-80/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1033.3420 (1056.5953)  grad_norm: 674.7248 (684.3830)  amp_scale: 1.0000 (1.0000)  time: 1.1188  data: 0.5511  max mem: 13835
[10:33:34.450392] [Train][Ep-80/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1016.9595 (1056.0541)  grad_norm: 667.1282 (684.2069)  amp_scale: 1.0000 (1.0000)  time: 1.0844  data: 0.4706  max mem: 13835
[10:33:34.451314] [Train][Ep-80/100] Total time: 0:28:02 (1.1727 s / it)
[10:33:34.451701] Syncing meters...
[10:33:35.066104] Averaged stats: lr: 0.0001 (0.0001)  loss: 1016.9595 (1055.2081)  grad_norm: 667.1282 (684.2069)  amp_scale: 1.0000 (1.0000)
[10:33:37.194133] [Eval][Ep-80/100]  [  0/121]  eta: 0:04:16  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.1197  data: 1.9596  max mem: 13835
[10:35:21.348146] [Eval][Ep-80/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9488  data: 0.7889  max mem: 13835
[10:35:39.897389] [Eval][Ep-80/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9274  data: 0.7702  max mem: 13835
[10:35:39.898408] [Eval][Ep-80/100] Total time: 0:02:04 (1.0316 s / it)
[10:35:40.211528] [Eval][Ep-80/100] val_acc1_image=26.56 | val_acc1_audio=42.19 | val_acc1_fusion=38.85 | val_acc1_all=52.74
[10:35:51.654095] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 81)
[10:35:53.723633] [Train][Ep-81/100]  [   0/1435]  eta: 0:49:17  lr: 0.0001 (0.0001)  time: 2.0610  data: 1.5809  max mem: 13835
[10:37:51.202394] [Train][Ep-81/100]  [ 100/1435]  eta: 0:26:20  lr: 0.0001 (0.0001)  loss: 1027.5261 (1046.5609)  grad_norm: 673.4800 (681.4575)  amp_scale: 1.0000 (1.0000)  time: 1.2021  data: 0.3688  max mem: 13835
[10:39:44.745884] [Train][Ep-81/100]  [ 200/1435]  eta: 0:23:51  lr: 0.0001 (0.0001)  loss: 1023.4033 (1034.9870)  grad_norm: 674.6576 (680.9622)  amp_scale: 1.0000 (1.0000)  time: 1.1501  data: 0.2711  max mem: 13835
[10:41:37.464030] [Train][Ep-81/100]  [ 300/1435]  eta: 0:21:43  lr: 0.0001 (0.0001)  loss: 1047.2469 (1041.1178)  grad_norm: 677.3405 (681.5164)  amp_scale: 1.0000 (1.0000)  time: 1.0924  data: 0.5185  max mem: 13835
[10:43:35.976830] [Train][Ep-81/100]  [ 400/1435]  eta: 0:19:58  lr: 0.0001 (0.0001)  loss: 1056.5643 (1045.3714)  grad_norm: 677.3985 (683.8157)  amp_scale: 1.0000 (1.0000)  time: 1.1424  data: 0.5725  max mem: 13835
[10:45:29.782749] [Train][Ep-81/100]  [ 500/1435]  eta: 0:17:58  lr: 0.0001 (0.0001)  loss: 1052.8258 (1048.7640)  grad_norm: 694.3003 (685.3914)  amp_scale: 1.0000 (1.0000)  time: 1.1305  data: 0.1100  max mem: 13835
[10:47:24.234752] [Train][Ep-81/100]  [ 600/1435]  eta: 0:16:02  lr: 0.0001 (0.0001)  loss: 1095.7434 (1052.8001)  grad_norm: 676.0696 (684.0411)  amp_scale: 1.0000 (1.0000)  time: 1.2068  data: 0.1636  max mem: 13835
[10:49:25.559887] [Train][Ep-81/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0001 (0.0001)  loss: 1025.0859 (1052.2789)  grad_norm: 685.8870 (684.0933)  amp_scale: 1.0000 (1.0000)  time: 1.2254  data: 0.4842  max mem: 13835
[10:51:25.944404] [Train][Ep-81/100]  [ 800/1435]  eta: 0:12:20  lr: 0.0001 (0.0001)  loss: 1066.2548 (1054.4824)  grad_norm: 694.9993 (685.3000)  amp_scale: 1.0000 (1.0000)  time: 1.2756  data: 0.2522  max mem: 13835
[10:53:24.297254] [Train][Ep-81/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0001 (0.0001)  loss: 1022.0170 (1052.6112)  grad_norm: 688.1995 (686.1200)  amp_scale: 1.0000 (1.0000)  time: 1.1159  data: 0.4954  max mem: 13835
[10:55:22.508553] [Train][Ep-81/100]  [1000/1435]  eta: 0:08:28  lr: 0.0001 (0.0001)  loss: 1079.3434 (1054.3538)  grad_norm: 686.4280 (686.5337)  amp_scale: 1.0000 (1.0000)  time: 1.1483  data: 0.1902  max mem: 13835
[10:57:17.936334] [Train][Ep-81/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1031.7186 (1054.1831)  grad_norm: 676.3654 (686.1713)  amp_scale: 1.0000 (1.0000)  time: 1.1684  data: 0.2764  max mem: 13835
[10:59:13.720542] [Train][Ep-81/100]  [1200/1435]  eta: 0:04:34  lr: 0.0001 (0.0001)  loss: 1037.9371 (1054.3216)  grad_norm: 689.1494 (686.9735)  amp_scale: 1.0000 (1.0000)  time: 1.1841  data: 0.4193  max mem: 13835
[11:01:06.919837] [Train][Ep-81/100]  [1300/1435]  eta: 0:02:37  lr: 0.0001 (0.0001)  loss: 1017.3674 (1052.2882)  grad_norm: 682.2766 (687.4952)  amp_scale: 1.0000 (1.0000)  time: 1.1354  data: 0.5128  max mem: 13835
[11:03:06.304775] [Train][Ep-81/100]  [1400/1435]  eta: 0:00:40  lr: 0.0001 (0.0001)  loss: 1039.7408 (1052.5695)  grad_norm: 686.8774 (687.4911)  amp_scale: 1.0000 (1.0000)  time: 1.2113  data: 0.6350  max mem: 13835
[11:03:44.498957] [Train][Ep-81/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1067.7977 (1053.1848)  grad_norm: 686.8774 (687.4577)  amp_scale: 1.0000 (1.0000)  time: 1.1201  data: 0.5383  max mem: 13835
[11:03:44.499846] [Train][Ep-81/100] Total time: 0:27:52 (1.1657 s / it)
[11:03:44.500313] Syncing meters...
[11:03:44.501666] Averaged stats: lr: 0.0001 (0.0001)  loss: 1067.7977 (1054.4260)  grad_norm: 686.8774 (687.4577)  amp_scale: 1.0000 (1.0000)
[11:03:53.128377] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 82)
[11:03:55.436315] [Train][Ep-82/100]  [   0/1435]  eta: 0:54:59  lr: 0.0001 (0.0001)  time: 2.2993  data: 1.8156  max mem: 13835
[11:05:51.792606] [Train][Ep-82/100]  [ 100/1435]  eta: 0:26:08  lr: 0.0001 (0.0001)  loss: 1065.6921 (1079.9443)  grad_norm: 689.2148 (692.6549)  amp_scale: 1.0000 (1.0000)  time: 1.1710  data: 0.4609  max mem: 13835
[11:07:49.128672] [Train][Ep-82/100]  [ 200/1435]  eta: 0:24:09  lr: 0.0001 (0.0001)  loss: 1082.2977 (1074.0205)  grad_norm: 691.2278 (693.8195)  amp_scale: 1.0000 (1.0000)  time: 1.0946  data: 0.3843  max mem: 13835
[11:09:50.270065] [Train][Ep-82/100]  [ 300/1435]  eta: 0:22:26  lr: 0.0001 (0.0001)  loss: 1062.3594 (1068.9349)  grad_norm: 683.0720 (691.4716)  amp_scale: 1.0000 (1.0000)  time: 1.1403  data: 0.1697  max mem: 13835
[11:11:44.668501] [Train][Ep-82/100]  [ 400/1435]  eta: 0:20:16  lr: 0.0001 (0.0001)  loss: 1050.4236 (1064.7872)  grad_norm: 674.7180 (688.5417)  amp_scale: 1.0000 (1.0000)  time: 1.1697  data: 0.3592  max mem: 13835
[11:13:41.479793] [Train][Ep-82/100]  [ 500/1435]  eta: 0:18:17  lr: 0.0001 (0.0001)  loss: 1035.4126 (1056.1363)  grad_norm: 659.9878 (685.5563)  amp_scale: 1.0000 (1.0000)  time: 1.1708  data: 0.5907  max mem: 13835
[11:15:39.952991] [Train][Ep-82/100]  [ 600/1435]  eta: 0:16:21  lr: 0.0001 (0.0001)  loss: 1073.6638 (1056.9512)  grad_norm: 695.6469 (686.4631)  amp_scale: 1.0000 (1.0000)  time: 1.1807  data: 0.5520  max mem: 13835
[11:17:37.302293] [Train][Ep-82/100]  [ 700/1435]  eta: 0:14:24  lr: 0.0001 (0.0001)  loss: 1063.2588 (1056.5258)  grad_norm: 672.0790 (685.3524)  amp_scale: 1.0000 (1.0000)  time: 1.1237  data: 0.5554  max mem: 13835
[11:19:34.795533] [Train][Ep-82/100]  [ 800/1435]  eta: 0:12:26  lr: 0.0001 (0.0001)  loss: 1055.6086 (1056.5106)  grad_norm: 692.8446 (685.5180)  amp_scale: 1.0000 (1.0000)  time: 1.1827  data: 0.6143  max mem: 13835
[11:21:31.639960] [Train][Ep-82/100]  [ 900/1435]  eta: 0:10:28  lr: 0.0001 (0.0001)  loss: 1060.9070 (1057.0228)  grad_norm: 682.1191 (686.4732)  amp_scale: 1.0000 (1.0000)  time: 1.1639  data: 0.1657  max mem: 13835
[11:23:32.256883] [Train][Ep-82/100]  [1000/1435]  eta: 0:08:32  lr: 0.0001 (0.0001)  loss: 1064.6490 (1058.0537)  grad_norm: 667.5654 (686.2222)  amp_scale: 1.0000 (1.0000)  time: 1.2419  data: 0.0669  max mem: 13835
[11:25:35.491976] [Train][Ep-82/100]  [1100/1435]  eta: 0:06:36  lr: 0.0001 (0.0001)  loss: 1034.4622 (1057.2600)  grad_norm: 668.5081 (685.2854)  amp_scale: 1.0000 (1.0000)  time: 1.2130  data: 0.2487  max mem: 13835
[11:27:36.085242] [Train][Ep-82/100]  [1200/1435]  eta: 0:04:38  lr: 0.0001 (0.0001)  loss: 1058.5295 (1056.6802)  grad_norm: 680.3416 (685.9134)  amp_scale: 1.0000 (1.0000)  time: 1.1938  data: 0.0156  max mem: 13835
[11:29:40.987143] [Train][Ep-82/100]  [1300/1435]  eta: 0:02:40  lr: 0.0001 (0.0001)  loss: 1062.8964 (1057.8955)  grad_norm: 675.3878 (685.6195)  amp_scale: 1.0000 (1.0000)  time: 1.2446  data: 0.0804  max mem: 13835
[11:31:40.725500] [Train][Ep-82/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1081.8910 (1058.9300)  grad_norm: 696.1475 (686.8689)  amp_scale: 1.0000 (1.0000)  time: 1.1865  data: 0.0006  max mem: 13835
[11:32:20.162281] [Train][Ep-82/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1060.0399 (1057.9759)  grad_norm: 697.3923 (686.8310)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.0693  max mem: 13835
[11:32:20.163188] [Train][Ep-82/100] Total time: 0:28:27 (1.1896 s / it)
[11:32:20.163667] Syncing meters...
[11:32:20.561638] Averaged stats: lr: 0.0001 (0.0001)  loss: 1060.0399 (1055.3000)  grad_norm: 697.3923 (686.8310)  amp_scale: 1.0000 (1.0000)
[11:32:30.361812] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 83)
[11:32:32.100962] [Train][Ep-83/100]  [   0/1435]  eta: 0:41:23  lr: 0.0001 (0.0001)  time: 1.7309  data: 1.2474  max mem: 13835
[11:34:29.946114] [Train][Ep-83/100]  [ 100/1435]  eta: 0:26:20  lr: 0.0001 (0.0001)  loss: 1036.9167 (1047.4537)  grad_norm: 688.0511 (691.2917)  amp_scale: 1.0000 (1.0000)  time: 1.1555  data: 0.5867  max mem: 13835
[11:36:27.918322] [Train][Ep-83/100]  [ 200/1435]  eta: 0:24:19  lr: 0.0001 (0.0001)  loss: 1041.2346 (1052.2202)  grad_norm: 693.4244 (697.1421)  amp_scale: 1.0000 (1.0000)  time: 1.1420  data: 0.5731  max mem: 13835
[11:38:24.506443] [Train][Ep-83/100]  [ 300/1435]  eta: 0:22:15  lr: 0.0001 (0.0001)  loss: 1050.2529 (1053.2012)  grad_norm: 680.9946 (692.1386)  amp_scale: 1.0000 (1.0000)  time: 1.1514  data: 0.4148  max mem: 13835
[11:40:22.959139] [Train][Ep-83/100]  [ 400/1435]  eta: 0:20:19  lr: 0.0001 (0.0001)  loss: 1067.5342 (1053.0981)  grad_norm: 673.6266 (691.8336)  amp_scale: 1.0000 (1.0000)  time: 1.1258  data: 0.0448  max mem: 13835
[11:42:20.749730] [Train][Ep-83/100]  [ 500/1435]  eta: 0:18:21  lr: 0.0001 (0.0001)  loss: 1034.6433 (1056.1050)  grad_norm: 681.6315 (690.0896)  amp_scale: 1.0000 (1.0000)  time: 1.1491  data: 0.0003  max mem: 13835
[11:44:16.196618] [Train][Ep-83/100]  [ 600/1435]  eta: 0:16:20  lr: 0.0001 (0.0001)  loss: 1046.0170 (1055.9000)  grad_norm: 676.1472 (689.2113)  amp_scale: 1.0000 (1.0000)  time: 1.1351  data: 0.0961  max mem: 13835
[11:46:15.835797] [Train][Ep-83/100]  [ 700/1435]  eta: 0:14:25  lr: 0.0001 (0.0001)  loss: 1040.1155 (1057.2309)  grad_norm: 661.7728 (687.2778)  amp_scale: 1.0000 (1.0000)  time: 1.2241  data: 0.1891  max mem: 13835
[11:48:13.683633] [Train][Ep-83/100]  [ 800/1435]  eta: 0:12:27  lr: 0.0001 (0.0001)  loss: 1054.9755 (1056.3391)  grad_norm: 678.4384 (686.7973)  amp_scale: 1.0000 (1.0000)  time: 1.2418  data: 0.0198  max mem: 13835
[11:50:13.519316] [Train][Ep-83/100]  [ 900/1435]  eta: 0:10:31  lr: 0.0001 (0.0001)  loss: 1034.9675 (1057.3174)  grad_norm: 680.5355 (686.6651)  amp_scale: 1.0000 (1.0000)  time: 1.2009  data: 0.1306  max mem: 13835
[11:52:15.151252] [Train][Ep-83/100]  [1000/1435]  eta: 0:08:34  lr: 0.0001 (0.0001)  loss: 1026.3903 (1056.3259)  grad_norm: 690.4045 (687.1541)  amp_scale: 1.0000 (1.0000)  time: 1.2262  data: 0.0031  max mem: 13835
[11:54:18.897929] [Train][Ep-83/100]  [1100/1435]  eta: 0:06:38  lr: 0.0001 (0.0001)  loss: 1057.5925 (1057.7507)  grad_norm: 680.5691 (686.5587)  amp_scale: 1.0000 (1.0000)  time: 1.2365  data: 0.0030  max mem: 13835
[11:56:17.795339] [Train][Ep-83/100]  [1200/1435]  eta: 0:04:39  lr: 0.0001 (0.0001)  loss: 1066.3077 (1058.4945)  grad_norm: 678.3096 (686.9267)  amp_scale: 1.0000 (1.0000)  time: 1.1908  data: 0.1758  max mem: 13835
[11:58:12.225888] [Train][Ep-83/100]  [1300/1435]  eta: 0:02:39  lr: 0.0001 (0.0001)  loss: 1041.1040 (1056.6067)  grad_norm: 696.1634 (687.9327)  amp_scale: 1.0000 (1.0000)  time: 1.1137  data: 0.5300  max mem: 13835
[12:00:11.531152] [Train][Ep-83/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1088.6390 (1057.4694)  grad_norm: 692.2389 (688.9125)  amp_scale: 1.0000 (1.0000)  time: 1.2029  data: 0.6373  max mem: 13835
[12:00:49.324151] [Train][Ep-83/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1053.8075 (1056.4536)  grad_norm: 694.0981 (689.2371)  amp_scale: 1.0000 (1.0000)  time: 1.0528  data: 0.4834  max mem: 13835
[12:00:49.325172] [Train][Ep-83/100] Total time: 0:28:18 (1.1839 s / it)
[12:00:49.325639] Syncing meters...
[12:00:49.327036] Averaged stats: lr: 0.0001 (0.0001)  loss: 1053.8075 (1055.0060)  grad_norm: 694.0981 (689.2371)  amp_scale: 1.0000 (1.0000)
[12:00:58.985362] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 84)
[12:01:00.837038] [Train][Ep-84/100]  [   0/1435]  eta: 0:44:05  lr: 0.0001 (0.0001)  time: 1.8434  data: 1.3597  max mem: 13835
[12:03:01.157419] [Train][Ep-84/100]  [ 100/1435]  eta: 0:26:54  lr: 0.0001 (0.0001)  loss: 1037.4651 (1029.1276)  grad_norm: 686.5132 (692.0082)  amp_scale: 1.0000 (1.0000)  time: 1.1848  data: 0.6049  max mem: 13835
[12:04:52.475499] [Train][Ep-84/100]  [ 200/1435]  eta: 0:23:54  lr: 0.0001 (0.0001)  loss: 1016.3312 (1035.3600)  grad_norm: 679.8607 (695.0947)  amp_scale: 1.0000 (1.0000)  time: 1.1015  data: 0.5165  max mem: 13835
[12:06:49.511724] [Train][Ep-84/100]  [ 300/1435]  eta: 0:22:01  lr: 0.0001 (0.0001)  loss: 1063.0984 (1044.7947)  grad_norm: 682.6522 (690.0884)  amp_scale: 1.0000 (1.0000)  time: 1.2448  data: 0.0007  max mem: 13835
[12:08:50.606117] [Train][Ep-84/100]  [ 400/1435]  eta: 0:20:17  lr: 0.0001 (0.0001)  loss: 1037.1849 (1043.7847)  grad_norm: 676.3447 (686.5445)  amp_scale: 1.0000 (1.0000)  time: 1.2276  data: 0.6530  max mem: 13835
[12:10:45.015759] [Train][Ep-84/100]  [ 500/1435]  eta: 0:18:13  lr: 0.0001 (0.0001)  loss: 1073.8920 (1044.7574)  grad_norm: 692.1842 (686.2865)  amp_scale: 1.0000 (1.0000)  time: 1.0429  data: 0.4568  max mem: 13835
[12:12:41.444491] [Train][Ep-84/100]  [ 600/1435]  eta: 0:16:15  lr: 0.0001 (0.0001)  loss: 1047.9431 (1050.1239)  grad_norm: 691.9242 (687.4208)  amp_scale: 1.0000 (1.0000)  time: 1.1622  data: 0.0431  max mem: 13835
[12:14:38.945229] [Train][Ep-84/100]  [ 700/1435]  eta: 0:14:19  lr: 0.0001 (0.0001)  loss: 1037.6948 (1049.1462)  grad_norm: 695.4940 (689.8350)  amp_scale: 1.0000 (1.0000)  time: 1.1842  data: 0.1616  max mem: 13835
[12:16:35.258476] [Train][Ep-84/100]  [ 800/1435]  eta: 0:12:22  lr: 0.0001 (0.0001)  loss: 1058.9448 (1050.7913)  grad_norm: 677.8946 (688.8985)  amp_scale: 1.0000 (1.0000)  time: 1.1663  data: 0.3081  max mem: 13835
[12:18:33.032979] [Train][Ep-84/100]  [ 900/1435]  eta: 0:10:25  lr: 0.0001 (0.0001)  loss: 1022.7520 (1050.3188)  grad_norm: 688.2394 (689.6954)  amp_scale: 1.0000 (1.0000)  time: 1.0957  data: 0.5012  max mem: 13835
[12:20:27.456957] [Train][Ep-84/100]  [1000/1435]  eta: 0:08:27  lr: 0.0001 (0.0001)  loss: 1086.4203 (1052.5765)  grad_norm: 702.2612 (690.2956)  amp_scale: 1.0000 (1.0000)  time: 1.1409  data: 0.3227  max mem: 13835
[12:22:25.701438] [Train][Ep-84/100]  [1100/1435]  eta: 0:06:31  lr: 0.0001 (0.0001)  loss: 1025.3740 (1049.8959)  grad_norm: 682.6564 (690.6962)  amp_scale: 1.0000 (1.0000)  time: 1.2787  data: 0.7102  max mem: 13835
[12:24:25.435923] [Train][Ep-84/100]  [1200/1435]  eta: 0:04:35  lr: 0.0001 (0.0001)  loss: 1050.0243 (1050.3052)  grad_norm: 677.3208 (690.6102)  amp_scale: 1.0000 (1.0000)  time: 1.1736  data: 0.5984  max mem: 13835
[12:26:24.510412] [Train][Ep-84/100]  [1300/1435]  eta: 0:02:38  lr: 0.0001 (0.0001)  loss: 1007.5735 (1048.5358)  grad_norm: 674.5454 (690.4139)  amp_scale: 1.0000 (1.0000)  time: 1.1623  data: 0.5957  max mem: 13835
[12:28:21.301227] [Train][Ep-84/100]  [1400/1435]  eta: 0:00:41  lr: 0.0001 (0.0001)  loss: 1039.5994 (1047.8084)  grad_norm: 697.3521 (689.8787)  amp_scale: 1.0000 (1.0000)  time: 1.2199  data: 0.0072  max mem: 13835
[12:29:00.136686] [Train][Ep-84/100]  [1434/1435]  eta: 0:00:01  lr: 0.0001 (0.0001)  loss: 1063.7827 (1048.3719)  grad_norm: 693.5850 (689.9171)  amp_scale: 1.0000 (1.0000)  time: 1.1950  data: 0.0134  max mem: 13835
[12:29:00.137582] [Train][Ep-84/100] Total time: 0:28:01 (1.1715 s / it)
[12:29:00.138037] Syncing meters...
[12:29:00.352121] Averaged stats: lr: 0.0001 (0.0001)  loss: 1063.7827 (1048.1204)  grad_norm: 693.5850 (689.9171)  amp_scale: 1.0000 (1.0000)
[12:29:10.346707] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 85)
[12:29:12.465175] [Train][Ep-85/100]  [   0/1435]  eta: 0:50:27  lr: 0.0001 (0.0001)  time: 2.1095  data: 1.6265  max mem: 13835
[12:31:08.891961] [Train][Ep-85/100]  [ 100/1435]  eta: 0:26:06  lr: 0.0001 (0.0001)  loss: 1032.3246 (1037.0614)  grad_norm: 684.9958 (691.8228)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.5902  max mem: 13835
[12:33:00.702804] [Train][Ep-85/100]  [ 200/1435]  eta: 0:23:35  lr: 0.0000 (0.0001)  loss: 1030.9352 (1044.4835)  grad_norm: 670.0302 (687.4771)  amp_scale: 1.0000 (1.0000)  time: 1.1218  data: 0.3696  max mem: 13835
[12:34:57.201082] [Train][Ep-85/100]  [ 300/1435]  eta: 0:21:47  lr: 0.0000 (0.0000)  loss: 1039.9890 (1039.8793)  grad_norm: 669.1379 (687.5740)  amp_scale: 1.0000 (1.0000)  time: 1.2049  data: 0.0192  max mem: 13835
[12:36:55.962059] [Train][Ep-85/100]  [ 400/1435]  eta: 0:20:01  lr: 0.0000 (0.0000)  loss: 1047.7690 (1041.9872)  grad_norm: 686.3672 (688.6730)  amp_scale: 1.0000 (1.0000)  time: 1.1439  data: 0.0012  max mem: 13835
[12:38:51.348592] [Train][Ep-85/100]  [ 500/1435]  eta: 0:18:04  lr: 0.0000 (0.0000)  loss: 1020.8358 (1039.9724)  grad_norm: 670.1310 (686.5050)  amp_scale: 1.0000 (1.0000)  time: 1.0910  data: 0.3832  max mem: 13835
[12:40:46.277088] [Train][Ep-85/100]  [ 600/1435]  eta: 0:16:06  lr: 0.0000 (0.0000)  loss: 1035.4883 (1041.2013)  grad_norm: 682.5488 (685.5152)  amp_scale: 1.0000 (1.0000)  time: 1.1704  data: 0.5902  max mem: 13835
[12:42:44.869770] [Train][Ep-85/100]  [ 700/1435]  eta: 0:14:13  lr: 0.0000 (0.0000)  loss: 1036.7031 (1043.5844)  grad_norm: 685.5567 (686.3149)  amp_scale: 1.0000 (1.0000)  time: 1.2202  data: 0.6458  max mem: 13835
[12:44:42.328619] [Train][Ep-85/100]  [ 800/1435]  eta: 0:12:18  lr: 0.0000 (0.0000)  loss: 1027.8790 (1042.4043)  grad_norm: 687.5089 (686.9376)  amp_scale: 1.0000 (1.0000)  time: 1.1844  data: 0.2223  max mem: 13835
[12:46:42.582843] [Train][Ep-85/100]  [ 900/1435]  eta: 0:10:24  lr: 0.0000 (0.0000)  loss: 1024.7742 (1042.9702)  grad_norm: 681.3967 (686.2231)  amp_scale: 1.0000 (1.0000)  time: 1.1439  data: 0.0789  max mem: 13835
[12:48:39.843685] [Train][Ep-85/100]  [1000/1435]  eta: 0:08:28  lr: 0.0000 (0.0000)  loss: 1050.5979 (1043.4655)  grad_norm: 685.2410 (686.9412)  amp_scale: 1.0000 (1.0000)  time: 1.1698  data: 0.1355  max mem: 13835
[12:50:37.105839] [Train][Ep-85/100]  [1100/1435]  eta: 0:06:31  lr: 0.0000 (0.0000)  loss: 1055.7356 (1045.8224)  grad_norm: 677.6981 (686.5965)  amp_scale: 1.0000 (1.0000)  time: 1.1105  data: 0.4907  max mem: 13835
[12:52:29.385765] [Train][Ep-85/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1033.7764 (1046.5535)  grad_norm: 690.6086 (686.5514)  amp_scale: 1.0000 (1.0000)  time: 1.1273  data: 0.1021  max mem: 13835
[12:54:24.228808] [Train][Ep-85/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1025.6887 (1046.1680)  grad_norm: 691.6834 (686.7935)  amp_scale: 1.0000 (1.0000)  time: 1.1989  data: 0.0449  max mem: 13835
[12:56:22.381752] [Train][Ep-85/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1045.3629 (1046.9381)  grad_norm: 677.1940 (686.5532)  amp_scale: 1.0000 (1.0000)  time: 1.1998  data: 0.1354  max mem: 13835
[12:57:02.588144] [Train][Ep-85/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1037.1481 (1046.3477)  grad_norm: 677.8402 (686.5897)  amp_scale: 1.0000 (1.0000)  time: 1.1822  data: 0.0126  max mem: 13835
[12:57:02.589118] [Train][Ep-85/100] Total time: 0:27:52 (1.1653 s / it)
[12:57:02.589582] Syncing meters...
[12:57:02.909838] Averaged stats: lr: 0.0000 (0.0000)  loss: 1037.1481 (1047.7032)  grad_norm: 677.8402 (686.5897)  amp_scale: 1.0000 (1.0000)
[12:57:11.862081] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 86)
[12:57:13.772707] [Train][Ep-86/100]  [   0/1435]  eta: 0:45:29  lr: 0.0000 (0.0000)  time: 1.9020  data: 1.4188  max mem: 13835
[12:59:11.809697] [Train][Ep-86/100]  [ 100/1435]  eta: 0:26:25  lr: 0.0000 (0.0000)  loss: 1038.5996 (1053.1040)  grad_norm: 685.8779 (697.1996)  amp_scale: 1.0000 (1.0000)  time: 1.1525  data: 0.5795  max mem: 13835
[13:01:11.273914] [Train][Ep-86/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0000 (0.0000)  loss: 1068.8082 (1052.0811)  grad_norm: 680.7664 (692.2754)  amp_scale: 1.0000 (1.0000)  time: 1.2291  data: 0.6602  max mem: 13835
[13:03:11.332569] [Train][Ep-86/100]  [ 300/1435]  eta: 0:22:35  lr: 0.0000 (0.0000)  loss: 1051.6527 (1046.2138)  grad_norm: 683.5003 (690.4604)  amp_scale: 1.0000 (1.0000)  time: 1.1831  data: 0.3647  max mem: 13835
[13:05:07.985060] [Train][Ep-86/100]  [ 400/1435]  eta: 0:20:28  lr: 0.0000 (0.0000)  loss: 1028.2860 (1044.5433)  grad_norm: 674.6774 (689.2773)  amp_scale: 1.0000 (1.0000)  time: 1.1613  data: 0.3649  max mem: 13835
[13:07:05.655753] [Train][Ep-86/100]  [ 500/1435]  eta: 0:18:28  lr: 0.0000 (0.0000)  loss: 1035.6721 (1047.6627)  grad_norm: 685.7780 (686.8714)  amp_scale: 1.0000 (1.0000)  time: 1.1965  data: 0.3023  max mem: 13835
[13:09:00.752527] [Train][Ep-86/100]  [ 600/1435]  eta: 0:16:24  lr: 0.0000 (0.0000)  loss: 1057.4012 (1047.9201)  grad_norm: 684.8220 (686.8907)  amp_scale: 1.0000 (1.0000)  time: 1.1743  data: 0.2339  max mem: 13835
[13:10:55.803210] [Train][Ep-86/100]  [ 700/1435]  eta: 0:14:23  lr: 0.0000 (0.0000)  loss: 1060.5358 (1047.8241)  grad_norm: 675.7974 (686.2343)  amp_scale: 1.0000 (1.0000)  time: 1.1744  data: 0.2391  max mem: 13835
[13:12:51.827339] [Train][Ep-86/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0000 (0.0000)  loss: 1030.4380 (1045.2025)  grad_norm: 669.1720 (685.1610)  amp_scale: 1.0000 (1.0000)  time: 1.2459  data: 0.6741  max mem: 13835
[13:14:48.998700] [Train][Ep-86/100]  [ 900/1435]  eta: 0:10:27  lr: 0.0000 (0.0000)  loss: 1023.2675 (1044.9327)  grad_norm: 693.1122 (686.5553)  amp_scale: 1.0000 (1.0000)  time: 1.1037  data: 0.4267  max mem: 13835
[13:16:46.079831] [Train][Ep-86/100]  [1000/1435]  eta: 0:08:30  lr: 0.0000 (0.0000)  loss: 1067.7552 (1046.4358)  grad_norm: 675.7424 (685.9051)  amp_scale: 1.0000 (1.0000)  time: 1.1149  data: 0.0962  max mem: 13835
[13:18:41.996931] [Train][Ep-86/100]  [1100/1435]  eta: 0:06:32  lr: 0.0000 (0.0000)  loss: 1057.8873 (1047.2745)  grad_norm: 666.6590 (685.8187)  amp_scale: 1.0000 (1.0000)  time: 1.1386  data: 0.5567  max mem: 13835
[13:20:32.114891] [Train][Ep-86/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1023.3745 (1046.3672)  grad_norm: 665.7202 (685.1797)  amp_scale: 1.0000 (1.0000)  time: 1.0968  data: 0.4875  max mem: 13835
[13:22:26.555961] [Train][Ep-86/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1051.5106 (1047.5703)  grad_norm: 688.4891 (685.6853)  amp_scale: 1.0000 (1.0000)  time: 1.0988  data: 0.5110  max mem: 13835
[13:24:21.238928] [Train][Ep-86/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1030.6261 (1047.3028)  grad_norm: 675.1705 (685.9671)  amp_scale: 1.0000 (1.0000)  time: 1.1689  data: 0.5861  max mem: 13835
[13:24:58.402719] [Train][Ep-86/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1040.1255 (1046.2252)  grad_norm: 703.4389 (686.2306)  amp_scale: 1.0000 (1.0000)  time: 1.0722  data: 0.5023  max mem: 13835
[13:24:58.403605] [Train][Ep-86/100] Total time: 0:27:46 (1.1613 s / it)
[13:24:58.404075] Syncing meters...
[13:24:58.405380] Averaged stats: lr: 0.0000 (0.0000)  loss: 1040.1255 (1041.4928)  grad_norm: 703.4389 (686.2306)  amp_scale: 1.0000 (1.0000)
[13:25:08.454318] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 87)
[13:25:10.226537] [Train][Ep-87/100]  [   0/1435]  eta: 0:42:10  lr: 0.0000 (0.0000)  time: 1.7635  data: 1.2786  max mem: 13835
[13:27:07.783824] [Train][Ep-87/100]  [ 100/1435]  eta: 0:26:17  lr: 0.0000 (0.0000)  loss: 1046.3768 (1065.8442)  grad_norm: 673.1435 (684.5685)  amp_scale: 1.0000 (1.0000)  time: 1.1425  data: 0.5701  max mem: 13835
[13:29:01.258179] [Train][Ep-87/100]  [ 200/1435]  eta: 0:23:50  lr: 0.0000 (0.0000)  loss: 1048.3359 (1052.5653)  grad_norm: 697.4131 (687.7000)  amp_scale: 1.0000 (1.0000)  time: 1.0917  data: 0.5122  max mem: 13835
[13:30:58.148282] [Train][Ep-87/100]  [ 300/1435]  eta: 0:21:58  lr: 0.0000 (0.0000)  loss: 1019.9345 (1052.9179)  grad_norm: 695.4122 (691.2045)  amp_scale: 1.0000 (1.0000)  time: 1.1638  data: 0.0834  max mem: 13835
[13:32:50.712734] [Train][Ep-87/100]  [ 400/1435]  eta: 0:19:52  lr: 0.0000 (0.0000)  loss: 1066.8832 (1051.5635)  grad_norm: 691.5726 (693.9207)  amp_scale: 1.0000 (1.0000)  time: 1.2439  data: 0.3610  max mem: 13835
[13:34:52.255655] [Train][Ep-87/100]  [ 500/1435]  eta: 0:18:09  lr: 0.0000 (0.0000)  loss: 1046.0082 (1049.6693)  grad_norm: 676.7221 (691.5571)  amp_scale: 1.0000 (1.0000)  time: 1.1933  data: 0.2721  max mem: 13835
[13:36:53.189587] [Train][Ep-87/100]  [ 600/1435]  eta: 0:16:19  lr: 0.0000 (0.0000)  loss: 1022.1531 (1047.1437)  grad_norm: 682.3405 (691.2902)  amp_scale: 1.0000 (1.0000)  time: 1.1732  data: 0.0029  max mem: 13835
[13:38:51.374027] [Train][Ep-87/100]  [ 700/1435]  eta: 0:14:22  lr: 0.0000 (0.0000)  loss: 1013.0623 (1045.5176)  grad_norm: 687.7368 (690.5590)  amp_scale: 1.0000 (1.0000)  time: 1.1626  data: 0.0144  max mem: 13835
[13:40:49.514700] [Train][Ep-87/100]  [ 800/1435]  eta: 0:12:25  lr: 0.0000 (0.0000)  loss: 1061.6868 (1044.4630)  grad_norm: 678.5020 (689.2275)  amp_scale: 1.0000 (1.0000)  time: 1.1405  data: 0.1074  max mem: 13835
[13:42:53.775941] [Train][Ep-87/100]  [ 900/1435]  eta: 0:10:32  lr: 0.0000 (0.0000)  loss: 1051.0430 (1044.6077)  grad_norm: 683.9012 (688.8209)  amp_scale: 1.0000 (1.0000)  time: 1.2574  data: 0.0096  max mem: 13835
[13:44:49.008130] [Train][Ep-87/100]  [1000/1435]  eta: 0:08:32  lr: 0.0000 (0.0000)  loss: 992.8718 (1041.3005)  grad_norm: 658.2999 (686.4424)  amp_scale: 1.0000 (1.0000)  time: 1.1001  data: 0.3655  max mem: 13835
[13:46:43.534570] [Train][Ep-87/100]  [1100/1435]  eta: 0:06:34  lr: 0.0000 (0.0000)  loss: 1029.7373 (1040.4822)  grad_norm: 693.2322 (687.0006)  amp_scale: 1.0000 (1.0000)  time: 1.1784  data: 0.1604  max mem: 13835
[13:48:39.372914] [Train][Ep-87/100]  [1200/1435]  eta: 0:04:36  lr: 0.0000 (0.0000)  loss: 1047.3623 (1041.3496)  grad_norm: 708.3586 (688.3349)  amp_scale: 1.0000 (1.0000)  time: 1.1744  data: 0.3651  max mem: 13835
[13:50:41.208129] [Train][Ep-87/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1045.3641 (1041.7633)  grad_norm: 685.2417 (688.7363)  amp_scale: 1.0000 (1.0000)  time: 1.1902  data: 0.3189  max mem: 13835
[13:52:41.405513] [Train][Ep-87/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1005.4277 (1040.7844)  grad_norm: 680.9670 (688.4327)  amp_scale: 1.0000 (1.0000)  time: 1.1572  data: 0.4810  max mem: 13835
[13:53:17.214469] [Train][Ep-87/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1005.4277 (1040.3656)  grad_norm: 671.6999 (688.2159)  amp_scale: 1.0000 (1.0000)  time: 1.0576  data: 0.3392  max mem: 13835
[13:53:17.215425] [Train][Ep-87/100] Total time: 0:28:08 (1.1768 s / it)
[13:53:17.215933] Syncing meters...
[13:53:17.830178] Averaged stats: lr: 0.0000 (0.0000)  loss: 1005.4277 (1041.9730)  grad_norm: 671.6999 (688.2159)  amp_scale: 1.0000 (1.0000)
[13:53:26.494113] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 88)
[13:53:28.300679] [Train][Ep-88/100]  [   0/1435]  eta: 0:43:00  lr: 0.0000 (0.0000)  time: 1.7986  data: 1.3148  max mem: 13835
[13:55:21.833079] [Train][Ep-88/100]  [ 100/1435]  eta: 0:25:24  lr: 0.0000 (0.0000)  loss: 1044.3242 (1036.6810)  grad_norm: 676.9260 (680.0479)  amp_scale: 1.0000 (1.0000)  time: 1.1088  data: 0.3289  max mem: 13835
[13:57:17.102916] [Train][Ep-88/100]  [ 200/1435]  eta: 0:23:36  lr: 0.0000 (0.0000)  loss: 1022.8857 (1036.7455)  grad_norm: 681.8529 (686.7960)  amp_scale: 1.0000 (1.0000)  time: 1.1747  data: 0.2507  max mem: 13835
[13:59:13.922628] [Train][Ep-88/100]  [ 300/1435]  eta: 0:21:49  lr: 0.0000 (0.0000)  loss: 1053.2175 (1049.6859)  grad_norm: 694.3609 (687.9269)  amp_scale: 1.0000 (1.0000)  time: 1.1375  data: 0.2672  max mem: 13835
[14:01:08.993516] [Train][Ep-88/100]  [ 400/1435]  eta: 0:19:53  lr: 0.0000 (0.0000)  loss: 1044.6399 (1051.2444)  grad_norm: 676.0596 (686.4988)  amp_scale: 1.0000 (1.0000)  time: 1.1017  data: 0.3260  max mem: 13835
[14:02:59.694876] [Train][Ep-88/100]  [ 500/1435]  eta: 0:17:49  lr: 0.0000 (0.0000)  loss: 1030.4739 (1048.8521)  grad_norm: 672.4248 (683.9258)  amp_scale: 1.0000 (1.0000)  time: 1.0994  data: 0.4879  max mem: 13835
[14:04:55.053524] [Train][Ep-88/100]  [ 600/1435]  eta: 0:15:56  lr: 0.0000 (0.0000)  loss: 1088.6609 (1055.3716)  grad_norm: 676.6422 (683.3897)  amp_scale: 1.0000 (1.0000)  time: 1.0998  data: 0.3570  max mem: 13835
[14:06:50.567586] [Train][Ep-88/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0000 (0.0000)  loss: 1034.5046 (1053.5251)  grad_norm: 684.5194 (685.2840)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.2223  max mem: 13835
[14:08:47.540134] [Train][Ep-88/100]  [ 800/1435]  eta: 0:12:10  lr: 0.0000 (0.0000)  loss: 1070.9097 (1055.0514)  grad_norm: 693.6207 (685.6958)  amp_scale: 1.0000 (1.0000)  time: 1.1767  data: 0.2750  max mem: 13835
[14:10:43.777901] [Train][Ep-88/100]  [ 900/1435]  eta: 0:10:15  lr: 0.0000 (0.0000)  loss: 1022.0554 (1054.1829)  grad_norm: 693.2846 (687.4980)  amp_scale: 1.0000 (1.0000)  time: 1.1644  data: 0.1769  max mem: 13835
[14:12:39.024789] [Train][Ep-88/100]  [1000/1435]  eta: 0:08:20  lr: 0.0000 (0.0000)  loss: 1029.0023 (1050.9074)  grad_norm: 697.9297 (689.2650)  amp_scale: 1.0000 (1.0000)  time: 1.1633  data: 0.0865  max mem: 13835
[14:14:34.060265] [Train][Ep-88/100]  [1100/1435]  eta: 0:06:25  lr: 0.0000 (0.0000)  loss: 1039.5924 (1051.2526)  grad_norm: 682.3528 (688.6286)  amp_scale: 1.0000 (1.0000)  time: 1.1437  data: 0.3978  max mem: 13835
[14:16:22.910432] [Train][Ep-88/100]  [1200/1435]  eta: 0:04:29  lr: 0.0000 (0.0000)  loss: 1034.0973 (1049.9162)  grad_norm: 689.0880 (688.7684)  amp_scale: 1.0000 (1.0000)  time: 1.0936  data: 0.5192  max mem: 13835
[14:18:14.987920] [Train][Ep-88/100]  [1300/1435]  eta: 0:02:34  lr: 0.0000 (0.0000)  loss: 1043.1112 (1049.3883)  grad_norm: 684.8789 (688.7174)  amp_scale: 1.0000 (1.0000)  time: 1.0602  data: 0.2970  max mem: 13835
[14:20:12.155278] [Train][Ep-88/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1011.4458 (1046.9837)  grad_norm: 676.8829 (689.5012)  amp_scale: 1.0000 (1.0000)  time: 1.1890  data: 0.0030  max mem: 13835
[14:20:51.774660] [Train][Ep-88/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1041.1111 (1047.0984)  grad_norm: 675.2147 (689.5868)  amp_scale: 1.0000 (1.0000)  time: 1.1996  data: 0.0244  max mem: 13835
[14:20:51.775334] [Train][Ep-88/100] Total time: 0:27:25 (1.1465 s / it)
[14:20:51.775739] Syncing meters...
[14:20:52.076403] Averaged stats: lr: 0.0000 (0.0000)  loss: 1041.1111 (1044.3352)  grad_norm: 675.2147 (689.5868)  amp_scale: 1.0000 (1.0000)
[14:21:00.606048] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 89)
[14:21:02.465104] [Train][Ep-89/100]  [   0/1435]  eta: 0:44:14  lr: 0.0000 (0.0000)  time: 1.8499  data: 1.3666  max mem: 13835
[14:22:57.874525] [Train][Ep-89/100]  [ 100/1435]  eta: 0:25:49  lr: 0.0000 (0.0000)  loss: 1044.3843 (1058.3486)  grad_norm: 672.8142 (685.2027)  amp_scale: 1.0000 (1.0000)  time: 1.2205  data: 0.6435  max mem: 13835
[14:24:52.683437] [Train][Ep-89/100]  [ 200/1435]  eta: 0:23:45  lr: 0.0000 (0.0000)  loss: 1070.2810 (1062.6409)  grad_norm: 671.2813 (679.6532)  amp_scale: 1.0000 (1.0000)  time: 1.0992  data: 0.5270  max mem: 13835
[14:26:54.107027] [Train][Ep-89/100]  [ 300/1435]  eta: 0:22:12  lr: 0.0000 (0.0000)  loss: 1022.8433 (1051.5715)  grad_norm: 680.2551 (679.7684)  amp_scale: 1.0000 (1.0000)  time: 1.1297  data: 0.0596  max mem: 13835
[14:28:51.428854] [Train][Ep-89/100]  [ 400/1435]  eta: 0:20:15  lr: 0.0000 (0.0000)  loss: 1011.4242 (1045.4508)  grad_norm: 683.8036 (679.1342)  amp_scale: 1.0000 (1.0000)  time: 1.1615  data: 0.5924  max mem: 13835
[14:30:48.205678] [Train][Ep-89/100]  [ 500/1435]  eta: 0:18:16  lr: 0.0000 (0.0000)  loss: 987.1630 (1039.3826)  grad_norm: 676.5896 (679.0829)  amp_scale: 1.0000 (1.0000)  time: 1.2255  data: 0.4918  max mem: 13835
[14:32:49.114041] [Train][Ep-89/100]  [ 600/1435]  eta: 0:16:24  lr: 0.0000 (0.0000)  loss: 1071.7961 (1045.2787)  grad_norm: 676.8076 (681.5640)  amp_scale: 1.0000 (1.0000)  time: 1.2730  data: 0.0264  max mem: 13835
[14:34:48.434647] [Train][Ep-89/100]  [ 700/1435]  eta: 0:14:27  lr: 0.0000 (0.0000)  loss: 1041.2651 (1046.3171)  grad_norm: 690.9594 (682.7321)  amp_scale: 1.0000 (1.0000)  time: 1.1381  data: 0.0336  max mem: 13835
[14:36:45.942115] [Train][Ep-89/100]  [ 800/1435]  eta: 0:12:29  lr: 0.0000 (0.0000)  loss: 1044.6366 (1045.9794)  grad_norm: 693.5831 (684.4638)  amp_scale: 1.0000 (1.0000)  time: 1.1929  data: 0.1833  max mem: 13835
[14:38:49.701347] [Train][Ep-89/100]  [ 900/1435]  eta: 0:10:34  lr: 0.0000 (0.0000)  loss: 1059.5424 (1046.7903)  grad_norm: 667.5660 (683.7922)  amp_scale: 1.0000 (1.0000)  time: 1.2140  data: 0.0031  max mem: 13835
[14:40:46.058167] [Train][Ep-89/100]  [1000/1435]  eta: 0:08:35  lr: 0.0000 (0.0000)  loss: 1034.2963 (1047.2273)  grad_norm: 678.5451 (683.4848)  amp_scale: 1.0000 (1.0000)  time: 1.1741  data: 0.1355  max mem: 13835
[14:42:43.542458] [Train][Ep-89/100]  [1100/1435]  eta: 0:06:36  lr: 0.0000 (0.0000)  loss: 1049.7594 (1047.1071)  grad_norm: 682.7642 (683.8149)  amp_scale: 1.0000 (1.0000)  time: 1.1871  data: 0.2188  max mem: 13835
[14:44:41.856363] [Train][Ep-89/100]  [1200/1435]  eta: 0:04:38  lr: 0.0000 (0.0000)  loss: 1046.5824 (1048.2740)  grad_norm: 690.8344 (684.9089)  amp_scale: 1.0000 (1.0000)  time: 1.1832  data: 0.3135  max mem: 13835
[14:46:34.395262] [Train][Ep-89/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1077.1398 (1049.2909)  grad_norm: 675.8359 (684.6187)  amp_scale: 1.0000 (1.0000)  time: 1.0951  data: 0.3631  max mem: 13835
[14:48:31.946608] [Train][Ep-89/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1008.1614 (1047.5969)  grad_norm: 678.1177 (684.6396)  amp_scale: 1.0000 (1.0000)  time: 1.1550  data: 0.5475  max mem: 13835
[14:49:08.763555] [Train][Ep-89/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1025.2970 (1048.6576)  grad_norm: 682.7498 (684.8238)  amp_scale: 1.0000 (1.0000)  time: 1.0815  data: 0.3709  max mem: 13835
[14:49:08.764409] [Train][Ep-89/100] Total time: 0:28:08 (1.1764 s / it)
[14:49:08.764936] Syncing meters...
[14:49:09.417221] Averaged stats: lr: 0.0000 (0.0000)  loss: 1025.2970 (1043.1638)  grad_norm: 682.7498 (684.8238)  amp_scale: 1.0000 (1.0000)
[14:49:19.345789] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 90)
[14:49:21.430949] [Train][Ep-90/100]  [   0/1435]  eta: 0:49:39  lr: 0.0000 (0.0000)  time: 2.0764  data: 1.5931  max mem: 13835
[14:51:19.186360] [Train][Ep-90/100]  [ 100/1435]  eta: 0:26:23  lr: 0.0000 (0.0000)  loss: 1018.0597 (1046.9417)  grad_norm: 690.2184 (695.8047)  amp_scale: 1.0000 (1.0000)  time: 1.1763  data: 0.6083  max mem: 13835
[14:53:14.834658] [Train][Ep-90/100]  [ 200/1435]  eta: 0:24:06  lr: 0.0000 (0.0000)  loss: 1054.3846 (1047.8973)  grad_norm: 689.5854 (693.5289)  amp_scale: 1.0000 (1.0000)  time: 1.1077  data: 0.4109  max mem: 13835
[14:55:12.429384] [Train][Ep-90/100]  [ 300/1435]  eta: 0:22:11  lr: 0.0000 (0.0000)  loss: 1009.0563 (1040.8785)  grad_norm: 685.6714 (688.0569)  amp_scale: 1.0000 (1.0000)  time: 1.2086  data: 0.0016  max mem: 13835
[14:57:11.073457] [Train][Ep-90/100]  [ 400/1435]  eta: 0:20:17  lr: 0.0000 (0.0000)  loss: 1074.0533 (1043.5307)  grad_norm: 689.9850 (686.8446)  amp_scale: 1.0000 (1.0000)  time: 1.2123  data: 0.0743  max mem: 13835
[14:59:04.973388] [Train][Ep-90/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0000 (0.0000)  loss: 1043.2811 (1042.9948)  grad_norm: 675.5085 (686.9049)  amp_scale: 1.0000 (1.0000)  time: 1.1711  data: 0.4007  max mem: 13835
[15:00:57.584380] [Train][Ep-90/100]  [ 600/1435]  eta: 0:16:10  lr: 0.0000 (0.0000)  loss: 1056.8767 (1047.5596)  grad_norm: 673.9120 (686.8009)  amp_scale: 1.0000 (1.0000)  time: 1.1155  data: 0.4369  max mem: 13835
[15:02:56.050756] [Train][Ep-90/100]  [ 700/1435]  eta: 0:14:16  lr: 0.0000 (0.0000)  loss: 1041.8849 (1048.1090)  grad_norm: 677.7430 (686.3642)  amp_scale: 1.0000 (1.0000)  time: 1.2669  data: 0.0204  max mem: 13835
[15:04:52.390388] [Train][Ep-90/100]  [ 800/1435]  eta: 0:12:19  lr: 0.0000 (0.0000)  loss: 1068.3574 (1048.0691)  grad_norm: 687.3744 (687.2542)  amp_scale: 1.0000 (1.0000)  time: 1.1544  data: 0.4618  max mem: 13835
[15:06:50.073794] [Train][Ep-90/100]  [ 900/1435]  eta: 0:10:23  lr: 0.0000 (0.0000)  loss: 1059.0930 (1047.6384)  grad_norm: 694.8636 (688.1581)  amp_scale: 1.0000 (1.0000)  time: 1.1462  data: 0.2481  max mem: 13835
[15:08:47.877868] [Train][Ep-90/100]  [1000/1435]  eta: 0:08:27  lr: 0.0000 (0.0000)  loss: 1028.6443 (1047.5221)  grad_norm: 675.7928 (688.6104)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.6073  max mem: 13835
[15:10:44.408712] [Train][Ep-90/100]  [1100/1435]  eta: 0:06:30  lr: 0.0000 (0.0000)  loss: 1028.0148 (1046.2631)  grad_norm: 687.2970 (689.1628)  amp_scale: 1.0000 (1.0000)  time: 1.1322  data: 0.5648  max mem: 13835
[15:12:40.172357] [Train][Ep-90/100]  [1200/1435]  eta: 0:04:34  lr: 0.0000 (0.0000)  loss: 988.2571 (1044.5620)  grad_norm: 696.7550 (690.2583)  amp_scale: 1.0000 (1.0000)  time: 1.1287  data: 0.5574  max mem: 13835
[15:14:38.236403] [Train][Ep-90/100]  [1300/1435]  eta: 0:02:37  lr: 0.0000 (0.0000)  loss: 1037.1760 (1043.9288)  grad_norm: 687.2938 (690.0098)  amp_scale: 1.0000 (1.0000)  time: 1.1584  data: 0.5849  max mem: 13835
[15:16:32.558681] [Train][Ep-90/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 999.5195 (1042.6853)  grad_norm: 668.1635 (689.2106)  amp_scale: 1.0000 (1.0000)  time: 1.1771  data: 0.0675  max mem: 13835
[15:17:10.975838] [Train][Ep-90/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1006.8777 (1042.4880)  grad_norm: 667.1482 (688.7867)  amp_scale: 1.0000 (1.0000)  time: 1.1637  data: 0.0006  max mem: 13835
[15:17:10.976699] [Train][Ep-90/100] Total time: 0:27:51 (1.1649 s / it)
[15:17:10.977204] Syncing meters...
[15:17:11.839917] Averaged stats: lr: 0.0000 (0.0000)  loss: 1006.8777 (1037.6939)  grad_norm: 667.1482 (688.7867)  amp_scale: 1.0000 (1.0000)
[15:17:13.897455] [Eval][Ep-90/100]  [  0/121]  eta: 0:04:07  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0493  data: 1.8888  max mem: 13835
[15:18:57.666784] [Eval][Ep-90/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9204  data: 0.7608  max mem: 13835
[15:19:15.618778] [Eval][Ep-90/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8973  data: 0.7410  max mem: 13835
[15:19:15.619636] [Eval][Ep-90/100] Total time: 0:02:03 (1.0229 s / it)
[15:19:16.602522] [Eval][Ep-90/100] val_acc1_image=27.05 | val_acc1_audio=42.39 | val_acc1_fusion=38.96 | val_acc1_all=52.76
[15:19:25.243199] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 91)
[15:19:27.026348] [Train][Ep-91/100]  [   0/1435]  eta: 0:42:27  lr: 0.0000 (0.0000)  time: 1.7749  data: 1.2968  max mem: 13835
[15:21:24.737922] [Train][Ep-91/100]  [ 100/1435]  eta: 0:26:19  lr: 0.0000 (0.0000)  loss: 1058.3104 (1060.0267)  grad_norm: 699.7507 (693.0439)  amp_scale: 1.0000 (1.0000)  time: 1.1350  data: 0.5645  max mem: 13835
[15:23:26.600955] [Train][Ep-91/100]  [ 200/1435]  eta: 0:24:42  lr: 0.0000 (0.0000)  loss: 1021.3115 (1052.0068)  grad_norm: 681.0566 (689.5035)  amp_scale: 1.0000 (1.0000)  time: 1.2333  data: 0.6626  max mem: 13835
[15:25:23.491809] [Train][Ep-91/100]  [ 300/1435]  eta: 0:22:30  lr: 0.0000 (0.0000)  loss: 1019.3780 (1041.0345)  grad_norm: 693.1761 (689.1814)  amp_scale: 1.0000 (1.0000)  time: 1.1717  data: 0.0004  max mem: 13835
[15:27:22.446396] [Train][Ep-91/100]  [ 400/1435]  eta: 0:20:31  lr: 0.0000 (0.0000)  loss: 1040.8284 (1038.8478)  grad_norm: 661.2055 (686.3625)  amp_scale: 1.0000 (1.0000)  time: 1.1569  data: 0.3322  max mem: 13835
[15:29:23.325293] [Train][Ep-91/100]  [ 500/1435]  eta: 0:18:36  lr: 0.0000 (0.0000)  loss: 1032.9207 (1038.6778)  grad_norm: 682.3907 (688.5326)  amp_scale: 1.0000 (1.0000)  time: 1.2623  data: 0.6918  max mem: 13835
[15:31:20.353300] [Train][Ep-91/100]  [ 600/1435]  eta: 0:16:33  lr: 0.0000 (0.0000)  loss: 1042.1659 (1040.3096)  grad_norm: 702.3462 (690.3302)  amp_scale: 1.0000 (1.0000)  time: 1.1208  data: 0.5530  max mem: 13835
[15:33:20.860335] [Train][Ep-91/100]  [ 700/1435]  eta: 0:14:35  lr: 0.0000 (0.0000)  loss: 1027.4221 (1039.4649)  grad_norm: 693.5027 (689.7125)  amp_scale: 1.0000 (1.0000)  time: 1.1199  data: 0.5512  max mem: 13835
[15:35:18.632615] [Train][Ep-91/100]  [ 800/1435]  eta: 0:12:35  lr: 0.0000 (0.0000)  loss: 1020.1177 (1038.2862)  grad_norm: 656.9213 (687.2891)  amp_scale: 1.0000 (1.0000)  time: 1.2105  data: 0.6391  max mem: 13835
[15:37:13.602018] [Train][Ep-91/100]  [ 900/1435]  eta: 0:10:34  lr: 0.0000 (0.0000)  loss: 1018.9918 (1038.2151)  grad_norm: 689.8151 (687.7284)  amp_scale: 1.0000 (1.0000)  time: 1.2133  data: 0.1572  max mem: 13835
[15:39:11.152342] [Train][Ep-91/100]  [1000/1435]  eta: 0:08:35  lr: 0.0000 (0.0000)  loss: 1016.1370 (1037.8871)  grad_norm: 687.3805 (688.1759)  amp_scale: 1.0000 (1.0000)  time: 1.1224  data: 0.5209  max mem: 13835
[15:41:09.600561] [Train][Ep-91/100]  [1100/1435]  eta: 0:06:36  lr: 0.0000 (0.0000)  loss: 1046.8663 (1038.7738)  grad_norm: 671.2575 (687.1574)  amp_scale: 1.0000 (1.0000)  time: 1.1742  data: 0.0003  max mem: 13835
[15:43:08.041255] [Train][Ep-91/100]  [1200/1435]  eta: 0:04:38  lr: 0.0000 (0.0000)  loss: 1029.7483 (1038.6521)  grad_norm: 681.3149 (686.6932)  amp_scale: 1.0000 (1.0000)  time: 1.0964  data: 0.2561  max mem: 13835
[15:45:03.009251] [Train][Ep-91/100]  [1300/1435]  eta: 0:02:39  lr: 0.0000 (0.0000)  loss: 1030.3435 (1037.7177)  grad_norm: 704.2490 (687.5130)  amp_scale: 1.0000 (1.0000)  time: 1.2016  data: 0.2767  max mem: 13835
[15:46:57.302832] [Train][Ep-91/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1015.4683 (1036.4219)  grad_norm: 677.1710 (687.4269)  amp_scale: 1.0000 (1.0000)  time: 1.1815  data: 0.0014  max mem: 13835
[15:47:36.060065] [Train][Ep-91/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1003.2601 (1036.3582)  grad_norm: 680.5489 (687.3858)  amp_scale: 1.0000 (1.0000)  time: 1.1617  data: 0.0032  max mem: 13835
[15:47:36.061097] [Train][Ep-91/100] Total time: 0:28:10 (1.1783 s / it)
[15:47:36.061642] Syncing meters...
[15:47:37.180804] Averaged stats: lr: 0.0000 (0.0000)  loss: 1003.2601 (1038.4661)  grad_norm: 680.5489 (687.3858)  amp_scale: 1.0000 (1.0000)
[15:47:47.306091] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 92)
[15:47:49.170410] [Train][Ep-92/100]  [   0/1435]  eta: 0:44:23  lr: 0.0000 (0.0000)  time: 1.8560  data: 1.3721  max mem: 13835
[15:49:50.524733] [Train][Ep-92/100]  [ 100/1435]  eta: 0:27:08  lr: 0.0000 (0.0000)  loss: 1057.2474 (1066.6517)  grad_norm: 683.0812 (678.5123)  amp_scale: 1.0000 (1.0000)  time: 1.2444  data: 0.6671  max mem: 13835
[15:51:48.454718] [Train][Ep-92/100]  [ 200/1435]  eta: 0:24:41  lr: 0.0000 (0.0000)  loss: 1010.6580 (1045.1887)  grad_norm: 677.3713 (680.9049)  amp_scale: 1.0000 (1.0000)  time: 1.1822  data: 0.4129  max mem: 13835
[15:53:42.207712] [Train][Ep-92/100]  [ 300/1435]  eta: 0:22:18  lr: 0.0000 (0.0000)  loss: 1019.6140 (1047.2678)  grad_norm: 675.4378 (683.5288)  amp_scale: 1.0000 (1.0000)  time: 1.1414  data: 0.1644  max mem: 13835
[15:55:42.766548] [Train][Ep-92/100]  [ 400/1435]  eta: 0:20:27  lr: 0.0000 (0.0000)  loss: 1054.7814 (1050.6206)  grad_norm: 679.1863 (684.6030)  amp_scale: 1.0000 (1.0000)  time: 1.2008  data: 0.0711  max mem: 13835
[15:57:44.154703] [Train][Ep-92/100]  [ 500/1435]  eta: 0:18:33  lr: 0.0000 (0.0000)  loss: 1010.6891 (1051.2749)  grad_norm: 691.5418 (684.8245)  amp_scale: 1.0000 (1.0000)  time: 1.1976  data: 0.0942  max mem: 13835
[15:59:45.882406] [Train][Ep-92/100]  [ 600/1435]  eta: 0:16:38  lr: 0.0000 (0.0000)  loss: 1070.5314 (1053.5464)  grad_norm: 684.3048 (687.2246)  amp_scale: 1.0000 (1.0000)  time: 1.2385  data: 0.0083  max mem: 13835
[16:01:45.519780] [Train][Ep-92/100]  [ 700/1435]  eta: 0:14:38  lr: 0.0000 (0.0000)  loss: 1052.9805 (1051.7282)  grad_norm: 688.4907 (685.7864)  amp_scale: 1.0000 (1.0000)  time: 1.1601  data: 0.4002  max mem: 13835
[16:03:39.386384] [Train][Ep-92/100]  [ 800/1435]  eta: 0:12:34  lr: 0.0000 (0.0000)  loss: 1016.1696 (1048.3396)  grad_norm: 674.8895 (685.3338)  amp_scale: 1.0000 (1.0000)  time: 1.0901  data: 0.2543  max mem: 13835
[16:05:40.140157] [Train][Ep-92/100]  [ 900/1435]  eta: 0:10:36  lr: 0.0000 (0.0000)  loss: 1014.9756 (1045.7902)  grad_norm: 670.2061 (684.0826)  amp_scale: 1.0000 (1.0000)  time: 1.1897  data: 0.3745  max mem: 13835
[16:07:41.625595] [Train][Ep-92/100]  [1000/1435]  eta: 0:08:38  lr: 0.0000 (0.0000)  loss: 1075.2240 (1047.4346)  grad_norm: 685.4911 (685.5615)  amp_scale: 1.0000 (1.0000)  time: 1.2308  data: 0.1851  max mem: 13835
[16:09:40.104963] [Train][Ep-92/100]  [1100/1435]  eta: 0:06:39  lr: 0.0000 (0.0000)  loss: 1006.4642 (1045.2126)  grad_norm: 668.0442 (684.2597)  amp_scale: 1.0000 (1.0000)  time: 1.1882  data: 0.4305  max mem: 13835
[16:11:39.801074] [Train][Ep-92/100]  [1200/1435]  eta: 0:04:40  lr: 0.0000 (0.0000)  loss: 1047.8424 (1044.0918)  grad_norm: 667.2503 (683.1438)  amp_scale: 1.0000 (1.0000)  time: 1.3017  data: 0.1237  max mem: 13835
[16:13:37.735427] [Train][Ep-92/100]  [1300/1435]  eta: 0:02:40  lr: 0.0000 (0.0000)  loss: 1040.0382 (1043.6111)  grad_norm: 648.7163 (682.2448)  amp_scale: 1.0000 (1.0000)  time: 1.2739  data: 0.0090  max mem: 13835
[16:15:42.338688] [Train][Ep-92/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1017.2565 (1042.5260)  grad_norm: 674.6912 (682.6439)  amp_scale: 1.0000 (1.0000)  time: 1.2663  data: 0.0676  max mem: 13835
[16:16:21.493328] [Train][Ep-92/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1060.3232 (1043.0004)  grad_norm: 676.5320 (682.8493)  amp_scale: 1.0000 (1.0000)  time: 1.2035  data: 0.0485  max mem: 13835
[16:16:21.494291] [Train][Ep-92/100] Total time: 0:28:34 (1.1946 s / it)
[16:16:21.494792] Syncing meters...
[16:16:21.525473] Averaged stats: lr: 0.0000 (0.0000)  loss: 1060.3232 (1039.3157)  grad_norm: 676.5320 (682.8493)  amp_scale: 1.0000 (1.0000)
[16:16:29.945568] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 93)
[16:16:32.262487] [Train][Ep-93/100]  [   0/1435]  eta: 0:55:13  lr: 0.0000 (0.0000)  time: 2.3089  data: 1.8247  max mem: 13835
[16:18:27.987525] [Train][Ep-93/100]  [ 100/1435]  eta: 0:26:00  lr: 0.0000 (0.0000)  loss: 1028.7723 (1059.6512)  grad_norm: 693.0610 (695.2351)  amp_scale: 1.0000 (1.0000)  time: 1.2064  data: 0.6397  max mem: 13835
[16:20:22.420916] [Train][Ep-93/100]  [ 200/1435]  eta: 0:23:48  lr: 0.0000 (0.0000)  loss: 1043.2965 (1052.4677)  grad_norm: 692.4954 (691.3307)  amp_scale: 1.0000 (1.0000)  time: 1.1150  data: 0.5467  max mem: 13835
[16:22:18.461369] [Train][Ep-93/100]  [ 300/1435]  eta: 0:21:54  lr: 0.0000 (0.0000)  loss: 1031.5286 (1042.5353)  grad_norm: 683.2239 (689.8266)  amp_scale: 1.0000 (1.0000)  time: 1.1838  data: 0.2078  max mem: 13835
[16:24:15.825117] [Train][Ep-93/100]  [ 400/1435]  eta: 0:20:02  lr: 0.0000 (0.0000)  loss: 1067.7140 (1050.2239)  grad_norm: 673.2133 (687.1666)  amp_scale: 1.0000 (1.0000)  time: 1.1674  data: 0.2085  max mem: 13835
[16:26:06.282680] [Train][Ep-93/100]  [ 500/1435]  eta: 0:17:55  lr: 0.0000 (0.0000)  loss: 1002.6200 (1042.5280)  grad_norm: 684.6287 (689.0856)  amp_scale: 1.0000 (1.0000)  time: 1.0926  data: 0.2805  max mem: 13835
[16:27:59.819079] [Train][Ep-93/100]  [ 600/1435]  eta: 0:15:58  lr: 0.0000 (0.0000)  loss: 1036.4144 (1039.9794)  grad_norm: 687.0436 (688.5356)  amp_scale: 1.0000 (1.0000)  time: 1.1803  data: 0.0975  max mem: 13835
[16:29:54.471297] [Train][Ep-93/100]  [ 700/1435]  eta: 0:14:03  lr: 0.0000 (0.0000)  loss: 1024.3550 (1038.1429)  grad_norm: 692.0770 (688.6133)  amp_scale: 1.0000 (1.0000)  time: 1.1492  data: 0.0012  max mem: 13835
[16:31:47.931326] [Train][Ep-93/100]  [ 800/1435]  eta: 0:12:07  lr: 0.0000 (0.0000)  loss: 1019.9468 (1038.3308)  grad_norm: 685.5335 (688.6297)  amp_scale: 1.0000 (1.0000)  time: 1.1123  data: 0.3898  max mem: 13835
[16:33:41.568133] [Train][Ep-93/100]  [ 900/1435]  eta: 0:10:12  lr: 0.0000 (0.0000)  loss: 1029.0623 (1037.7890)  grad_norm: 675.2800 (688.3026)  amp_scale: 1.0000 (1.0000)  time: 1.1413  data: 0.0045  max mem: 13835
[16:35:37.510918] [Train][Ep-93/100]  [1000/1435]  eta: 0:08:18  lr: 0.0000 (0.0000)  loss: 1023.0925 (1036.6743)  grad_norm: 681.0762 (687.4295)  amp_scale: 1.0000 (1.0000)  time: 1.1296  data: 0.0005  max mem: 13835
[16:37:33.256533] [Train][Ep-93/100]  [1100/1435]  eta: 0:06:24  lr: 0.0000 (0.0000)  loss: 998.5464 (1036.6411)  grad_norm: 683.3638 (687.2253)  amp_scale: 1.0000 (1.0000)  time: 1.2039  data: 0.0061  max mem: 13835
[16:39:32.807371] [Train][Ep-93/100]  [1200/1435]  eta: 0:04:30  lr: 0.0000 (0.0000)  loss: 1017.3079 (1036.8920)  grad_norm: 680.1188 (687.2577)  amp_scale: 1.0000 (1.0000)  time: 1.2072  data: 0.0781  max mem: 13835
[16:41:30.638579] [Train][Ep-93/100]  [1300/1435]  eta: 0:02:35  lr: 0.0000 (0.0000)  loss: 1039.8811 (1037.0755)  grad_norm: 671.2246 (687.9321)  amp_scale: 1.0000 (1.0000)  time: 1.2132  data: 0.0485  max mem: 13835
[16:43:28.143159] [Train][Ep-93/100]  [1400/1435]  eta: 0:00:40  lr: 0.0000 (0.0000)  loss: 1039.6119 (1038.7855)  grad_norm: 683.9150 (688.1719)  amp_scale: 1.0000 (1.0000)  time: 1.1692  data: 0.1047  max mem: 13835
[16:44:08.111841] [Train][Ep-93/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1039.6119 (1038.9768)  grad_norm: 665.5785 (687.6130)  amp_scale: 1.0000 (1.0000)  time: 1.2549  data: 0.0413  max mem: 13835
[16:44:08.112705] [Train][Ep-93/100] Total time: 0:27:38 (1.1555 s / it)
[16:44:08.113144] Syncing meters...
[16:44:08.924209] Averaged stats: lr: 0.0000 (0.0000)  loss: 1039.6119 (1040.6151)  grad_norm: 665.5785 (687.6130)  amp_scale: 1.0000 (1.0000)
[16:44:19.093803] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 94)
[16:44:20.916597] [Train][Ep-94/100]  [   0/1435]  eta: 0:43:23  lr: 0.0000 (0.0000)  time: 1.8141  data: 1.3298  max mem: 13835
[16:46:21.890447] [Train][Ep-94/100]  [ 100/1435]  eta: 0:27:02  lr: 0.0000 (0.0000)  loss: 1002.2646 (1017.2102)  grad_norm: 670.2369 (677.9366)  amp_scale: 1.0000 (1.0000)  time: 1.2366  data: 0.6489  max mem: 13835
[16:48:20.365745] [Train][Ep-94/100]  [ 200/1435]  eta: 0:24:42  lr: 0.0000 (0.0000)  loss: 1008.0977 (1027.9484)  grad_norm: 675.3788 (682.3818)  amp_scale: 1.0000 (1.0000)  time: 1.1773  data: 0.6053  max mem: 13835
[16:50:15.999476] [Train][Ep-94/100]  [ 300/1435]  eta: 0:22:25  lr: 0.0000 (0.0000)  loss: 1014.4496 (1025.2106)  grad_norm: 676.0103 (686.9278)  amp_scale: 1.0000 (1.0000)  time: 1.1325  data: 0.3635  max mem: 13835
[16:52:11.929166] [Train][Ep-94/100]  [ 400/1435]  eta: 0:20:20  lr: 0.0000 (0.0000)  loss: 1065.4513 (1032.1317)  grad_norm: 685.8506 (686.8369)  amp_scale: 1.0000 (1.0000)  time: 1.1462  data: 0.0005  max mem: 13835
[16:54:04.409999] [Train][Ep-94/100]  [ 500/1435]  eta: 0:18:12  lr: 0.0000 (0.0000)  loss: 997.8099 (1033.6481)  grad_norm: 689.6369 (685.6572)  amp_scale: 1.0000 (1.0000)  time: 1.1432  data: 0.0629  max mem: 13835
[16:56:02.698703] [Train][Ep-94/100]  [ 600/1435]  eta: 0:16:17  lr: 0.0000 (0.0000)  loss: 1040.5801 (1033.3222)  grad_norm: 680.6720 (686.1425)  amp_scale: 1.0000 (1.0000)  time: 1.2227  data: 0.2743  max mem: 13835
[16:58:00.604886] [Train][Ep-94/100]  [ 700/1435]  eta: 0:14:21  lr: 0.0000 (0.0000)  loss: 1029.2286 (1033.4133)  grad_norm: 675.6702 (686.5044)  amp_scale: 1.0000 (1.0000)  time: 1.1919  data: 0.2458  max mem: 13835
[16:59:55.023101] [Train][Ep-94/100]  [ 800/1435]  eta: 0:12:21  lr: 0.0000 (0.0000)  loss: 1017.9693 (1033.3765)  grad_norm: 670.7971 (684.8105)  amp_scale: 1.0000 (1.0000)  time: 1.0774  data: 0.4861  max mem: 13835
[17:01:47.302378] [Train][Ep-94/100]  [ 900/1435]  eta: 0:10:22  lr: 0.0000 (0.0000)  loss: 1008.0033 (1031.9066)  grad_norm: 680.5552 (684.8359)  amp_scale: 1.0000 (1.0000)  time: 1.0933  data: 0.4776  max mem: 13835
[17:03:42.455856] [Train][Ep-94/100]  [1000/1435]  eta: 0:08:25  lr: 0.0000 (0.0000)  loss: 1047.7445 (1033.3282)  grad_norm: 715.3280 (686.6351)  amp_scale: 1.0000 (1.0000)  time: 1.2059  data: 0.2188  max mem: 13835
[17:05:36.919611] [Train][Ep-94/100]  [1100/1435]  eta: 0:06:28  lr: 0.0000 (0.0000)  loss: 1023.7452 (1033.9005)  grad_norm: 683.6233 (687.4306)  amp_scale: 1.0000 (1.0000)  time: 1.1587  data: 0.5298  max mem: 13835
[17:07:35.835346] [Train][Ep-94/100]  [1200/1435]  eta: 0:04:33  lr: 0.0000 (0.0000)  loss: 1010.1006 (1032.9840)  grad_norm: 676.2141 (687.0490)  amp_scale: 1.0000 (1.0000)  time: 1.1960  data: 0.1088  max mem: 13835
