
[17:15:19.183868] Start training for 100 epochs
/home/wiss/zverev/miniconda3/envs/efav/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[17:15:27.854103] [Train][Ep-94/100]  [   0/1435]  eta: 3:27:14  lr: 0.0000 (0.0000)  time: 8.6653  data: 7.6423  max mem: 12727
[17:17:54.036033] [Train][Ep-94/100]  [ 100/1435]  eta: 0:34:06  lr: 0.0000 (0.0000)  loss: 1028.7607 (1054.4064)  grad_norm: 681.0569 (686.5943)  amp_scale: 1.0000 (1.0000)  time: 1.4609  data: 0.8992  max mem: 13835
[17:20:09.821856] [Train][Ep-94/100]  [ 200/1435]  eta: 0:29:45  lr: 0.0000 (0.0000)  loss: 1049.4426 (1043.4536)  grad_norm: 663.4958 (679.1235)  amp_scale: 1.0000 (1.0000)  time: 1.3252  data: 0.7522  max mem: 13835
[17:22:23.845041] [Train][Ep-94/100]  [ 300/1435]  eta: 0:26:41  lr: 0.0000 (0.0000)  loss: 1037.7882 (1047.8117)  grad_norm: 689.3660 (685.1084)  amp_scale: 1.0000 (1.0000)  time: 1.3558  data: 0.6924  max mem: 13835
[17:24:40.480697] [Train][Ep-94/100]  [ 400/1435]  eta: 0:24:08  lr: 0.0000 (0.0000)  loss: 1017.7035 (1044.0492)  grad_norm: 672.9575 (684.4746)  amp_scale: 1.0000 (1.0000)  time: 1.3626  data: 0.7401  max mem: 13835
[17:26:53.997654] [Train][Ep-94/100]  [ 500/1435]  eta: 0:21:36  lr: 0.0000 (0.0000)  loss: 1013.9903 (1038.6376)  grad_norm: 679.4110 (684.7640)  amp_scale: 1.0000 (1.0000)  time: 1.3409  data: 0.7244  max mem: 13835
[17:29:04.276945] [Train][Ep-94/100]  [ 600/1435]  eta: 0:19:06  lr: 0.0000 (0.0000)  loss: 1053.2765 (1037.8641)  grad_norm: 676.3755 (683.0225)  amp_scale: 1.0000 (1.0000)  time: 1.3421  data: 0.7755  max mem: 13835
[17:31:12.774705] [Train][Ep-94/100]  [ 700/1435]  eta: 0:16:39  lr: 0.0000 (0.0000)  loss: 1020.3615 (1038.0825)  grad_norm: 675.7573 (682.2139)  amp_scale: 1.0000 (1.0000)  time: 1.3128  data: 0.2851  max mem: 13835
[17:33:25.951828] [Train][Ep-94/100]  [ 800/1435]  eta: 0:14:21  lr: 0.0000 (0.0000)  loss: 1024.1042 (1039.6793)  grad_norm: 670.0941 (683.2114)  amp_scale: 1.0000 (1.0000)  time: 1.3411  data: 0.7756  max mem: 13835
[17:35:36.963566] [Train][Ep-94/100]  [ 900/1435]  eta: 0:12:03  lr: 0.0000 (0.0000)  loss: 1028.9525 (1039.5040)  grad_norm: 673.8909 (683.4460)  amp_scale: 1.0000 (1.0000)  time: 1.3139  data: 0.7485  max mem: 13835
[17:37:42.987093] [Train][Ep-94/100]  [1000/1435]  eta: 0:09:43  lr: 0.0000 (0.0000)  loss: 1007.3251 (1038.3564)  grad_norm: 686.1594 (683.3431)  amp_scale: 1.0000 (1.0000)  time: 1.3104  data: 0.2118  max mem: 13835
[17:39:54.709505] [Train][Ep-94/100]  [1100/1435]  eta: 0:07:28  lr: 0.0000 (0.0000)  loss: 1022.4802 (1037.1532)  grad_norm: 674.0745 (683.0396)  amp_scale: 1.0000 (1.0000)  time: 1.3736  data: 0.0248  max mem: 13835
[17:42:05.307090] [Train][Ep-94/100]  [1200/1435]  eta: 0:05:14  lr: 0.0000 (0.0000)  loss: 1024.8801 (1036.3813)  grad_norm: 680.4291 (683.9666)  amp_scale: 1.0000 (1.0000)  time: 1.2525  data: 0.4414  max mem: 13835
[17:44:23.054300] [Train][Ep-94/100]  [1300/1435]  eta: 0:03:00  lr: 0.0000 (0.0000)  loss: 1012.8833 (1036.6298)  grad_norm: 699.0513 (684.8097)  amp_scale: 1.0000 (1.0000)  time: 1.3754  data: 0.1276  max mem: 13835
[17:46:30.916233] [Train][Ep-94/100]  [1400/1435]  eta: 0:00:46  lr: 0.0000 (0.0000)  loss: 1021.9155 (1036.5814)  grad_norm: 674.6180 (685.4792)  amp_scale: 1.0000 (1.0000)  time: 1.2236  data: 0.4120  max mem: 13835
[17:47:11.859606] [Train][Ep-94/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1020.7795 (1036.1205)  grad_norm: 658.8064 (685.2466)  amp_scale: 1.0000 (1.0000)  time: 1.1919  data: 0.2810  max mem: 13835
[17:47:11.860558] [Train][Ep-94/100] Total time: 0:31:52 (1.3329 s / it)
[17:47:11.860995] Syncing meters...
[17:47:13.347129] Averaged stats: lr: 0.0000 (0.0000)  loss: 1020.7795 (1040.6980)  grad_norm: 658.8064 (685.2466)  amp_scale: 1.0000 (1.0000)
[17:47:19.138814] [Eval][Ep-94/100]  [  0/121]  eta: 0:11:39  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 5.7837  data: 5.6236  max mem: 13835
[17:49:02.333060] [Eval][Ep-94/100]  [100/121]  eta: 0:00:22  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9435  data: 0.7841  max mem: 13835
[17:49:20.359650] [Eval][Ep-94/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9011  data: 0.7445  max mem: 13835
[17:49:20.360452] [Eval][Ep-94/100] Total time: 0:02:07 (1.0496 s / it)
[17:49:20.669364] [Eval][Ep-94/100] val_acc1_image=26.83 | val_acc1_audio=42.46 | val_acc1_fusion=39.16 | val_acc1_all=52.88
[17:49:29.325994] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 95)
[17:49:31.291735] [Train][Ep-95/100]  [   0/1435]  eta: 0:46:48  lr: 0.0000 (0.0000)  time: 1.9571  data: 1.4762  max mem: 13835
[17:51:35.631785] [Train][Ep-95/100]  [ 100/1435]  eta: 0:27:49  lr: 0.0000 (0.0000)  loss: 1036.7585 (1040.1829)  grad_norm: 678.0992 (688.0612)  amp_scale: 1.0000 (1.0000)  time: 1.2964  data: 0.7306  max mem: 13835
[17:53:46.896012] [Train][Ep-95/100]  [ 200/1435]  eta: 0:26:22  lr: 0.0000 (0.0000)  loss: 1010.6780 (1038.1304)  grad_norm: 690.4009 (689.1255)  amp_scale: 1.0000 (1.0000)  time: 1.3738  data: 0.0364  max mem: 13835
[17:56:00.210702] [Train][Ep-95/100]  [ 300/1435]  eta: 0:24:33  lr: 0.0000 (0.0000)  loss: 1006.1030 (1031.8504)  grad_norm: 685.2559 (686.8698)  amp_scale: 1.0000 (1.0000)  time: 1.2718  data: 0.1503  max mem: 13835
[17:58:17.920381] [Train][Ep-95/100]  [ 400/1435]  eta: 0:22:44  lr: 0.0000 (0.0000)  loss: 1065.9679 (1030.0822)  grad_norm: 685.8933 (687.8291)  amp_scale: 1.0000 (1.0000)  time: 1.3463  data: 0.2167  max mem: 13835
[18:00:35.754488] [Train][Ep-95/100]  [ 500/1435]  eta: 0:20:43  lr: 0.0000 (0.0000)  loss: 1018.9875 (1031.2920)  grad_norm: 688.1768 (687.0040)  amp_scale: 1.0000 (1.0000)  time: 1.4352  data: 0.0188  max mem: 13835
[18:02:52.681769] [Train][Ep-95/100]  [ 600/1435]  eta: 0:18:36  lr: 0.0000 (0.0000)  loss: 1032.4274 (1031.0167)  grad_norm: 674.0734 (685.7610)  amp_scale: 1.0000 (1.0000)  time: 1.3591  data: 0.0009  max mem: 13835
[18:05:02.651755] [Train][Ep-95/100]  [ 700/1435]  eta: 0:16:18  lr: 0.0000 (0.0000)  loss: 1011.3115 (1030.5948)  grad_norm: 682.8092 (686.3205)  amp_scale: 1.0000 (1.0000)  time: 1.3460  data: 0.0039  max mem: 13835
[18:07:18.059389] [Train][Ep-95/100]  [ 800/1435]  eta: 0:14:07  lr: 0.0000 (0.0000)  loss: 1032.3424 (1032.4453)  grad_norm: 705.4583 (687.1584)  amp_scale: 1.0000 (1.0000)  time: 1.3306  data: 0.0002  max mem: 13835
[18:09:29.317118] [Train][Ep-95/100]  [ 900/1435]  eta: 0:11:52  lr: 0.0000 (0.0000)  loss: 1027.3492 (1032.8554)  grad_norm: 694.9360 (687.9861)  amp_scale: 1.0000 (1.0000)  time: 1.2867  data: 0.0003  max mem: 13835
[18:11:43.823354] [Train][Ep-95/100]  [1000/1435]  eta: 0:09:39  lr: 0.0000 (0.0000)  loss: 1043.7090 (1031.5098)  grad_norm: 683.8582 (687.1800)  amp_scale: 1.0000 (1.0000)  time: 1.4090  data: 0.0935  max mem: 13835
[18:14:02.233555] [Train][Ep-95/100]  [1100/1435]  eta: 0:07:28  lr: 0.0000 (0.0000)  loss: 1030.6921 (1031.6016)  grad_norm: 675.9677 (687.3771)  amp_scale: 1.0000 (1.0000)  time: 1.3238  data: 0.0216  max mem: 13835
[18:16:18.369406] [Train][Ep-95/100]  [1200/1435]  eta: 0:05:14  lr: 0.0000 (0.0000)  loss: 1046.4640 (1032.2312)  grad_norm: 676.0037 (687.5843)  amp_scale: 1.0000 (1.0000)  time: 1.3962  data: 0.0233  max mem: 13835
[18:18:31.891748] [Train][Ep-95/100]  [1300/1435]  eta: 0:03:00  lr: 0.0000 (0.0000)  loss: 1030.3813 (1032.5005)  grad_norm: 680.7020 (687.0390)  amp_scale: 1.0000 (1.0000)  time: 1.3394  data: 0.0862  max mem: 13835
[18:20:43.780630] [Train][Ep-95/100]  [1400/1435]  eta: 0:00:46  lr: 0.0000 (0.0000)  loss: 1012.5472 (1030.9751)  grad_norm: 688.8481 (686.7969)  amp_scale: 1.0000 (1.0000)  time: 1.3438  data: 0.0283  max mem: 13835
[18:21:26.659890] [Train][Ep-95/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1021.0917 (1031.0701)  grad_norm: 682.4998 (686.7729)  amp_scale: 1.0000 (1.0000)  time: 1.2544  data: 0.0903  max mem: 13835
[18:21:26.660856] [Train][Ep-95/100] Total time: 0:31:57 (1.3361 s / it)
[18:21:26.661397] Syncing meters...
[18:21:28.681569] Averaged stats: lr: 0.0000 (0.0000)  loss: 1021.0917 (1037.1113)  grad_norm: 682.4998 (686.7729)  amp_scale: 1.0000 (1.0000)
[18:21:38.296228] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 96)
[18:21:40.295097] [Train][Ep-96/100]  [   0/1435]  eta: 0:47:35  lr: 0.0000 (0.0000)  time: 1.9899  data: 1.5076  max mem: 13835
[18:23:41.899967] [Train][Ep-96/100]  [ 100/1435]  eta: 0:27:13  lr: 0.0000 (0.0000)  loss: 1018.0555 (1019.9843)  grad_norm: 678.4828 (681.4550)  amp_scale: 1.0000 (1.0000)  time: 1.2596  data: 0.6846  max mem: 13835
[18:25:48.632595] [Train][Ep-96/100]  [ 200/1435]  eta: 0:25:38  lr: 0.0000 (0.0000)  loss: 983.4357 (1016.9156)  grad_norm: 685.6272 (683.1020)  amp_scale: 1.0000 (1.0000)  time: 1.1604  data: 0.5666  max mem: 13835
[18:27:56.762033] [Train][Ep-96/100]  [ 300/1435]  eta: 0:23:47  lr: 0.0000 (0.0000)  loss: 1031.7312 (1021.1039)  grad_norm: 692.6030 (685.9128)  amp_scale: 1.0000 (1.0000)  time: 1.2730  data: 0.0171  max mem: 13835
[18:29:58.637342] [Train][Ep-96/100]  [ 400/1435]  eta: 0:21:31  lr: 0.0000 (0.0000)  loss: 1013.2450 (1022.5338)  grad_norm: 683.5967 (688.3139)  amp_scale: 1.0000 (1.0000)  time: 1.2319  data: 0.1393  max mem: 13835
[18:32:08.092451] [Train][Ep-96/100]  [ 500/1435]  eta: 0:19:35  lr: 0.0000 (0.0000)  loss: 1064.6776 (1025.7540)  grad_norm: 665.0565 (684.2755)  amp_scale: 1.0000 (1.0000)  time: 1.2886  data: 0.1676  max mem: 13835
[18:34:13.900147] [Train][Ep-96/100]  [ 600/1435]  eta: 0:17:29  lr: 0.0000 (0.0000)  loss: 1073.5785 (1031.5852)  grad_norm: 683.4323 (684.4729)  amp_scale: 1.0000 (1.0000)  time: 1.2670  data: 0.0054  max mem: 13835
[18:36:19.439412] [Train][Ep-96/100]  [ 700/1435]  eta: 0:15:23  lr: 0.0000 (0.0000)  loss: 1063.1370 (1035.4648)  grad_norm: 694.6241 (685.8169)  amp_scale: 1.0000 (1.0000)  time: 1.3397  data: 0.1194  max mem: 13835
[18:38:24.212478] [Train][Ep-96/100]  [ 800/1435]  eta: 0:13:17  lr: 0.0000 (0.0000)  loss: 1047.0433 (1035.7473)  grad_norm: 693.8911 (688.0075)  amp_scale: 1.0000 (1.0000)  time: 1.2681  data: 0.7005  max mem: 13835
[18:40:26.877788] [Train][Ep-96/100]  [ 900/1435]  eta: 0:11:10  lr: 0.0000 (0.0000)  loss: 1030.3319 (1036.3236)  grad_norm: 680.3100 (687.2301)  amp_scale: 1.0000 (1.0000)  time: 1.2926  data: 0.0815  max mem: 13835
[18:42:41.040798] [Train][Ep-96/100]  [1000/1435]  eta: 0:09:08  lr: 0.0000 (0.0000)  loss: 1011.5995 (1035.8634)  grad_norm: 689.1064 (687.6296)  amp_scale: 1.0000 (1.0000)  time: 1.2999  data: 0.0033  max mem: 13835
[18:44:56.591665] [Train][Ep-96/100]  [1100/1435]  eta: 0:07:05  lr: 0.0000 (0.0000)  loss: 1065.3088 (1037.6304)  grad_norm: 691.0024 (687.3745)  amp_scale: 1.0000 (1.0000)  time: 1.3155  data: 0.0002  max mem: 13835
[18:47:08.602315] [Train][Ep-96/100]  [1200/1435]  eta: 0:04:59  lr: 0.0000 (0.0000)  loss: 1047.8236 (1038.8557)  grad_norm: 656.9741 (686.6238)  amp_scale: 1.0000 (1.0000)  time: 1.2987  data: 0.0177  max mem: 13835
[18:49:16.266645] [Train][Ep-96/100]  [1300/1435]  eta: 0:02:52  lr: 0.0000 (0.0000)  loss: 1030.1012 (1039.5999)  grad_norm: 671.0575 (686.3154)  amp_scale: 1.0000 (1.0000)  time: 1.2804  data: 0.0547  max mem: 13835
[18:51:20.720588] [Train][Ep-96/100]  [1400/1435]  eta: 0:00:44  lr: 0.0000 (0.0000)  loss: 1012.5652 (1038.8986)  grad_norm: 658.9276 (686.2085)  amp_scale: 1.0000 (1.0000)  time: 1.2459  data: 0.1147  max mem: 13835
[18:52:01.151002] [Train][Ep-96/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1012.5652 (1039.5191)  grad_norm: 658.9276 (685.9726)  amp_scale: 1.0000 (1.0000)  time: 1.2517  data: 0.0305  max mem: 13835
[18:52:01.151843] [Train][Ep-96/100] Total time: 0:30:22 (1.2703 s / it)
[18:52:01.152228] Syncing meters...
[18:52:01.995961] Averaged stats: lr: 0.0000 (0.0000)  loss: 1012.5652 (1036.3999)  grad_norm: 658.9276 (685.9726)  amp_scale: 1.0000 (1.0000)
[18:52:10.207177] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 97)
[18:52:12.149978] [Train][Ep-97/100]  [   0/1435]  eta: 0:46:15  lr: 0.0000 (0.0000)  time: 1.9342  data: 1.4511  max mem: 13835
[18:54:09.148418] [Train][Ep-97/100]  [ 100/1435]  eta: 0:26:11  lr: 0.0000 (0.0000)  loss: 1043.0754 (1045.4496)  grad_norm: 675.2723 (684.8699)  amp_scale: 1.0000 (1.0000)  time: 1.1217  data: 0.5469  max mem: 13835
[18:56:09.495793] [Train][Ep-97/100]  [ 200/1435]  eta: 0:24:30  lr: 0.0000 (0.0000)  loss: 1042.1721 (1041.9486)  grad_norm: 682.6307 (691.7432)  amp_scale: 1.0000 (1.0000)  time: 1.1765  data: 0.6086  max mem: 13835
[18:58:08.636804] [Train][Ep-97/100]  [ 300/1435]  eta: 0:22:31  lr: 0.0000 (0.0000)  loss: 991.0350 (1036.6803)  grad_norm: 683.3366 (690.4965)  amp_scale: 1.0000 (1.0000)  time: 1.1999  data: 0.5041  max mem: 13835
[19:00:05.010523] [Train][Ep-97/100]  [ 400/1435]  eta: 0:20:25  lr: 0.0000 (0.0000)  loss: 1038.5500 (1042.0074)  grad_norm: 668.2566 (688.0119)  amp_scale: 1.0000 (1.0000)  time: 1.2468  data: 0.3194  max mem: 13835
[19:02:12.994768] [Train][Ep-97/100]  [ 500/1435]  eta: 0:18:44  lr: 0.0000 (0.0000)  loss: 1031.4011 (1045.2177)  grad_norm: 676.6089 (687.3863)  amp_scale: 1.0000 (1.0000)  time: 1.2442  data: 0.2425  max mem: 13835
[19:04:16.154508] [Train][Ep-97/100]  [ 600/1435]  eta: 0:16:48  lr: 0.0000 (0.0000)  loss: 1043.2208 (1045.6318)  grad_norm: 687.7733 (686.8673)  amp_scale: 1.0000 (1.0000)  time: 1.1945  data: 0.0016  max mem: 13835
[19:06:22.903695] [Train][Ep-97/100]  [ 700/1435]  eta: 0:14:53  lr: 0.0000 (0.0000)  loss: 1022.9901 (1043.7469)  grad_norm: 679.6957 (686.7352)  amp_scale: 1.0000 (1.0000)  time: 1.3224  data: 0.1343  max mem: 13835
[19:08:31.586263] [Train][Ep-97/100]  [ 800/1435]  eta: 0:12:57  lr: 0.0000 (0.0000)  loss: 990.8336 (1040.9626)  grad_norm: 671.3866 (685.9539)  amp_scale: 1.0000 (1.0000)  time: 1.2707  data: 0.0797  max mem: 13835
[19:10:37.491825] [Train][Ep-97/100]  [ 900/1435]  eta: 0:10:57  lr: 0.0000 (0.0000)  loss: 1030.9816 (1041.3145)  grad_norm: 678.1453 (686.1097)  amp_scale: 1.0000 (1.0000)  time: 1.2405  data: 0.0276  max mem: 13835
[19:12:40.832470] [Train][Ep-97/100]  [1000/1435]  eta: 0:08:54  lr: 0.0000 (0.0000)  loss: 1037.7919 (1042.1860)  grad_norm: 686.5427 (686.4502)  amp_scale: 1.0000 (1.0000)  time: 1.2147  data: 0.3900  max mem: 13835
[19:14:44.649015] [Train][Ep-97/100]  [1100/1435]  eta: 0:06:52  lr: 0.0000 (0.0000)  loss: 1036.8619 (1043.4302)  grad_norm: 686.2928 (686.4355)  amp_scale: 1.0000 (1.0000)  time: 1.1606  data: 0.5923  max mem: 13835
[19:16:43.943744] [Train][Ep-97/100]  [1200/1435]  eta: 0:04:48  lr: 0.0000 (0.0000)  loss: 1033.9520 (1042.5373)  grad_norm: 675.2556 (685.4618)  amp_scale: 1.0000 (1.0000)  time: 1.1807  data: 0.6133  max mem: 13835
[19:18:51.984606] [Train][Ep-97/100]  [1300/1435]  eta: 0:02:46  lr: 0.0000 (0.0000)  loss: 1046.6976 (1042.4294)  grad_norm: 686.2825 (685.3928)  amp_scale: 1.0000 (1.0000)  time: 1.3706  data: 0.7995  max mem: 13835
[19:21:02.170851] [Train][Ep-97/100]  [1400/1435]  eta: 0:00:43  lr: 0.0000 (0.0000)  loss: 1038.8553 (1042.3885)  grad_norm: 677.8887 (685.1624)  amp_scale: 1.0000 (1.0000)  time: 1.3495  data: 0.7815  max mem: 13835
[19:21:41.972229] [Train][Ep-97/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1037.3129 (1041.7564)  grad_norm: 682.8631 (685.1714)  amp_scale: 1.0000 (1.0000)  time: 1.1552  data: 0.5859  max mem: 13835
[19:21:41.973097] [Train][Ep-97/100] Total time: 0:29:31 (1.2347 s / it)
[19:21:41.973580] Syncing meters...
[19:21:41.974892] Averaged stats: lr: 0.0000 (0.0000)  loss: 1037.3129 (1039.3085)  grad_norm: 682.8631 (685.1714)  amp_scale: 1.0000 (1.0000)
[19:21:50.416515] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 98)
[19:21:52.514873] [Train][Ep-98/100]  [   0/1435]  eta: 0:49:58  lr: 0.0000 (0.0000)  time: 2.0897  data: 1.6071  max mem: 13835
[19:23:52.980607] [Train][Ep-98/100]  [ 100/1435]  eta: 0:26:59  lr: 0.0000 (0.0000)  loss: 1072.8240 (1066.4066)  grad_norm: 675.8929 (688.0382)  amp_scale: 1.0000 (1.0000)  time: 1.2151  data: 0.6307  max mem: 13835
[19:25:51.836825] [Train][Ep-98/100]  [ 200/1435]  eta: 0:24:43  lr: 0.0000 (0.0000)  loss: 1024.0569 (1048.9029)  grad_norm: 672.4583 (681.8668)  amp_scale: 1.0000 (1.0000)  time: 1.3109  data: 0.7426  max mem: 13835
[19:28:00.392868] [Train][Ep-98/100]  [ 300/1435]  eta: 0:23:14  lr: 0.0000 (0.0000)  loss: 1047.0247 (1050.9766)  grad_norm: 679.6808 (688.1953)  amp_scale: 1.0000 (1.0000)  time: 1.2144  data: 0.6395  max mem: 13835
[19:29:57.238282] [Train][Ep-98/100]  [ 400/1435]  eta: 0:20:56  lr: 0.0000 (0.0000)  loss: 1029.8462 (1047.0750)  grad_norm: 686.1617 (686.7869)  amp_scale: 1.0000 (1.0000)  time: 1.1279  data: 0.3267  max mem: 13835
[19:31:56.266529] [Train][Ep-98/100]  [ 500/1435]  eta: 0:18:50  lr: 0.0000 (0.0000)  loss: 1036.1414 (1043.8747)  grad_norm: 677.3120 (685.5364)  amp_scale: 1.0000 (1.0000)  time: 1.2393  data: 0.2516  max mem: 13835
[19:33:54.523638] [Train][Ep-98/100]  [ 600/1435]  eta: 0:16:45  lr: 0.0000 (0.0000)  loss: 1033.9413 (1043.3343)  grad_norm: 683.3601 (684.3486)  amp_scale: 1.0000 (1.0000)  time: 1.1944  data: 0.0056  max mem: 13835
[19:35:52.163916] [Train][Ep-98/100]  [ 700/1435]  eta: 0:14:42  lr: 0.0000 (0.0000)  loss: 1005.5228 (1040.6049)  grad_norm: 697.6207 (684.9446)  amp_scale: 1.0000 (1.0000)  time: 1.2189  data: 0.0683  max mem: 13835
[19:37:54.663432] [Train][Ep-98/100]  [ 800/1435]  eta: 0:12:44  lr: 0.0000 (0.0000)  loss: 1051.3408 (1043.0943)  grad_norm: 677.2247 (684.3570)  amp_scale: 1.0000 (1.0000)  time: 1.2160  data: 0.1009  max mem: 13835
[19:39:59.775697] [Train][Ep-98/100]  [ 900/1435]  eta: 0:10:46  lr: 0.0000 (0.0000)  loss: 1010.6405 (1041.9655)  grad_norm: 679.3730 (684.2870)  amp_scale: 1.0000 (1.0000)  time: 1.1909  data: 0.0385  max mem: 13835
[19:42:03.119127] [Train][Ep-98/100]  [1000/1435]  eta: 0:08:46  lr: 0.0000 (0.0000)  loss: 1080.0840 (1047.1494)  grad_norm: 681.9633 (684.0353)  amp_scale: 1.0000 (1.0000)  time: 1.2298  data: 0.2677  max mem: 13835
[19:44:05.712062] [Train][Ep-98/100]  [1100/1435]  eta: 0:06:46  lr: 0.0000 (0.0000)  loss: 1028.3696 (1044.7370)  grad_norm: 676.4935 (683.2848)  amp_scale: 1.0000 (1.0000)  time: 1.2845  data: 0.0005  max mem: 13835
[19:46:09.242003] [Train][Ep-98/100]  [1200/1435]  eta: 0:04:45  lr: 0.0000 (0.0000)  loss: 1033.8467 (1044.9705)  grad_norm: 659.5255 (682.0900)  amp_scale: 1.0000 (1.0000)  time: 1.2651  data: 0.2329  max mem: 13835
[19:48:21.284921] [Train][Ep-98/100]  [1300/1435]  eta: 0:02:45  lr: 0.0000 (0.0000)  loss: 1024.6398 (1043.9345)  grad_norm: 670.5190 (682.6167)  amp_scale: 1.0000 (1.0000)  time: 1.3344  data: 0.2152  max mem: 13835
[19:50:27.670790] [Train][Ep-98/100]  [1400/1435]  eta: 0:00:42  lr: 0.0000 (0.0000)  loss: 1019.1564 (1042.0352)  grad_norm: 686.7501 (682.6189)  amp_scale: 1.0000 (1.0000)  time: 1.2814  data: 0.0196  max mem: 13835
[19:51:10.852877] [Train][Ep-98/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1019.1564 (1041.9587)  grad_norm: 672.5867 (682.1043)  amp_scale: 1.0000 (1.0000)  time: 1.3003  data: 0.0031  max mem: 13835
[19:51:10.853766] [Train][Ep-98/100] Total time: 0:29:20 (1.2268 s / it)
[19:51:10.854214] Syncing meters...
[19:51:11.846688] Averaged stats: lr: 0.0000 (0.0000)  loss: 1019.1564 (1038.6337)  grad_norm: 672.5867 (682.1043)  amp_scale: 1.0000 (1.0000)
[19:51:21.596750] => saved checkpoint '/home/wiss/zverev/DeepAVFusion/checkpoints/deepavfusion_vitb_vggsound_ep200/finetune_vggsound/checkpoints/checkpoint_latest.pth' (epoch 99)
[19:51:23.381609] [Train][Ep-99/100]  [   0/1435]  eta: 0:42:29  lr: 0.0000 (0.0000)  time: 1.7763  data: 1.2938  max mem: 13835
[19:53:26.693297] [Train][Ep-99/100]  [ 100/1435]  eta: 0:27:33  lr: 0.0000 (0.0000)  loss: 998.8411 (1013.1837)  grad_norm: 682.0072 (693.3116)  amp_scale: 1.0000 (1.0000)  time: 1.1834  data: 0.6159  max mem: 13835
[19:55:25.803299] [Train][Ep-99/100]  [ 200/1435]  eta: 0:25:00  lr: 0.0000 (0.0000)  loss: 1004.1400 (1016.0721)  grad_norm: 668.1910 (688.9023)  amp_scale: 1.0000 (1.0000)  time: 1.2501  data: 0.6806  max mem: 13835
[19:57:23.553555] [Train][Ep-99/100]  [ 300/1435]  eta: 0:22:44  lr: 0.0000 (0.0000)  loss: 1035.4377 (1020.3944)  grad_norm: 677.9246 (688.5036)  amp_scale: 1.0000 (1.0000)  time: 1.1954  data: 0.6256  max mem: 13835
[19:59:18.421893] [Train][Ep-99/100]  [ 400/1435]  eta: 0:20:30  lr: 0.0000 (0.0000)  loss: 1028.3732 (1028.4482)  grad_norm: 686.5548 (687.6766)  amp_scale: 1.0000 (1.0000)  time: 1.1208  data: 0.3386  max mem: 13835
[20:01:12.647781] [Train][Ep-99/100]  [ 500/1435]  eta: 0:18:22  lr: 0.0000 (0.0000)  loss: 1059.8236 (1033.4837)  grad_norm: 694.3716 (687.8057)  amp_scale: 1.0000 (1.0000)  time: 1.1463  data: 0.4639  max mem: 13835
[20:03:12.585021] [Train][Ep-99/100]  [ 600/1435]  eta: 0:16:27  lr: 0.0000 (0.0000)  loss: 1062.0073 (1039.7310)  grad_norm: 669.3903 (687.6446)  amp_scale: 1.0000 (1.0000)  time: 1.2043  data: 0.4745  max mem: 13835
[20:05:11.849283] [Train][Ep-99/100]  [ 700/1435]  eta: 0:14:30  lr: 0.0000 (0.0000)  loss: 1016.9601 (1037.3094)  grad_norm: 687.9910 (689.3964)  amp_scale: 1.0000 (1.0000)  time: 1.1627  data: 0.1691  max mem: 13835
[20:07:11.402261] [Train][Ep-99/100]  [ 800/1435]  eta: 0:12:32  lr: 0.0000 (0.0000)  loss: 994.5083 (1035.0985)  grad_norm: 680.4097 (689.0960)  amp_scale: 1.0000 (1.0000)  time: 1.2138  data: 0.5342  max mem: 13835
[20:09:13.934821] [Train][Ep-99/100]  [ 900/1435]  eta: 0:10:36  lr: 0.0000 (0.0000)  loss: 1030.4436 (1035.0159)  grad_norm: 678.2458 (687.9043)  amp_scale: 1.0000 (1.0000)  time: 1.2076  data: 0.4909  max mem: 13835
[20:11:17.698735] [Train][Ep-99/100]  [1000/1435]  eta: 0:08:39  lr: 0.0000 (0.0000)  loss: 1024.3618 (1035.0349)  grad_norm: 693.9371 (688.1320)  amp_scale: 1.0000 (1.0000)  time: 1.1762  data: 0.1160  max mem: 13835
[20:13:18.919416] [Train][Ep-99/100]  [1100/1435]  eta: 0:06:40  lr: 0.0000 (0.0000)  loss: 1026.1464 (1033.7567)  grad_norm: 673.3134 (686.4514)  amp_scale: 1.0000 (1.0000)  time: 1.1904  data: 0.1204  max mem: 13835
[20:15:17.194127] [Train][Ep-99/100]  [1200/1435]  eta: 0:04:40  lr: 0.0000 (0.0000)  loss: 1014.4022 (1032.8259)  grad_norm: 677.5936 (685.8520)  amp_scale: 1.0000 (1.0000)  time: 1.2466  data: 0.3368  max mem: 13835
[20:17:18.250328] [Train][Ep-99/100]  [1300/1435]  eta: 0:02:41  lr: 0.0000 (0.0000)  loss: 1029.8381 (1031.7430)  grad_norm: 695.9566 (686.7507)  amp_scale: 1.0000 (1.0000)  time: 1.2346  data: 0.6647  max mem: 13835
[20:19:20.942455] [Train][Ep-99/100]  [1400/1435]  eta: 0:00:41  lr: 0.0000 (0.0000)  loss: 1012.2141 (1031.5002)  grad_norm: 677.6138 (686.3739)  amp_scale: 1.0000 (1.0000)  time: 1.2525  data: 0.0142  max mem: 13835
[20:20:01.259692] [Train][Ep-99/100]  [1434/1435]  eta: 0:00:01  lr: 0.0000 (0.0000)  loss: 1044.0859 (1032.0946)  grad_norm: 676.6177 (686.4783)  amp_scale: 1.0000 (1.0000)  time: 1.2479  data: 0.0003  max mem: 13835
[20:20:01.260529] [Train][Ep-99/100] Total time: 0:28:39 (1.1984 s / it)
[20:20:01.260943] Syncing meters...
[20:20:02.656428] Averaged stats: lr: 0.0000 (0.0000)  loss: 1044.0859 (1036.2798)  grad_norm: 676.6177 (686.4783)  amp_scale: 1.0000 (1.0000)
[20:20:04.712343] [Eval][Ep-99/100]  [  0/121]  eta: 0:04:07  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 2.0476  data: 1.8855  max mem: 13835
[20:21:48.095401] [Eval][Ep-99/100]  [100/121]  eta: 0:00:21  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.9080  data: 0.7486  max mem: 13835
[20:22:05.779160] [Eval][Ep-99/100]  [120/121]  eta: 0:00:01  recall_image: nan (nan)  recall_audio: nan (nan)  recall_fusion: nan (nan)  recall_all: nan (nan)  time: 0.8841  data: 0.7281  max mem: 13835
[20:22:05.780112] [Eval][Ep-99/100] Total time: 0:02:03 (1.0175 s / it)
[20:22:06.194589] [Eval][Ep-99/100] val_acc1_image=26.79 | val_acc1_audio=42.61 | val_acc1_fusion=39.19 | val_acc1_all=52.98
